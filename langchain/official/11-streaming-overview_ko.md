# ê°œìš”

Agent ì‹¤í–‰ì—ì„œ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.

LangChainì€ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ë¥¼ ì œê³µí•˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.
ìŠ¤íŠ¸ë¦¬ë°ì€ LLMì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ëœ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë°˜ì‘ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤. ì™„ì „í•œ ì‘ë‹µì´ ì¤€ë¹„ë˜ê¸° ì „ì— ì¶œë ¥ì„ ì ì§„ì ìœ¼ë¡œ í‘œì‹œí•¨ìœ¼ë¡œì¨ ìŠ¤íŠ¸ë¦¬ë°ì€ íŠ¹íˆ LLMì˜ ì§€ì—° ì‹œê°„ì„ ë‹¤ë£° ë•Œ ì‚¬ìš©ì ê²½í—˜(UX)ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

## ê°œìš”

LangChainì˜ ìŠ¤íŠ¸ë¦¬ë° ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ë©´ Agent ì‹¤í–‰ì—ì„œ ì‹¤ì‹œê°„ í”¼ë“œë°±ì„ ì• í”Œë¦¬ì¼€ì´ì…˜ì— í‘œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

LangChain ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ê°€ëŠ¥í•œ ê²ƒ:

-  [Agent ì§„í–‰ ìƒí™© ìŠ¤íŠ¸ë¦¬ë°](#agent-progress) â€” ê° Agent ë‹¨ê³„ í›„ ìƒíƒœ ì—…ë°ì´íŠ¸ ì–»ê¸°.
-  [LLM í† í° ìŠ¤íŠ¸ë¦¬ë°](#llm-tokens) â€” ì–¸ì–´ ëª¨ë¸ í† í°ì„ ìƒì„±ë˜ëŠ” ëŒ€ë¡œ ìŠ¤íŠ¸ë¦¬ë°.
-  [ì‚¬ìš©ì ì •ì˜ ì—…ë°ì´íŠ¸ ìŠ¤íŠ¸ë¦¬ë°](#custom-updates) â€” ì‚¬ìš©ì ì •ì˜ ì‹ í˜¸ ë‚´ë³´ë‚´ê¸°(ì˜ˆ: "10/100ê°œ ë ˆì½”ë“œ ê°€ì ¸ì˜´").
-  [ì—¬ëŸ¬ ëª¨ë“œ ìŠ¤íŠ¸ë¦¬ë°](#stream-multiple-modes) â€” ì—…ë°ì´íŠ¸(Agent ì§„í–‰ ìƒí™©), ë©”ì‹œì§€(LLM í† í° + ë©”íƒ€ë°ì´í„°) ë˜ëŠ” ì‚¬ìš©ì ì •ì˜(ì„ì˜ ì‚¬ìš©ì ë°ì´í„°) ì¤‘ì—ì„œ ì„ íƒ.

ì•„ë˜ì˜ [ì¼ë°˜ì ì¸ íŒ¨í„´](#common-patterns) ì„¹ì…˜ì—ì„œ ì¶”ê°€ ì¢…ë‹¨ ê°„ ì˜ˆì œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

## ì§€ì›ë˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ

ë‹¤ìŒ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì¤‘ í•˜ë‚˜ ì´ìƒì„ `stream` ë˜ëŠ” `astream` ë©”ì„œë“œì— ëª©ë¡ìœ¼ë¡œ ì „ë‹¬í•˜ì„¸ìš”:

| ëª¨ë“œ | ì„¤ëª… |
| :--- | :--- |
| `updates` | ê° Agent ë‹¨ê³„ í›„ ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤. ê°™ì€ ë‹¨ê³„ì—ì„œ ì—¬ëŸ¬ ì—…ë°ì´íŠ¸ê°€ ì´ë£¨ì–´ì§€ë©´(ì˜ˆ: ì—¬ëŸ¬ ë…¸ë“œê°€ ì‹¤í–‰ë¨) ê·¸ ì—…ë°ì´íŠ¸ë“¤ì´ ë³„ë„ë¡œ ìŠ¤íŠ¸ë¦¬ë°ë©ë‹ˆë‹¤. |
| `messages` | LLMì´ í˜¸ì¶œë˜ëŠ” ëª¨ë“  ê·¸ë˜í”„ ë…¸ë“œì—ì„œ `(token, metadata)` íŠœí”Œì„ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤. |
| `custom` | ìŠ¤íŠ¸ë¦¼ ë¼ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜í”„ ë…¸ë“œ ë‚´ë¶€ì—ì„œ ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤. |

## Agent ì§„í–‰ ìƒí™©

Agent ì§„í–‰ ìƒí™©ì„ ìŠ¤íŠ¸ë¦¬ë°í•˜ë ¤ë©´ `stream_mode="updates"`ì™€ í•¨ê»˜ `stream` ë˜ëŠ” `astream` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì´ëŠ” ëª¨ë“  Agent ë‹¨ê³„ í›„ì— ì´ë²¤íŠ¸ë¥¼ ë‚´ë³´ëƒ…ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ í•œ ë²ˆ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ëŠ” Agentê°€ ìˆë‹¤ë©´ ë‹¤ìŒ ì—…ë°ì´íŠ¸ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **LLM ë…¸ë“œ**: ë„êµ¬ í˜¸ì¶œ ìš”ì²­ì´ ìˆëŠ” `AIMessage`
- **ë„êµ¬ ë…¸ë“œ**: ì‹¤í–‰ ê²°ê³¼ê°€ ìˆëŠ” `ToolMessage`
- **LLM ë…¸ë“œ**: ìµœì¢… AI ì‘ë‹µ

```python
from langchain.agents import create_agent

def get_weather(city: str) -> str:
    """ì£¼ì–´ì§„ ë„ì‹œì˜ ë‚ ì”¨ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    return f"It's always sunny in {city}!"

agent = create_agent(
    model="gpt-5-nano",
    tools=[get_weather],
)

for chunk in agent.stream(
    {"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
    stream_mode="updates",
):
    for step, data in chunk.items():
        print(f"step: {step}")
        print(f"content: {data['messages'][-1].content_blocks}")
```

**ì¶œë ¥:**

```text
step: model
content: [{'type': 'tool_call', 'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_OW2NYNsNSKhRZpjW0wm2Aszd'}]
step: tools
content: [{'type': 'text', 'text': "It's always sunny in San Francisco!"}]
step: model
content: [{'type': 'text', 'text': 'It\'s always sunny in San Francisco!'}]
```

## LLM í† í°

LLMì—ì„œ ìƒì„±ë˜ëŠ” í† í°ì„ ìŠ¤íŠ¸ë¦¬ë°í•˜ë ¤ë©´ `stream_mode="messages"`ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì•„ë˜ì—ì„œ Agentê°€ ë„êµ¬ í˜¸ì¶œê³¼ ìµœì¢… ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°í•˜ëŠ” ì¶œë ¥ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from langchain.agents import create_agent

def get_weather(city: str) -> str:
    """ì£¼ì–´ì§„ ë„ì‹œì˜ ë‚ ì”¨ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    return f"It's always sunny in {city}!"

agent = create_agent(
    model="gpt-5-nano",
    tools=[get_weather],
)

for token, metadata in agent.stream(
    {"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
    stream_mode="messages",
):
    print(f"node: {metadata['langgraph_node']}")
    print(f"content: {token.content_blocks}")
    print("\n")
```

**ì¶œë ¥:**

```text
node: model
content: [{'type': 'tool_call_chunk', 'id': 'call_vbCyBcP8VuneUzyYlSBZZsVa', 'name': 'get_weather', 'args': '', 'index': 0}]
# ... (ê°„ëµí•¨ì„ ìœ„í•´ ì¤‘ê°„ ì²­í¬ëŠ” ìƒëµë¨) ...
node: model
content: [{'type': 'text', 'text': 'San'}]
node: model
content: [{'type': 'text', 'text': ' Francisco'}]
node: model
content: [{'type': 'text', 'text': '!"\n\n'}]
```

## ì‚¬ìš©ì ì •ì˜ ì—…ë°ì´íŠ¸

ë„êµ¬ê°€ ì‹¤í–‰ë  ë•Œ ì—…ë°ì´íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ë ¤ë©´ `get_stream_writer`ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from langchain.agents import create_agent
from langgraph.config import get_stream_writer

def get_weather(city: str) -> str:
    """ì£¼ì–´ì§„ ë„ì‹œì˜ ë‚ ì”¨ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    writer = get_stream_writer()
    # ì„ì˜ì˜ ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤
    writer(f"Looking up data for city: {city}")
    writer(f"Acquired data for city: {city}")
    return f"It's always sunny in {city}!"

agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    tools=[get_weather],
)

for chunk in agent.stream(
    {"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
    stream_mode="custom"
):
    print(chunk)
```

**ì¶œë ¥:**

```text
Looking up data for city: San Francisco
Acquired data for city: San Francisco
```

> Tool ë‚´ë¶€ì— `get_stream_writer`ë¥¼ ì¶”ê°€í•˜ë©´ LangGraph ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ì™¸ë¶€ì—ì„œ ë„êµ¬ë¥¼ í˜¸ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

## ì—¬ëŸ¬ ëª¨ë“œ ìŠ¤íŠ¸ë¦¬ë°

ìŠ¤íŠ¸ë¦¼ ëª¨ë“œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬í•˜ì—¬ ì—¬ëŸ¬ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: `stream_mode=["updates", "custom"]`.

ìŠ¤íŠ¸ë¦¬ë°ëœ ì¶œë ¥ì€ `(mode, chunk)` íŠœí”Œì´ ë˜ë©°, ì—¬ê¸°ì„œ `mode`ëŠ” ìŠ¤íŠ¸ë¦¼ ëª¨ë“œì˜ ì´ë¦„ì´ê³  `chunk`ëŠ” í•´ë‹¹ ëª¨ë“œì—ì„œ ìŠ¤íŠ¸ë¦¬ë°ë˜ëŠ” ë°ì´í„°ì…ë‹ˆë‹¤.

```python
from langchain.agents import create_agent
from langgraph.config import get_stream_writer

def get_weather(city: str) -> str:
    """ì£¼ì–´ì§„ ë„ì‹œì˜ ë‚ ì”¨ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    writer = get_stream_writer()
    writer(f"Looking up data for city: {city}")
    writer(f"Acquired data for city: {city}")
    return f"It's always sunny in {city}!"

agent = create_agent(
    model="gpt-5-nano",
    tools=[get_weather],
)

for stream_mode, chunk in agent.stream(
    {"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
    stream_mode=["updates", "custom"]
):
    print(f"stream_mode: {stream_mode}")
    print(f"content: {chunk}")
    print("\n")
```

**ì¶œë ¥:**

```text
stream_mode: updates
content: {'model': {'messages': [...]}}

stream_mode: custom
content: Looking up data for city: San Francisco

stream_mode: custom
content: Acquired data for city: San Francisco

stream_mode: updates
content: {'tools': {'messages': [...]}}

stream_mode: updates
content: {'model': {'messages': [...]}}
```

## ì¼ë°˜ì ì¸ íŒ¨í„´

### ë„êµ¬ í˜¸ì¶œ ìŠ¤íŠ¸ë¦¬ë°

ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ëª¨ë‘ ìŠ¤íŠ¸ë¦¬ë°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1.  ë¶€ë¶„ JSON - [ë„êµ¬ í˜¸ì¶œ](https://docs.langchain.com/oss/python/langchain/models#tool-calling)ì´ ìƒì„±ë  ë•Œ
2.  ì‹¤í–‰ë˜ëŠ” ì™„ì„±ë˜ê³  íŒŒì‹±ëœ ë„êµ¬ í˜¸ì¶œ

`stream_mode="messages"`ë¥¼ ì§€ì •í•˜ë©´ Agentì˜ ëª¨ë“  LLM í˜¸ì¶œì—ì„œ ìƒì„±ëœ ì¦ë¶„ [ë©”ì‹œì§€ ì²­í¬](https://docs.langchain.com/oss/python/langchain/messages#message-chunks)ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤. íŒŒì‹±ëœ ë„êµ¬ í˜¸ì¶œì´ ìˆëŠ” ì™„ì„±ëœ ë©”ì‹œì§€ì— ì ‘ê·¼í•˜ë ¤ë©´:

1.  í•´ë‹¹ ë©”ì‹œì§€ê°€ [ìƒíƒœ](https://docs.langchain.com/oss/python/langgraph/concepts/state)ì—ì„œ ì¶”ì ë˜ëŠ” ê²½ìš°([`create_agent`](https://docs.langchain.com/oss/python/langchain/agents#create-agent)ì˜ ëª¨ë¸ ë…¸ë“œì²˜ëŸ¼) `stream_mode=["messages", "updates"]`ë¥¼ ì‚¬ìš©í•˜ì—¬ [ìƒíƒœ ì—…ë°ì´íŠ¸](https://docs.langchain.com/oss/python/langgraph/how-tos/stream-updates)ë¥¼ í†µí•´ ì™„ì„±ëœ ë©”ì‹œì§€ì— ì ‘ê·¼í•©ë‹ˆë‹¤(ì•„ë˜ ì‹œì—°).
2.  í•´ë‹¹ ë©”ì‹œì§€ê°€ ìƒíƒœì—ì„œ ì¶”ì ë˜ì§€ ì•ŠëŠ” ê²½ìš° [ì‚¬ìš©ì ì •ì˜ ì—…ë°ì´íŠ¸](https://docs.langchain.com/oss/python/langchain/streaming/custom-updates) ë˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„ ì¤‘ì— ì²­í¬ë¥¼ ì§‘ê³„í•©ë‹ˆë‹¤([ë‹¤ìŒ ì„¹ì…˜](https://docs.langchain.com/oss/python/langchain/streaming/custom-updates)).

> [!INFO]
> Agentì— ì—¬ëŸ¬ LLMì´ í¬í•¨ëœ ê²½ìš° [sub-agentì—ì„œ ìŠ¤íŠ¸ë¦¬ë°](https://docs.langchain.com/oss/python/langchain/streaming/sub-agents)ì— ëŒ€í•œ ì•„ë˜ ì„¹ì…˜ì„ ì°¸ì¡°í•˜ì„¸ìš”.

```python
from typing import Any
from langchain.agents import create_agent
from langchain.messages import AIMessage, AIMessageChunk, AnyMessage, ToolMessage

def get_weather(city: str) -> str:
    """ì£¼ì–´ì§„ ë„ì‹œì˜ ë‚ ì”¨ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    return f"It's always sunny in {city}!"

agent = create_agent("openai:gpt-5.2", tools=[get_weather])

def _render_message_chunk(token: AIMessageChunk) -> None:
    if token.text:
        print(token.text, end="|")
    if token.tool_call_chunks:
        print(token.tool_call_chunks)
    # N.B. ëª¨ë“  ì½˜í…ì¸ ëŠ” token.content_blocksë¥¼ í†µí•´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

def _render_completed_message(message: AnyMessage) -> None:
    if isinstance(message, AIMessage) and message.tool_calls:
        print(f"Tool calls: {message.tool_calls}")
    if isinstance(message, ToolMessage):
        print(f"Tool response: {message.content_blocks}")

input_message = {"role": "user", "content": "What is the weather in Boston?"}

for stream_mode, data in agent.stream(
    {"messages": [input_message]},
    stream_mode=["messages", "updates"],
):
    if stream_mode == "messages":
        token, metadata = data
        if isinstance(token, AIMessageChunk):
            _render_message_chunk(token)

    if stream_mode == "updates":
        for source, update in data.items():
            if source in ("model", "tools"):
                # `source`ëŠ” ë…¸ë“œ ì´ë¦„ì„ ìº¡ì²˜í•©ë‹ˆë‹¤
                _render_completed_message(update["messages"][-1])
```

**ì¶œë ¥:**

```text
[{'name': 'get_weather', 'args': '', 'id': 'call_D3Orjr89KgsLTZ9hTzYv7Hpf', 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'city', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '":"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'Boston', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
Tool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_D3Orjr89KgsLTZ9hTzYv7Hpf', 'type': 'tool_call'}]
Tool response: [{'type': 'text', 'text': "It's always sunny in Boston!"}]
The| weather| in| Boston| is| **|sun|ny|**|.|
```

### ì™„ì„±ëœ ë©”ì‹œì§€ ì ‘ê·¼

> ì™„ì„±ëœ ë©”ì‹œì§€ê°€ Agentì˜ ìƒíƒœì—ì„œ ì¶”ì ë˜ë©´ ìœ„ì— ì‹œì—°ëœ ëŒ€ë¡œ `stream_mode=["messages", "updates"]`ë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì— ì™„ì„±ëœ ë©”ì‹œì§€ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê²½ìš°ì— ë”°ë¼ ì™„ì„±ëœ ë©”ì‹œì§€ê°€ ìƒíƒœ ì—…ë°ì´íŠ¸ì— ë°˜ì˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Agent ë‚´ë¶€ì— ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤ë©´ [ì‚¬ìš©ì ì •ì˜ ì—…ë°ì´íŠ¸](#custom-updates)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì— ì´ëŸ¬í•œ ë©”ì‹œì§€ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ ì™¸ì—ëŠ” ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„ì—ì„œ ë©”ì‹œì§€ ì²­í¬ë¥¼ ì§‘ê³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì•„ë˜ ì°¸ì¡°).

ì•„ë˜ì˜ ì˜ˆì œë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”. ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ìŠ¤íŠ¸ë¦¼ ë¼ì´í„°ë¥¼ ê°„ì†Œí™”ëœ ê°€ë“œë ˆì¼ Middlewareì— í†µí•©í•©ë‹ˆë‹¤. ì´ MiddlewareëŠ” êµ¬ì¡°í™”ëœ "ì•ˆì „/ìœ„í—˜" í‰ê°€ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ë„êµ¬ í˜¸ì¶œì„ ì‹œì—°í•©ë‹ˆë‹¤(ì´ë¥¼ ìœ„í•´ êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤):

```python
from typing import Any, Literal

from langchain.agents.middleware import after_agent, AgentState
from langgraph.runtime import Runtime
from langchain.messages import AIMessage
from langchain.chat_models import init_chat_model
from langgraph.config import get_stream_writer
from pydantic import BaseModel


class ResponseSafety(BaseModel):
    """ì‘ë‹µì„ ì•ˆì „ ë˜ëŠ” ìœ„í—˜ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤."""
    evaluation: Literal["safe", "unsafe"]


safety_model = init_chat_model("openai:gpt-5.2")

@after_agent(can_jump_to=["end"])
def safety_guardrail(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    """ëª¨ë¸ ê¸°ë°˜ ê°€ë“œë ˆì¼: LLMì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ì•ˆì „ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤."""
    stream_writer = get_stream_writer()
    # ëª¨ë¸ ì‘ë‹µì„ ê°€ì ¸ì˜µë‹ˆë‹¤
    if not state["messages"]:
        return None

    last_message = state["messages"][-1]
    if not isinstance(last_message, AIMessage):
        return None

    # ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì•ˆì „ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤
    model_with_tools = safety_model.bind_tools([ResponseSafety], tool_choice="any")
    result = model_with_tools.invoke(
        [
            {
                "role": "system",
                "content": "Evaluate this AI response as generally safe or unsafe."
            },
            {
                "role": "user",
                "content": f"AI response: {last_message.text}"
            }
        ]
    )
    stream_writer(result)

    tool_call = result.tool_calls[0]
    if tool_call["args"]["evaluation"] == "unsafe":
        last_message.content = "I cannot provide that response. Please rephrase your request."

    return None
```

ê·¸ëŸ¬ë©´ ì´ Middlewareë¥¼ Agentì— í†µí•©í•˜ê³  ì‚¬ìš©ì ì •ì˜ ìŠ¤íŠ¸ë¦¼ ì´ë²¤íŠ¸ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
from typing import Any

from langchain.agents import create_agent
from langchain.messages import AIMessageChunk, AIMessage, AnyMessage


def get_weather(city: str) -> str:
    """ì£¼ì–´ì§„ ë„ì‹œì˜ ë‚ ì”¨ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""

    return f"It's always sunny in {city}!"


agent = create_agent(
    model="openai:gpt-5.2",
    tools=[get_weather],
    middleware=[safety_guardrail],
)

def _render_message_chunk(token: AIMessageChunk) -> None:
    if token.text:
        print(token.text, end="|")
    if token.tool_call_chunks:
        print(token.tool_call_chunks)


def _render_completed_message(message: AnyMessage) -> None:
    if isinstance(message, AIMessage) and message.tool_calls:
        print(f"Tool calls: {message.tool_calls}")
    if isinstance(message, ToolMessage):
        print(f"Tool response: {message.content_blocks}")


input_message = {"role": "user", "content": "What is the weather in Boston?"}
for stream_mode, data in agent.stream(
    {"messages": [input_message]},
    stream_mode=["messages", "updates", "custom"],
):
    if stream_mode == "messages":
        token, metadata = data
        if isinstance(token, AIMessageChunk):
            _render_message_chunk(token)
    if stream_mode == "updates":
        for source, update in data.items():
            if source in ("model", "tools"):
                _render_completed_message(update["messages"][-1])
    if stream_mode == "custom":
        # ìŠ¤íŠ¸ë¦¼ì—ì„œ ì™„ì„±ëœ ë©”ì‹œì§€ì— ì ‘ê·¼í•©ë‹ˆë‹¤
        print(f"Tool calls: {data.tool_calls}")
```

**ì¶œë ¥:**

```text
[{'name': 'get_weather', 'args': '', 'id': 'call_je6LWgxYzuZ84mmoDalTYMJC', 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'city', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '":"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'Boston', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
Tool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_je6LWgxYzuZ84mmoDalTYMJC', 'type': 'tool_call'}]
Tool response: [{'type': 'text', 'text': "It's always sunny in Boston!"}]
The| weather| in| **|Boston|**| is| **|sun|ny|**|.|[{'name': 'ResponseSafety', 'args': '', 'id': 'call_O8VJIbOG4Q9nQF0T8ltVi58O', 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'evaluation', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '":"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'safe', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
Tool calls: [{'name': 'ResponseSafety', 'args': {'evaluation': 'safe'}, 'id': 'call_O8VJIbOG4Q9nQF0T8ltVi58O', 'type': 'tool_call'}]
```

ë˜ëŠ” ìŠ¤íŠ¸ë¦¼ì— ì‚¬ìš©ì ì •ì˜ ì´ë²¤íŠ¸ë¥¼ ì¶”ê°€í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„ ë‚´ì—ì„œ ë©”ì‹œì§€ ì²­í¬ë¥¼ ì§‘ê³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
input_message = {"role": "user", "content": "What is the weather in Boston?"}
full_message = None
for stream_mode, data in agent.stream(
    {"messages": [input_message]},
    stream_mode=["messages", "updates"],
):
    if stream_mode == "messages":
        token, metadata = data
        if isinstance(token, AIMessageChunk):
            _render_message_chunk(token)
            full_message = token if full_message is None else full_message + token
            if token.chunk_position == "last":
                if full_message.tool_calls:
                    print(f"Tool calls: {full_message.tool_calls}")
                full_message = None
    if stream_mode == "updates":
        for source, update in data.items():
            if source == "tools":
                _render_completed_message(update["messages"][-1])
```

### ì¸ê°„ in the loopë¥¼ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë°

1.  ì¸ê°„ in the loop Middlewareì™€ checkpointerë¡œ Agentë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
2.  "updates" ìŠ¤íŠ¸ë¦¼ ëª¨ë“œ ì¤‘ì— ìƒì„±ëœ ì¤‘ë‹¨ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
3.  ëª…ë ¹ìœ¼ë¡œ í•´ë‹¹ ì¤‘ë‹¨ì— ì‘ë‹µí•©ë‹ˆë‹¤.

```python
from typing import Any
from langchain.agents import create_agent
from langchain.agents.middleware import HumanInTheLoopMiddleware
from langchain.messages import AIMessage, AIMessageChunk, AnyMessage, ToolMessage
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.types import Command, Interrupt

def get_weather(city: str) -> str:
    """ì£¼ì–´ì§„ ë„ì‹œì˜ ë‚ ì”¨ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    return f"It's always sunny in {city}!"

checkpointer = InMemorySaver()

agent = create_agent(
    "openai:gpt-5.2",
    tools=[get_weather],
    middleware=[
        HumanInTheLoopMiddleware(interrupt_on={"get_weather": True}),
    ],
    checkpointer=checkpointer,
)

def _render_message_chunk(token: AIMessageChunk) -> None:
    if token.text:
        print(token.text, end="|")
    if token.tool_call_chunks:
        print(token.tool_call_chunks)

def _render_completed_message(message: AnyMessage) -> None:
    if isinstance(message, AIMessage) and message.tool_calls:
        print(f"Tool calls: {message.tool_calls}")
    if isinstance(message, ToolMessage):
        print(f"Tool response: {message.content_blocks}")

def _render_interrupt(interrupt: Interrupt) -> None:
    interrupts = interrupt.value
    for request in interrupts["action_requests"]:
        print(request["description"])

input_message = {
    "role": "user",
    "content": (
        "Can you look up the weather in Boston and San Francisco?"
    ),
}
config = {"configurable": {"thread_id": "some_id"}}

interrupts = []
for stream_mode, data in agent.stream(
    {"messages": [input_message]},
    config=config,
    stream_mode=["messages", "updates"],
):
    if stream_mode == "messages":
        token, metadata = data
        if isinstance(token, AIMessageChunk):
            _render_message_chunk(token)

    if stream_mode == "updates":
        for source, update in data.items():
            if source in ("model", "tools"):
                _render_completed_message(update["messages"][-1])
                _render_interrupt(update[0])
```

**ì¶œë ¥:**

```text
[{'name': 'get_weather', 'args': '', 'id': 'call_GOwNaQHeqMixay2qy80padfE', 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"ci', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'ty": ', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"Bosto', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'n"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': 'get_weather', 'args': '', 'id': 'call_Ndb4jvWm2uMA0JDQXu37wDH6', 'index': 1, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"ci', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'ty": ', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"San F', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'ranc', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'isco"', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '}', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]
Tool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_GOwNaQHeqMixay2qy80padfE', 'type': 'tool_call'}, {'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_Ndb4jvWm2uMA0JDQXu37wDH6', 'type': 'tool_call'}]
Tool execution requires approval

Tool: get_weather
Args: {'city': 'Boston'}
Tool execution requires approval

Tool: get_weather
Args: {'city': 'San Francisco'}
```

ë‹¤ìŒìœ¼ë¡œ ê° ì¤‘ë‹¨ì— ëŒ€í•œ ê²°ì •ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤. ì¤‘ìš”í•˜ê²Œë„ ê²°ì •ì˜ ìˆœì„œëŠ” ìˆ˜ì§‘í•œ ì‘ì—…ì˜ ìˆœì„œì™€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
ì„¤ëª…í•˜ê¸° ìœ„í•´ í•œ ë„êµ¬ í˜¸ì¶œì„ í¸ì§‘í•˜ê³  ë‹¤ë¥¸ í•˜ë‚˜ë¥¼ ìˆ˜ë½í•©ë‹ˆë‹¤:

```python
def _get_interrupt_decisions(interrupt: Interrupt) -> list[dict]:
    return [
        {
            "type": "edit",
            "edited_action": {
                "name": "get_weather",
                "args": {"city": "Boston, U.K."},
            },
        }
        if "boston" in request["description"].lower()
        else {"type": "approve"}
        for request in interrupt.value["action_requests"]
    ]

decisions = {}
for interrupt in interrupts:
    decisions[interrupt.id] = {
        "decisions": _get_interrupt_decisions(interrupt)
    }

decisions
```

**ì¶œë ¥:**

```text
{
    'a96c40474e429d661b5b32a8d86f0f3e': {
        'decisions': [
            {
                'type': 'edit',
                 'edited_action': {
                     'name': 'get_weather',
                     'args': {'city': 'Boston, U.K.'}
                 }
            },
            {'type': 'approve'},
        ]
    }
}
```

ê·¸ëŸ¬ë©´ ê°™ì€ ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„ì— ëª…ë ¹ì„ ì „ë‹¬í•˜ì—¬ ì¬ê°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
interrupts = []
for stream_mode, data in agent.stream(
    Command(resume=decisions),
    config=config,
    stream_mode=["messages", "updates"],
):
    # ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„ëŠ” ë³€ê²½ ì‚¬í•­ ì—†ìŒ
    if stream_mode == "messages":
        token, metadata = data
        if isinstance(token, AIMessageChunk):
            _render_message_chunk(token)
    if stream_mode == "updates":
        for source, update in data.items():
            if source in ("model", "tools"):
                _render_completed_message(update["messages"][-1])
            if source == "__interrupt__":
                interrupts.extend(update)
                _render_interrupt(update[0])
```

**ì¶œë ¥:**

```text
Tool response: [{'type': 'text', 'text': "It's always sunny in Boston, U.K.!"}]
Tool response: [{'type': 'text', 'text': "It's always sunny in San Francisco!"}]
-| **|Boston|**|:| It|'s| always| sunny| in| Boston|,| U|.K|.|
|-| **|San| Francisco|**|:| It|'s| always| sunny| in| San| Francisco|!|
```

### Sub-agentì—ì„œ ìŠ¤íŠ¸ë¦¬ë°

Agentì˜ ì–´ë–¤ ì§€ì ì— ì—¬ëŸ¬ LLMì´ ìˆì„ ë•Œ ìƒì„±ë˜ëŠ” ë©”ì‹œì§€ì˜ ì¶œì²˜ë¥¼ ëª…í™•íˆ í•´ì•¼ í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.

ì´ë¥¼ ìœ„í•´ Agentë¥¼ ë§Œë“¤ ë•Œ ê° Agentì— `name`ì„ ì „ë‹¬í•˜ì„¸ìš”. ì´ ì´ë¦„ì€ `"messages"` ëª¨ë“œì—ì„œ ìŠ¤íŠ¸ë¦¬ë°í•  ë•Œ `lc_agent_name` í‚¤ë¥¼ í†µí•´ ë©”íƒ€ë°ì´í„°ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

ì•„ë˜ì—ì„œ [ë„êµ¬ í˜¸ì¶œ ìŠ¤íŠ¸ë¦¬ë°](https://docs.langchain.com/oss/python/langchain/streaming/streaming-tool-calls) ì˜ˆì œë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤:

1.  Toolì„ ë‚´ë¶€ì ìœ¼ë¡œ Agentë¥¼ í˜¸ì¶œí•˜ëŠ” `call_weather_agent` Toolë¡œ êµì²´í•©ë‹ˆë‹¤
2.  ê° Agentì— `name`ì„ ì¶”ê°€í•©ë‹ˆë‹¤
3.  ìŠ¤íŠ¸ë¦¼ì„ ë§Œë“¤ ë•Œ `subgraphs=True`ë¥¼ ì§€ì •í•©ë‹ˆë‹¤
4.  ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ëŠ” ì´ì „ê³¼ ë™ì¼í•˜ì§€ë§Œ `create_agent`ì˜ `name` ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–´ëŠ Agentê°€ í™œì„± ì¤‘ì¸ì§€ ì¶”ì í•˜ëŠ” ë¡œì§ì„ ì¶”ê°€í•©ë‹ˆë‹¤

> [!TIP]
> Agentì— `name`ì„ ì„¤ì •í•˜ë©´ í•´ë‹¹ ì´ë¦„ì´ Agentì—ì„œ ìƒì„±ëœ ëª¨ë“  `AIMessage`ì—ë„ ì²¨ë¶€ë©ë‹ˆë‹¤.

ë¨¼ì € Agentë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤:

```python
from typing import Any

from langchain.agents import create_agent
from langchain.chat_models import init_chat_model
from langchain.messages import AIMessage, AnyMessage


def get_weather(city: str) -> str:
    """ì£¼ì–´ì§„ ë„ì‹œì˜ ë‚ ì”¨ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""

    return f"It's always sunny in {city}!"


weather_model = init_chat_model("openai:gpt-5.2")
weather_agent = create_agent(
    model=weather_model,
    tools=[get_weather],
    name="weather_agent",
)


def call_weather_agent(query: str) -> str:
    """ë‚ ì”¨ Agentë¥¼ ì¿¼ë¦¬í•©ë‹ˆë‹¤."""
    result = weather_agent.invoke({
        "messages": [{"role": "user", "content": query}]
    })
    return result["messages"][-1].text


supervisor_model = init_chat_model("openai:gpt-5.2")
agent = create_agent(
    model=supervisor_model,
    tools=[call_weather_agent],
    name="supervisor",
)
```

ë‹¤ìŒìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„ì— ë¡œì§ì„ ì¶”ê°€í•˜ì—¬ ì–´ëŠ Agentê°€ í† í°ì„ ë‚´ë³´ë‚´ëŠ”ì§€ ë³´ê³ í•©ë‹ˆë‹¤:

```python
def _render_message_chunk(token: AIMessageChunk) -> None:
    if token.text:
        print(token.text, end="|")
    if token.tool_call_chunks:
        print(token.tool_call_chunks)


def _render_completed_message(message: AnyMessage) -> None:
    if isinstance(message, AIMessage) and message.tool_calls:
        print(f"Tool calls: {message.tool_calls}")
    if isinstance(message, ToolMessage):
        print(f"Tool response: {message.content_blocks}")


input_message = {"role": "user", "content": "What is the weather in Boston?"}
current_agent = None
for _, stream_mode, data in agent.stream(
    {"messages": [input_message]},
    stream_mode=["messages", "updates"],
    subgraphs=True,
):
    if stream_mode == "messages":
        token, metadata = data
        if agent_name := metadata.get("lc_agent_name"):
            if agent_name != current_agent:
                print(f"ğŸ¤– {agent_name}: ")
                current_agent = agent_name
        if isinstance(token, AIMessage):
            _render_message_chunk(token)
    if stream_mode == "updates":
        for source, update in data.items():
            if source in ("model", "tools"):
                _render_completed_message(update["messages"][-1])
```

**ì¶œë ¥:**

```text
ğŸ¤– supervisor:
[{'name': 'call_weather_agent', 'args': '', 'id': 'call_asorzUf0mB6sb7MiKfgojp7I', 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'query', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '":"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'Boston', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': ' weather', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': ' right', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': ' now', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': ' and', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': " today's", 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': ' forecast', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
Tool calls: [{'name': 'call_weather_agent', 'args': {'query': "Boston weather right now and today's forecast"}, 'id': 'call_asorzUf0mB6sb7MiKfgojp7I', 'type': 'tool_call'}]
ğŸ¤– weather_agent:
[{'name': 'get_weather', 'args': '', 'id': 'call_LZ89lT8fW6w8vqck5pZeaDIx', 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'city', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '":"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'Boston', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
Tool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_LZ89lT8fW6w8vqck5pZeaDIx', 'type': 'tool_call'}]
Tool response: [{'type': 'text', 'text': "It's always sunny in Boston!"}]
Boston| weather| right| now|:| **|Sunny|**|.

|Today|'s| forecast| for| Boston|:| **|Sunny| all| day|**|.|Tool response: [{'type': 'text', 'text': 'Boston weather right now: **Sunny**.\n\nToday's forecast for Boston: **Sunny all day**.'}]
ğŸ¤– supervisor:
Boston| weather| right| now|:| **|Sunny|**|.

|Today|'s| forecast| for| Boston|:| **|Sunny| all| day|**|.|
```

## ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”

ì¼ë¶€ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œëŠ” ì£¼ì–´ì§„ ëª¨ë¸ì— ëŒ€í•œ ê°œë³„ í† í°ì˜ ìŠ¤íŠ¸ë¦¬ë°ì„ ë¹„í™œì„±í™”í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë‹¤ìŒì˜ ê²½ìš°ì— ìœ ìš©í•©ë‹ˆë‹¤:

- [ë‹¤ì¤‘ Agent](https://docs.langchain.com/oss/python/langchain/multi-agent) ì‹œìŠ¤í…œìœ¼ë¡œ ì‘ì—…í•˜ì—¬ ì–´ëŠ Agentê°€ ì¶œë ¥ì„ ìŠ¤íŠ¸ë¦¬ë°í• ì§€ ì œì–´
- ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ëŠ” ëª¨ë¸ê³¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ í˜¼í•©
- [LangSmith](https://docs.langchain.com/langsmith/home)ì— ë°°í¬í•˜ê³  íŠ¹ì • ëª¨ë¸ ì¶œë ¥ì´ í´ë¼ì´ì–¸íŠ¸ë¡œ ìŠ¤íŠ¸ë¦¬ë°ë˜ì§€ ì•Šë„ë¡ í•˜ê³  ì‹¶ìŒ

ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ë•Œ `streaming=False`ë¥¼ ì„¤ì •í•˜ì„¸ìš”.

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(
    model="gpt-4o",
    streaming=False
)
```

> [LangSmith](https://docs.langchain.com/langsmith/home)ì— ë°°í¬í•  ë•Œ ì¶œë ¥ì„ í´ë¼ì´ì–¸íŠ¸ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•˜ì§€ ì•Šìœ¼ë ¤ëŠ” ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ `streaming=False`ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì´ëŠ” ë°°í¬ ì „ì— ê·¸ë˜í”„ ì½”ë“œì—ì„œ êµ¬ì„±ë©ë‹ˆë‹¤.

> ëª¨ë“  ì±„íŒ… ëª¨ë¸ í†µí•©ì´ `streaming` ë§¤ê°œë³€ìˆ˜ë¥¼ ì§€ì›í•˜ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ëª¨ë¸ì´ ì§€ì›í•˜ì§€ ì•Šìœ¼ë©´ ëŒ€ì‹  `disable_streaming=True`ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì´ ë§¤ê°œë³€ìˆ˜ëŠ” ê¸°ë³¸ í´ë˜ìŠ¤ë¥¼ í†µí•´ ëª¨ë“  ì±„íŒ… ëª¨ë¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ìì„¸í•œ ë‚´ìš©ì€ [LangGraph ìŠ¤íŠ¸ë¦¬ë° ê°€ì´ë“œ](https://docs.langchain.com/oss/python/langgraph/streaming#disable-streaming-for-specific-chat-models)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

## ê´€ë ¨

- [Frontend ìŠ¤íŠ¸ë¦¬ë°](https://docs.langchain.com/oss/python/langchain/streaming/frontend) â€” useStreamì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ Agent ìƒí˜¸ì‘ìš©ì„ ìœ„í•œ React UI êµ¬ì¶•
- [ì±„íŒ… ëª¨ë¸ë¡œ ìŠ¤íŠ¸ë¦¬ë°](https://docs.langchain.com/oss/python/langchain/models#stream) â€” Agent ë˜ëŠ” ê·¸ë˜í”„ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì±„íŒ… ëª¨ë¸ì—ì„œ ì§ì ‘ í† í° ìŠ¤íŠ¸ë¦¬ë°
- [ì¸ê°„ in the loopë¥¼ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë°](https://docs.langchain.com/oss/python/langchain/human-in-the-loop#streaming-with-hil) â€” ì¸ê°„ ê²€í† ë¥¼ ìœ„í•œ ì¤‘ë‹¨ì„ ì²˜ë¦¬í•˜ë©´ì„œ Agent ì§„í–‰ ìƒí™© ìŠ¤íŠ¸ë¦¬ë°
- [LangGraph ìŠ¤íŠ¸ë¦¬ë°](https://docs.langchain.com/oss/python/langgraph/streaming) â€” ê°’, ë””ë²„ê·¸ ëª¨ë“œ, subgraph ìŠ¤íŠ¸ë¦¬ë°ì„ í¬í•¨í•œ ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë° ì˜µì…˜
