# ëª¨ë¸

LLMì€ ì¸ê°„ì²˜ëŸ¼ í…ìŠ¤íŠ¸ë¥¼ í•´ì„í•˜ê³  ìƒì„±í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ AI ë„êµ¬ì…ë‹ˆë‹¤. ê° ì‘ì—…ì— íŠ¹í™”ëœ í•™ìŠµ ì—†ì´ë„ ì½˜í…ì¸  ì‘ì„±, ì–¸ì–´ ë²ˆì—­, ìš”ì•½ ë° ì§ˆë¬¸ ë‹µë³€ ë“± ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í…ìŠ¤íŠ¸ ìƒì„± ì™¸ì—ë„ ë§ì€ ëª¨ë¸ì€ ë‹¤ìŒì„ ì§€ì›í•©ë‹ˆë‹¤:
- [Tool í˜¸ì¶œ](https://docs.langchain.com/oss/python/langchain/models#tool-calling) - ì™¸ë¶€ ë„êµ¬(ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ë˜ëŠ” API í˜¸ì¶œ ë“±)ë¥¼ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ ì‘ë‹µì— ì‚¬ìš©í•©ë‹ˆë‹¤.
- [êµ¬ì¡°í™”ëœ ì¶œë ¥](https://docs.langchain.com/oss/python/langchain/models#structured-output) - ëª¨ë¸ì˜ ì‘ë‹µì´ ì •ì˜ëœ í˜•ì‹ì„ ë”°ë¥´ë„ë¡ ì œí•œë©ë‹ˆë‹¤.
- [ë©€í‹°ëª¨ë‹¬](https://docs.langchain.com/oss/python/langchain/models#multimodal) - í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë° ë¹„ë””ì˜¤ì™€ ê°™ì€ í…ìŠ¤íŠ¸ ì´ì™¸ì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.
- [ì¶”ë¡ ](https://docs.langchain.com/oss/python/langchain/models#reasoning) - ëª¨ë¸ì´ ë‹¤ë‹¨ê³„ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ì—¬ ê²°ë¡ ì— ë„ë‹¬í•©ë‹ˆë‹¤.

ëª¨ë¸ì€ Agentì˜ ì¶”ë¡  ì—”ì§„ì…ë‹ˆë‹¤. ëª¨ë¸ì€ Agentì˜ ì˜ì‚¬ ê²°ì • í”„ë¡œì„¸ìŠ¤ë¥¼ ì£¼ë„í•˜ë©°, ì–´ë–¤ Toolì„ í˜¸ì¶œí• ì§€, ê²°ê³¼ë¥¼ ì–´ë–»ê²Œ í•´ì„í• ì§€, ì–¸ì œ ìµœì¢… ë‹µë³€ì„ ì œê³µí• ì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.

ì„ íƒí•œ ëª¨ë¸ì˜ í’ˆì§ˆê³¼ ê¸°ëŠ¥ì€ Agentì˜ ê¸°ë³¸ ì‹ ë¢°ì„±ê³¼ ì„±ëŠ¥ì— ì§ì ‘ì ìœ¼ë¡œ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ íƒì›”í•©ë‹ˆë‹¤. ì¼ë¶€ëŠ” ë³µì¡í•œ ì§€ì¹¨ ë”°ë¥´ê¸°ì— ë” ë‚˜ìœ¼ë©°, ë‹¤ë¥¸ ì¼ë¶€ëŠ” êµ¬ì¡°í™”ëœ ì¶”ë¡ ì— íƒì›”í•˜ê³ , ë” ë§ì€ ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë” í° ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ë¥¼ ì§€ì›í•˜ê¸°ë„ í•©ë‹ˆë‹¤.

LangChainì˜ í‘œì¤€ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ëŠ” ë§ì€ ë‹¤ì–‘í•œ ì œê³µì í†µí•©ì— ì ‘ê·¼í•˜ê²Œ í•˜ë¯€ë¡œ, ì‚¬ìš© ì‚¬ë¡€ì— ê°€ì¥ ì í•©í•œ ëª¨ë¸ì„ ì°¾ê¸° ìœ„í•´ ëª¨ë¸ ê°„ì— ì‰½ê²Œ ì‹¤í—˜í•˜ê³  ì „í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì œê³µìë³„ í†µí•© ì •ë³´ ë° ê¸°ëŠ¥ì— ëŒ€í•´ì„œëŠ” [ì œê³µìì˜ Chat ëª¨ë¸ í˜ì´ì§€](https://docs.langchain.com/oss/python/integrations/chat)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

## ê¸°ë³¸ ì‚¬ìš©ë²•

ëª¨ë¸ì€ ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. [Agent](https://docs.langchain.com/oss/python/langchain/agents#model)ì™€ í•¨ê»˜ - Agent ìƒì„± ì‹œ ëª¨ë¸ì„ ë™ì ìœ¼ë¡œ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. ë…ë¦½ ì‹¤í–‰í˜• - ëª¨ë¸ì„ ì§ì ‘ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(Agent ë£¨í”„ ì™¸ë¶€ì—ì„œ). í…ìŠ¤íŠ¸ ìƒì„±, ë¶„ë¥˜ ë˜ëŠ” ì¶”ì¶œ ê°™ì€ ì‘ì—…ì„ Agent í”„ë ˆì„ì›Œí¬ í•„ìš” ì—†ì´ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë™ì¼í•œ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ê°€ ë‘ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‘ë™í•˜ë¯€ë¡œ, í•„ìš”ì— ë”°ë¼ ê°„ë‹¨í•˜ê²Œ ì‹œì‘í•˜ì—¬ ë” ë³µì¡í•œ Agent ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°ë¡œ í™•ì¥í•  ìˆ˜ ìˆëŠ” ìœ ì—°ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.

### ëª¨ë¸ ì´ˆê¸°í™”

LangChainì—ì„œ ë…ë¦½ ì‹¤í–‰í˜• ëª¨ë¸ì„ ì‹œì‘í•˜ëŠ” ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì€ ì„ íƒí•œ Chat ëª¨ë¸ ì œê³µìì—ì„œ ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê¸° ìœ„í•´ `init_chat_model`ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤(ì•„ë˜ ì˜ˆì‹œ):

#### OpenAI

ğŸ‘‰ [OpenAI Chat ëª¨ë¸ í†µí•© ë¬¸ì„œ ì½ê¸°](https://docs.langchain.com/oss/python/integrations/chat/openai)

```bash
pip install -U "langchain[openai]"
```

**init_chat_model**

```python
import os
from langchain.chat_models import init_chat_model

os.environ["OPENAI_API_KEY"] = "sk-..."

model = init_chat_model("gpt-4.1")
```

**ëª¨ë¸ í´ë˜ìŠ¤**

```python
import os
from langchain_openai import ChatOpenAI

os.environ["OPENAI_API_KEY"] = "sk-..."

model = ChatOpenAI(model="gpt-4.1")
```

#### Anthropic

ğŸ‘‰ [Anthropic Chat ëª¨ë¸ í†µí•© ë¬¸ì„œ ì½ê¸°](https://docs.langchain.com/oss/python/integrations/chat/anthropic)

```bash
pip install -U "langchain[anthropic]"
```

**init_chat_model**

```python
import os
from langchain.chat_models import init_chat_model

os.environ["ANTHROPIC_API_KEY"] = "sk-..."

model = init_chat_model("claude-sonnet-4-5-20250929")
```

**ëª¨ë¸ í´ë˜ìŠ¤**

```python
import os
from langchain_anthropic import ChatAnthropic

os.environ["ANTHROPIC_API_KEY"] = "sk-..."

model = ChatAnthropic(model = "claude-sonnet-4-5-20250929")
```

#### Azure

ğŸ‘‰ [Azure Chat ëª¨ë¸ í†µí•© ë¬¸ì„œ ì½ê¸°](https://docs.langchain.com/oss/python/integrations/chat/azure_chat_openai)

```bash
pip install -U "langchain[openai]"
```

**init_chat_model**

```python
import os
from langchain.chat_models import init_chat_model

os.environ["AZURE_OPENAI_API_KEY"] = "..."
os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"

model = init_chat_model(
    "azure_openai:gpt-4.1",
    azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
)
```

**ëª¨ë¸ í´ë˜ìŠ¤**

```python
import os
from langchain_openai import AzureChatOpenAI

os.environ["AZURE_OPENAI_API_KEY"] = "..."
os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"

model = AzureChatOpenAI(
    model="gpt-4.1",
    azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"]
)
```

#### Google Gemini

ğŸ‘‰ [Google GenAI Chat ëª¨ë¸ í†µí•© ë¬¸ì„œ ì½ê¸°](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai)

```bash
pip install -U "langchain[google-genai]"
```

**init_chat_model**

```python
import os
from langchain.chat_models import init_chat_model

os.environ["GOOGLE_API_KEY"] = "..."

model = init_chat_model("google_genai:gemini-2.5-flash-lite")
```

**ëª¨ë¸ í´ë˜ìŠ¤**

```python
import os
from langchain_google_genai import ChatGoogleGenerativeAI

os.environ["GOOGLE_API_KEY"] = "..."

model = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite")
```

#### AWS Bedrock

ğŸ‘‰ [AWS Bedrock Chat ëª¨ë¸ í†µí•© ë¬¸ì„œ ì½ê¸°](https://docs.langchain.com/oss/python/integrations/chat/bedrock)

```bash
pip install -U "langchain[aws]"
```

**init_chat_model**

```python
from langchain.chat_models import init_chat_model

# ì´ê³³ì˜ ë‹¨ê³„ì— ë”°ë¼ ìê²© ì¦ëª…ì„ êµ¬ì„±í•˜ì„¸ìš”:
# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html

model = init_chat_model(
    "anthropic.claude-3-5-sonnet-20240620-v1:0",
    model_provider="bedrock_converse",
)
```

**ëª¨ë¸ í´ë˜ìŠ¤**

```python
from langchain_aws import ChatBedrock

model = ChatBedrock(
    model="anthropic.claude-3-5-sonnet-20240620-v1:0"
)
```

#### HuggingFace

ğŸ‘‰ [HuggingFace Chat ëª¨ë¸ í†µí•© ë¬¸ì„œ ì½ê¸°](https://docs.langchain.com/oss/python/integrations/chat/huggingface)

```bash
pip install -U "langchain[huggingface]"
```

**init_chat_model**

```python
import os
from langchain.chat_models import init_chat_model

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "hf_..."

model = init_chat_model(
    "microsoft/Phi-3-mini-4k-instruct",
    model_provider="huggingface",
    temperature=0.7,
    max_tokens=1024,
)
```

**ëª¨ë¸ í´ë˜ìŠ¤**

```python
import os
from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "hf_..."

llm = HuggingFaceEndpoint(
    repo_id="microsoft/Phi-3-mini-4k-instruct",
    temperature=0.7,
    max_length=1024,
)
model = ChatHuggingFace(llm=llm)
```

```python
response = model.invoke("Why do parrots talk?")
```

ìì„¸í•œ ë‚´ìš©ì€ `init_chat_model`ì„ ì°¸ì¡°í•˜ì„¸ìš”. ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ë¥¼ ì „ë‹¬í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ì •ë³´ë„ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

### ì§€ì›ë˜ëŠ” ëª¨ë¸

LangChainì€ OpenAI, Anthropic, Google, Azure, AWS Bedrock ë“±ì„ í¬í•¨í•œ ëª¨ë“  ì£¼ìš” ëª¨ë¸ ì œê³µìë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ê° ì œê³µìëŠ” ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ê°€ì§„ ë‹¤ì–‘í•œ ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤. LangChainì—ì„œ ì§€ì›ë˜ëŠ” ëª¨ë¸ì˜ ì „ì²´ ëª©ë¡ì€ [í†µí•© í˜ì´ì§€](https://docs.langchain.com/oss/python/integrations/providers/overview)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

### ì£¼ìš” ë©”ì„œë“œ

<table style="width: 100%; border-spacing: 0 10px; border-collapse: separate;">
  <tr>
    <td style="border: 1px solid #e0e0e0; border-radius: 10px; padding: 15px;">
        <a href="https://docs.langchain.com/oss/python/langchain/models#invoke" style="text-decoration: none; color: inherit; display: block;">
            <div style="font-weight: bold; font-size: 1.1em; margin-bottom: 5px;">Invoke</div>
            <div style="color: #555;">ëª¨ë¸ì€ ë©”ì‹œì§€ë¥¼ ì…ë ¥ë°›ì•„ ì™„ì „í•œ ì‘ë‹µì„ ìƒì„±í•œ í›„ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.</div>
        </a>
    </td>
  </tr>
  <tr>
    <td style="border: 1px solid #e0e0e0; border-radius: 10px; padding: 15px;">
        <a href="https://docs.langchain.com/oss/python/langchain/models#stream" style="text-decoration: none; color: inherit; display: block;">
            <div style="font-weight: bold; font-size: 1.1em; margin-bottom: 5px;">Stream</div>
            <div style="color: #555;">ëª¨ë¸ì„ í˜¸ì¶œí•˜ë˜, ìƒì„±ë˜ëŠ” ë™ì•ˆ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥ì„ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.</div>
        </a>
    </td>
  </tr>
  <tr>
    <td style="border: 1px solid #e0e0e0; border-radius: 10px; padding: 15px;">
        <a href="https://docs.langchain.com/oss/python/langchain/models#batch" style="text-decoration: none; color: inherit; display: block;">
            <div style="font-weight: bold; font-size: 1.1em; margin-bottom: 5px;">Batch</div>
            <div style="color: #555;">ì—¬ëŸ¬ ìš”ì²­ì„ ëª¨ë¸ë¡œ ì¼ê´„ ì²˜ë¦¬í•˜ì—¬ ë” íš¨ìœ¨ì ì¸ ì²˜ë¦¬ë¥¼ í•©ë‹ˆë‹¤.</div>
        </a>
    </td>
  </tr>
</table>

> Chat ëª¨ë¸ ì™¸ì—ë„ LangChainì€ ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„° ì €ì¥ì†Œì™€ ê°™ì€ ì¸ì ‘í•œ ê¸°íƒ€ ê¸°ìˆ ì„ ì§€ì›í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [í†µí•© í˜ì´ì§€](https://docs.langchain.com/oss/python/integrations/providers/overview)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

## ë§¤ê°œë³€ìˆ˜

Chat ëª¨ë¸ì€ ë™ì‘ì„ êµ¬ì„±í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì§€ì›ë˜ëŠ” ë§¤ê°œë³€ìˆ˜ì˜ ì „ì²´ ì§‘í•©ì€ ëª¨ë¸ê³¼ ì œê³µìì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ, í‘œì¤€ ë§¤ê°œë³€ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

<div style="margin-bottom: 20px;">
  <div style="display: flex; align-items: center; gap: 10px; margin-bottom: 10px;">
    <code style="color: #00766c; font-weight: bold; font-size: 1.1em;">model</code>
    <span style="background-color: #fca5a533; color: #b91c1c; padding: 2px 8px; border-radius: 4px; font-size: 0.9em; font-family: monospace;">í•„ìˆ˜</span>
    <span style="background-color: #f3f4f6; color: #1f2937; padding: 2px 8px; border-radius: 4px; font-size: 0.9em; font-family: monospace;">ë¬¸ìì—´</span>
  </div>
  <p style="margin-top: 0; color: #374151;">ì œê³µìì™€ í•¨ê»˜ ì‚¬ìš©í•˜ë ¤ëŠ” íŠ¹ì • ëª¨ë¸ì˜ ì´ë¦„ ë˜ëŠ” ì‹ë³„ìì…ë‹ˆë‹¤. ':' í˜•ì‹ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ê³¼ ì œê³µìë¥¼ í•˜ë‚˜ì˜ ì¸ìˆ˜ë¡œ ì§€ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 'openai:o1'ì…ë‹ˆë‹¤.</p>
</div>

<div style="margin-bottom: 20px;">
  <div style="display: flex; align-items: center; gap: 10px; margin-bottom: 10px;">
    <code style="color: #00766c; font-weight: bold; font-size: 1.1em;">api_key</code>
    <span style="background-color: #f3f4f6; color: #1f2937; padding: 2px 8px; border-radius: 4px; font-size: 0.9em; font-family: monospace;">ë¬¸ìì—´</span>
  </div>
  <p style="margin-top: 0; color: #374151;">ëª¨ë¸ì˜ ì œê³µìë¡œ ì¸ì¦í•˜ê¸° ìœ„í•´ í•„ìš”í•œ í‚¤ì…ë‹ˆë‹¤. ì´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë¸ì— ëŒ€í•œ ì ‘ê·¼ì´ ìŠ¹ì¸ë˜ì—ˆì„ ë•Œ ë°œê¸‰ë©ë‹ˆë‹¤. í”íˆ <a href="https://docs.langchain.com/oss/python/langchain/models/messages#environment-variables">í™˜ê²½ ë³€ìˆ˜</a>ë¥¼ ì„¤ì •í•˜ì—¬ ì ‘ê·¼ë©ë‹ˆë‹¤.</p>
</div>

<div style="margin-bottom: 20px;">
  <div style="display: flex; align-items: center; gap: 10px; margin-bottom: 10px;">
    <code style="color: #00766c; font-weight: bold; font-size: 1.1em;">temperature</code>
    <span style="background-color: #f3f4f6; color: #1f2937; padding: 2px 8px; border-radius: 4px; font-size: 0.9em; font-family: monospace;">ìˆ«ì</span>
  </div>
  <p style="margin-top: 0; color: #374151;">ëª¨ë¸ ì¶œë ¥ì˜ ì„ì˜ì„±ì„ ì œì–´í•©ë‹ˆë‹¤. ë†’ì€ ìˆ«ìì¼ìˆ˜ë¡ ì‘ë‹µì´ ë” ì°½ì˜ì ì´ë©°, ë‚®ì€ ìˆ«ìì¼ìˆ˜ë¡ ë” ê²°ì •ë¡ ì ì…ë‹ˆë‹¤.</p>
</div>

<div style="margin-bottom: 20px;">
  <div style="display: flex; align-items: center; gap: 10px; margin-bottom: 10px;">
    <code style="color: #00766c; font-weight: bold; font-size: 1.1em;">max_tokens</code>
    <span style="background-color: #f3f4f6; color: #1f2937; padding: 2px 8px; border-radius: 4px; font-size: 0.9em; font-family: monospace;">ìˆ«ì</span>
  </div>
  <p style="margin-top: 0; color: #374151;">ì‘ë‹µì˜ ì´ <a href="https://docs.langchain.com/oss/python/langchain/models/messages#tokens">í† í°</a> ìˆ˜ë¥¼ ì œí•œí•˜ë©°, ì¶œë ¥ì˜ ê¸¸ì´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì œì–´í•©ë‹ˆë‹¤.</p>
</div>

<div style="margin-bottom: 20px;">
  <div style="display: flex; align-items: center; gap: 10px; margin-bottom: 10px;">
    <code style="color: #00766c; font-weight: bold; font-size: 1.1em;">timeout</code>
    <span style="background-color: #f3f4f6; color: #1f2937; padding: 2px 8px; border-radius: 4px; font-size: 0.9em; font-family: monospace;">ìˆ«ì</span>
  </div>
  <p style="margin-top: 0; color: #374151;">ëª¨ë¸ë¡œë¶€í„° ì‘ë‹µì„ ê¸°ë‹¤ë¦´ ìˆ˜ ìˆëŠ” ìµœëŒ€ ì‹œê°„(ì´ˆ)ìœ¼ë¡œ, ì´ ì‹œê°„ì´ ì§€ë‚˜ë©´ ìš”ì²­ì´ ì·¨ì†Œë©ë‹ˆë‹¤.</p>
</div>

<div style="margin-bottom: 20px;">
  <div style="display: flex; align-items: center; gap: 10px; margin-bottom: 10px;">
    <code style="color: #00766c; font-weight: bold; font-size: 1.1em;">max_retries</code>
    <span style="background-color: #f3f4f6; color: #1f2937; padding: 2px 8px; border-radius: 4px; font-size: 0.9em; font-family: monospace;">ìˆ«ì</span>
  </div>
  <p style="margin-top: 0; color: #374151;">ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒì´ë‚˜ ì†ë„ ì œí•œ ê°™ì€ ë¬¸ì œë¡œ ì¸í•´ ìš”ì²­ì´ ì‹¤íŒ¨í•  ë•Œ ì‹œìŠ¤í…œì´ ìš”ì²­ì„ ë‹¤ì‹œ ë³´ë‚´ë ¤ê³  ì‹œë„í•  ìµœëŒ€ íšŸìˆ˜ì…ë‹ˆë‹¤.</p>
</div>

`init_chat_model`ì„ ì‚¬ìš©í•  ë•Œ, ì´ëŸ¬í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¸ë¼ì¸ `**kwargs`ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤:

#### ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ˆê¸°í™”

```python
model = init_chat_model(
    "claude-sonnet-4-5-20250929",
    # ëª¨ë¸ë¡œ ì „ë‹¬ë˜ëŠ” Kwargs:
    temperature=0.7,
    timeout=30,
    max_tokens=1000,
)
```

> ê° Chat ëª¨ë¸ í†µí•©ì€ ì œê³µìë³„ ê¸°ëŠ¥ì„ ì œì–´í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì¶”ê°€ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> ì˜ˆë¥¼ ë“¤ì–´, `ChatOpenAI`ëŠ” OpenAI Responses API ë˜ëŠ” Completions API ì¤‘ ì–´ëŠ ê²ƒì„ ì‚¬ìš©í• ì§€ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ `use_responses_api`ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
> ì£¼ì–´ì§„ Chat ëª¨ë¸ì´ ì§€ì›í•˜ëŠ” ëª¨ë“  ë§¤ê°œë³€ìˆ˜ë¥¼ ì°¾ìœ¼ë ¤ë©´ [Chat ëª¨ë¸ í†µí•©](https://docs.langchain.com/oss/python/integrations/chat) í˜ì´ì§€ë¡œ ì´ë™í•˜ì„¸ìš”.

## í˜¸ì¶œ

Chat ëª¨ë¸ì€ ì¶œë ¥ì„ ìƒì„±í•˜ê¸° ìœ„í•´ í˜¸ì¶œë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ê°ê° ë‹¤ì–‘í•œ ì‚¬ìš© ì‚¬ë¡€ì— ì í•©í•œ ì„¸ ê°€ì§€ ê¸°ë³¸ í˜¸ì¶œ ë©”ì„œë“œê°€ ìˆìŠµë‹ˆë‹¤.

### Invoke

ëª¨ë¸ì„ í˜¸ì¶œí•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ ë‹¨ì¼ ë©”ì‹œì§€ ë˜ëŠ” ë©”ì‹œì§€ ëª©ë¡ê³¼ í•¨ê»˜ `invoke()`ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

#### ë‹¨ì¼ ë©”ì‹œì§€

```python
response = model.invoke("Why do parrots have colorful feathers?")
print(response)
```

Chat ëª¨ë¸ì— ë©”ì‹œì§€ ëª©ë¡ì„ ì œê³µí•˜ì—¬ ëŒ€í™” ê¸°ë¡ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° ë©”ì‹œì§€ì—ëŠ” ëª¨ë¸ì´ ëŒ€í™”ì—ì„œ ë©”ì‹œì§€ë¥¼ ë³´ë‚¸ ì‚¬ëŒì„ ë‚˜íƒ€ë‚´ëŠ” ì—­í• ì´ ìˆìŠµë‹ˆë‹¤.

ë©”ì‹œì§€ì˜ ì—­í• , ìœ í˜• ë° ë‚´ìš©ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [ë©”ì‹œì§€ ê°€ì´ë“œ](https://docs.langchain.com/oss/python/langchain/models/messages)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

#### ë”•ì…”ë„ˆë¦¬ í˜•ì‹

```python
conversation = [
    {"role": "system", "content": "You are a helpful assistant that translates English to French."},
    {"role": "user", "content": "Translate: I love programming."},
    {"role": "assistant", "content": "J'adore la programmation."},
    {"role": "user", "content": "Translate: I love building applications."}
]

response = model.invoke(conversation)
print(response)  # AIMessage("J'adore crÃ©er des applications.")
```

#### ë©”ì‹œì§€ ê°ì²´

```python
from langchain.messages import HumanMessage, AIMessage, SystemMessage

conversation = [
    SystemMessage("You are a helpful assistant that translates English to French."),
    HumanMessage("Translate: I love programming."),
    AIMessage("J'adore la programmation."),
    HumanMessage("Translate: I love building applications.")
]

response = model.invoke(conversation)
print(response)  # AIMessage("J'adore crÃ©er des applications.")
```

> í˜¸ì¶œì˜ ë°˜í™˜ íƒ€ì…ì´ ë¬¸ìì—´ì¸ ê²½ìš°, LLMì´ ì•„ë‹Œ Chat ëª¨ë¸ì„ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”. ë ˆê±°ì‹œ í…ìŠ¤íŠ¸ ì™„ì„± LLMì€ ë¬¸ìì—´ì„ ì§ì ‘ ë°˜í™˜í•©ë‹ˆë‹¤. LangChain Chat ëª¨ë¸ì€ "Chat"ìœ¼ë¡œ ì‹œì‘í•˜ë©°, ì˜ˆ: [`ChatOpenAI`](https://docs.langchain.com/oss/integrations/chat/openai)

### Stream

ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì€ ìƒì„±ë˜ëŠ” ë™ì•ˆ ì¶œë ¥ ì½˜í…ì¸ ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¶œë ¥ì„ ì ì§„ì ìœ¼ë¡œ í‘œì‹œí•˜ë©´ íŠ¹íˆ ë” ê¸´ ì‘ë‹µì˜ ê²½ìš° ì‚¬ìš©ì í™˜ê²½ì´ ìƒë‹¹íˆ ê°œì„ ë©ë‹ˆë‹¤.

`stream()`ì„ í˜¸ì¶œí•˜ë©´ ìƒì„±ë˜ëŠ” ë™ì•ˆ ì¶œë ¥ ì²­í¬ë¥¼ ìƒì„±í•˜ëŠ” ë°˜ë³µìê°€ ë°˜í™˜ë©ë‹ˆë‹¤. ë£¨í”„ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ì²­í¬ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

#### ê¸°ë³¸ í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°

```python
for chunk in model.stream("Why do parrots have colorful feathers?"):
    print(chunk.text, end="|", flush=True)
```

#### Tool í˜¸ì¶œ, ì¶”ë¡  ë° ê¸°íƒ€ ì½˜í…ì¸  ìŠ¤íŠ¸ë¦¬ë°

```python
for chunk in model.stream("What color is the sky?"):
    for block in chunk.content_blocks:
        if block["type"] == "reasoning" and (reasoning := block.get("reasoning")):
            print(f"Reasoning: {reasoning}")
        elif block["type"] == "tool_call_chunk":
            print(f"Tool call chunk: {block}")
        elif block["type"] == "text":
            print(block["text"])
        else:
            ...
```

`invoke()`ì™€ ë‹¤ë¥´ê²Œ, ì™„ì „í•œ ì‘ë‹µì„ ìƒì„±í•œ í›„ ë‹¨ì¼ `AIMessage`ë¥¼ ë°˜í™˜í•˜ëŠ” ê²ƒê³¼ ë‹¬ë¦¬, `stream()`ì€ ì—¬ëŸ¬ `AIMessageChunk` ê°ì²´ë¥¼ ë°˜í™˜í•˜ë©°, ê°ê°ì€ ì¶œë ¥ í…ìŠ¤íŠ¸ì˜ ì¼ë¶€ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì¤‘ìš”í•˜ê²Œ, ìŠ¤íŠ¸ë¦¼ì˜ ê° ì²­í¬ëŠ” í•©ì‚°ì„ í†µí•´ ì™„ì „í•œ ë©”ì‹œì§€ë¡œ ìˆ˜ì§‘ë˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤:

#### AIMessage êµ¬ì„±

```python
full = None  # None | AIMessageChunk
for chunk in model.stream("What color is the sky?"):
    full = chunk if full is None else full + chunk
    print(full.text)

# The
# The sky
# The sky is
# The sky is typically
# The sky is typically blue
# ...

print(full.content_blocks)
# [{"type": "text", "text": "The sky is typically blue..."}]
```

ê²°ê³¼ ë©”ì‹œì§€ëŠ” `invoke()`ë¡œ ìƒì„±ëœ ë©”ì‹œì§€ì™€ ë™ì¼í•˜ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë©”ì‹œì§€ ê¸°ë¡ì— ì§‘ê³„ë˜ê³  ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¡œ ëª¨ë¸ì— ë‹¤ì‹œ ì „ë‹¬ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> ìŠ¤íŠ¸ë¦¬ë°ì€ í”„ë¡œê·¸ë¨ì˜ ëª¨ë“  ë‹¨ê³„ê°€ ì²­í¬ ìŠ¤íŠ¸ë¦¼ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì•Œê³  ìˆì„ ë•Œë§Œ ì‘ë™í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ì´ ì—†ëŠ” ì‘ìš© í”„ë¡œê·¸ë¨ì€ ì²˜ë¦¬ ì „ì— ì „ì²´ ì¶œë ¥ì„ ë©”ëª¨ë¦¬ì— ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.

<details>
<summary>ê³ ê¸‰ ìŠ¤íŠ¸ë¦¬ë° ì£¼ì œ</summary>

<details>
<summary>ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸</summary>

LangChain Chat ëª¨ë¸ì€ ë˜í•œ `astream_events()`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ë¯¸ë¡ ì  ì´ë²¤íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì´ë²¤íŠ¸ ìœ í˜• ë° ê¸°íƒ€ ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•„í„°ë§ì´ ê°„ë‹¨í•´ì§€ë©°, ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì™„ì „í•œ ë©”ì‹œì§€ê°€ ì§‘ê³„ë©ë‹ˆë‹¤. ì•„ë˜ ì˜ˆì‹œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

```python
async for event in model.astream_events("Hello"):

    if event["event"] == "on_chat_model_start":
        print(f"Input: {event['data']['input']}")

    elif event["event"] == "on_chat_model_stream":
        print(f"Token: {event['data']['chunk'].text}")

    elif event["event"] == "on_chat_model_end":
        print(f"Full message: {event['data']['output'].text}")

    else:
        pass
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```text
Input: Hello
Token: Hi
Token:  there
Token: !
Token:  How
Token:  can
Token:  I
...
Full message: Hi there! How can I help today?
```

ì´ë²¤íŠ¸ ìœ í˜• ë° ê¸°íƒ€ ì„¸ë¶€ ì •ë³´ëŠ” `astream_events()` ì°¸ì¡°ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

</details>

<details>
<summary>"ìë™ ìŠ¤íŠ¸ë¦¬ë°" Chat ëª¨ë¸</summary>

LangChainì€ ëª…ì‹œì ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì§€ ì•Šì•„ë„ íŠ¹ì • ê²½ìš°ì— ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¥¼ ìë™ìœ¼ë¡œ í™œì„±í™”í•˜ì—¬ Chat ëª¨ë¸ë¡œë¶€í„°ì˜ ìŠ¤íŠ¸ë¦¬ë°ì„ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤. ì´ëŠ” íŠ¹íˆ ë¹„ìŠ¤íŠ¸ë¦¬ë° `invoke` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ ì—¬ì „íˆ Chat ëª¨ë¸ì˜ ì¤‘ê°„ ê²°ê³¼ë¥¼ í¬í•¨í•˜ì—¬ ì „ì²´ ì‘ìš© í”„ë¡œê·¸ë¨ì„ ìŠ¤íŠ¸ë¦¬ë°í•˜ë ¤ê³  í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ **LangGraph Agent**ì—ì„œëŠ” ë…¸ë“œ ë‚´ì—ì„œ `model.invoke()`ë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆì§€ë§Œ, LangChainì€ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì—ì„œ ì‹¤í–‰ ì¤‘ì´ë©´ ìë™ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ìœ„ì„í•  ê²ƒì…ë‹ˆë‹¤.

**ì‘ë™ ë°©ì‹**

Chat ëª¨ë¸ì„ `invoke()`í•  ë•Œ, LangChainì€ ì „ì²´ ì‘ìš© í”„ë¡œê·¸ë¨ì„ ìŠ¤íŠ¸ë¦¬ë°í•˜ë ¤ê³  ì‹œë„ ì¤‘ì¸ ê²ƒì„ ê°ì§€í•˜ë©´ ìë™ìœ¼ë¡œ ë‚´ë¶€ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤. í˜¸ì¶œì˜ ê²°ê³¼ëŠ” invokeë¥¼ ì‚¬ìš© ì¤‘ì¸ ì½”ë“œ ê´€ì ì—ì„œëŠ” ë™ì¼í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ Chat ëª¨ë¸ì„ ìŠ¤íŠ¸ë¦¬ë°í•˜ëŠ” ë™ì•ˆ LangChainì€ LangChainì˜ ì½œë°± ì‹œìŠ¤í…œì—ì„œ `on_llm_new_token` ì´ë²¤íŠ¸ë¥¼ í˜¸ì¶œí•˜ëŠ” ê²ƒì„ ì²˜ë¦¬í•  ê²ƒì…ë‹ˆë‹¤.

ì½œë°± ì´ë²¤íŠ¸ëŠ” LangGraph `stream()` ë° `astream_events()`ê°€ Chat ëª¨ë¸ì˜ ì¶œë ¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œí•˜ë„ë¡ í•©ë‹ˆë‹¤.

</details>

</details>

### Batch

ëª¨ë¸ë¡œ ë³´ë‚´ëŠ” ë…ë¦½ì ì¸ ìš”ì²­ ì»¬ë ‰ì…˜ì„ ì¼ê´„ ì²˜ë¦¬í•˜ë©´ ì²˜ë¦¬ê°€ ë³‘ë ¬ë¡œ ìˆ˜í–‰ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ê³  ë¹„ìš©ì´ ê°ì†Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

#### ì¼ê´„ ì²˜ë¦¬

```python
responses = model.batch([
    "Why do parrots have colorful feathers?",
    "How do airplanes fly?",
    "What is quantum computing?"
])
for response in responses:
    print(response)
```

> ì´ ì„¹ì…˜ì€ Chat ëª¨ë¸ ë©”ì„œë“œ `batch()`ì— ëŒ€í•´ ì„¤ëª…í•˜ë©°, í´ë¼ì´ì–¸íŠ¸ ì¸¡ì—ì„œ ëª¨ë¸ í˜¸ì¶œì„ ë³‘ë ¬í™”í•©ë‹ˆë‹¤.
>
> OpenAI ë˜ëŠ” Anthropicê³¼ ê°™ì€ ì¶”ë¡  ì œê³µìê°€ ì§€ì›í•˜ëŠ” Batch APIì™€ëŠ” ë³„ê°œì…ë‹ˆë‹¤.

ê¸°ë³¸ì ìœ¼ë¡œ `batch()`ëŠ” ì „ì²´ Batchì˜ ìµœì¢… ì¶œë ¥ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤. ê° ê°œë³„ ì…ë ¥ì´ ìƒì„±ì„ ì™„ë£Œí•˜ë©´ ì¶œë ¥ì„ ë°›ìœ¼ë ¤ë©´ `batch_as_completed()`ë¡œ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

#### Batch ì‘ë‹µì„ ì™„ë£Œ ì‹œ ìƒì„±

```python
for response in model.batch_as_completed([
    "Why do parrots have colorful feathers?",
    "How do airplanes fly?",
    "What is quantum computing?"
]):
    print(response)
```

> `batch_as_completed()`ë¥¼ ì‚¬ìš©í•  ë•Œ, ê²°ê³¼ê°€ ìˆœì„œë¥¼ ë²—ì–´ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°ê°ì€ í•„ìš”ì— ë”°ë¼ ì›ë˜ ìˆœì„œë¥¼ ì¬êµ¬ì„±í•˜ê¸° ìœ„í•´ ë§¤ì¹­í•  ì…ë ¥ ì¸ë±ìŠ¤ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

> ë§ì€ ìˆ˜ì˜ ì…ë ¥ì„ `batch()` ë˜ëŠ” `batch_as_completed()`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬í•  ë•Œ, ë³‘ë ¬ í˜¸ì¶œì˜ ìµœëŒ€ ìˆ˜ë¥¼ ì œì–´í•˜ë ¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” `RunnableConfig` ë”•ì…”ë„ˆë¦¬ì—ì„œ `max_concurrency` ì†ì„±ì„ ì„¤ì •í•˜ì—¬ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
>
> #### ìµœëŒ€ ë™ì‹œì„±ì„ ì‚¬ìš©í•œ Batch
>
> ```python
> model.batch(
>     list_of_inputs,
>     config={
>         'max_concurrency': 5,  # 5ê°œì˜ ë³‘ë ¬ í˜¸ì¶œë¡œ ì œí•œ
>     }
> )
> ```
>
> ì§€ì›ë˜ëŠ” ëª¨ë“  ì†ì„±ì˜ ì „ì²´ ëª©ë¡ì€ [RunnableConfig ì°¸ì¡°](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.config.RunnableConfig.html)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

ì¼ê´„ ì²˜ë¦¬ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [ì°¸ì¡°](https://python.langchain.com/docs/how_to/#batch)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

## Tool í˜¸ì¶œ

ëª¨ë¸ì€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê¸°, ì›¹ ê²€ìƒ‰ ë˜ëŠ” ì½”ë“œ ì‹¤í–‰ê³¼ ê°™ì€ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” Toolì„ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Toolì€ ë‹¤ìŒìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

1. Toolì˜ ì´ë¦„, ì„¤ëª… ë°/ë˜ëŠ” ì¸ìˆ˜ ì •ì˜(ì¢…ì¢… JSON ìŠ¤í‚¤ë§ˆ)ë¥¼ í¬í•¨í•˜ëŠ” ìŠ¤í‚¤ë§ˆ
2. ì‹¤í–‰í•  í•¨ìˆ˜ ë˜ëŠ” ì½”ë£¨í‹´ì…ë‹ˆë‹¤.

> "í•¨ìˆ˜ í˜¸ì¶œ"ì´ë¼ëŠ” ìš©ì–´ë¥¼ ë“¤ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ë¥¼ "Tool í˜¸ì¶œ"ê³¼ ìƒí˜¸ êµí™˜ì ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì‚¬ìš©ìì™€ ëª¨ë¸ ê°„ì˜ ê¸°ë³¸ Tool í˜¸ì¶œ í”Œë¡œìš°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```mermaid
sequenceDiagram
    participant U as User
    participant M as Model
    participant T as Tools

    U->>M: "What's the weather in SF and NYC?"
    Note over M: Analyze request & decide tools needed

    par [Parallel Tool Calls]
        M->>T: get_weather("San Francisco")
        M->>T: get_weather("New York")
    end

    par [Tool Execution]
        T-->>M: SF weather data
        T-->>M: NYC weather data
    end

    Note over M: Process results & generate response
    M-->>U: "SF: 72Â°F sunny, NYC: 68Â°F cloudy"
```

ì •ì˜í•œ Toolì„ ëª¨ë¸ì´ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ë ¤ë©´ `bind_tools`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°”ì¸ë“œí•´ì•¼ í•©ë‹ˆë‹¤. í›„ì† í˜¸ì¶œì—ì„œ ëª¨ë¸ì€ í•„ìš”ì— ë”°ë¼ ë°”ì¸ë“œëœ Tool ì¤‘ í•˜ë‚˜ë¥¼ í˜¸ì¶œí•˜ë„ë¡ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì¼ë¶€ ëª¨ë¸ ì œê³µìëŠ” ëª¨ë¸ ë˜ëŠ” í˜¸ì¶œ ë§¤ê°œë³€ìˆ˜(ì˜ˆ: `ChatOpenAI`, `ChatAnthropic`)ë¥¼ í†µí•´ í™œì„±í™”í•  ìˆ˜ ìˆëŠ” ê¸°ë³¸ ì œê³µ Toolì„ ì œê³µí•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ê° ì œê³µì ì°¸ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.

> Tool ìƒì„±ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš© ë° ê¸°íƒ€ ì˜µì…˜ì€ [Tool ê°€ì´ë“œ](https://python.langchain.com/docs/how_to/tool_calling/)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

### ì‚¬ìš©ì Tool ë°”ì¸ë”©

```python
from langchain.tools import tool

@tool
def get_weather(location: str) -> str:
    """Get the weather at a location."""
    return f"It's sunny in {location}."


model_with_tools = model.bind_tools([get_weather])

response = model_with_tools.invoke("What's the weather like in Boston?")
for tool_call in response.tool_calls:
    # View tool calls made by the model
    print(f"Tool: {tool_call['name']}")
    print(f"Args: {tool_call['args']}")
```

ì‚¬ìš©ì ì •ì˜ Toolì„ ë°”ì¸ë”©í•  ë•Œ, ëª¨ë¸ì˜ ì‘ë‹µì—ëŠ” Toolì„ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ìš”ì²­ì´ í¬í•¨ë©ë‹ˆë‹¤. Agentì™€ ë³„ë„ë¡œ ëª¨ë¸ì„ ì‚¬ìš©í•  ë•ŒëŠ” ìš”ì²­ëœ Toolì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ëª¨ë¸ë¡œ ë°˜í™˜í•˜ì—¬ í›„ì† ì¶”ë¡ ì— ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë‹¹ì‹ ì˜ ì±…ì„ì…ë‹ˆë‹¤. Agentë¥¼ ì‚¬ìš©í•  ë•Œ, Agent ë£¨í”„ëŠ” Tool ì‹¤í–‰ ë£¨í”„ë¥¼ ë‹¹ì‹ ì„ ìœ„í•´ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ì•„ë˜ì— Tool í˜¸ì¶œì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëª‡ ê°€ì§€ ì¼ë°˜ì ì¸ ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

<details>
<summary>Tool ì‹¤í–‰ ë£¨í”„</summary>

ëª¨ë¸ì´ Tool í˜¸ì¶œì„ ë°˜í™˜í•  ë•Œ, Toolì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ëª¨ë¸ë¡œ ë‹¤ì‹œ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤. ì´ê²ƒì€ ëª¨ë¸ì´ Tool ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” ëŒ€í™” ë£¨í”„ë¥¼ ë§Œë“­ë‹ˆë‹¤. LangChainì€ ì´ ì¡°ì •ì„ ë‹¹ì‹ ì„ ìœ„í•´ ì²˜ë¦¬í•˜ëŠ” **Agent** ì¶”ìƒí™”ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

ë‹¤ìŒì€ ì´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œì…ë‹ˆë‹¤:

```python
# Toolì„ ëª¨ë¸ì— ë°”ì¸ë“œ(ì ì¬ì ìœ¼ë¡œ ì—¬ëŸ¬ ê°œ)
model_with_tools = model.bind_tools([get_weather])

# ë‹¨ê³„ 1: ëª¨ë¸ì´ Tool í˜¸ì¶œ ìƒì„±
messages = [{"role": "user", "content": "What's the weather in Boston?"}]
ai_msg = model_with_tools.invoke(messages)
messages.append(ai_msg)

# ë‹¨ê³„ 2: Toolì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ìˆ˜ì§‘
for tool_call in ai_msg.tool_calls:
    # ìƒì„±ëœ ì¸ìˆ˜ë¡œ Tool ì‹¤í–‰
    tool_result = get_weather.invoke(tool_call)
    messages.append(tool_result)

# ë‹¨ê³„ 3: ê²°ê³¼ë¥¼ ëª¨ë¸ë¡œ ë‹¤ì‹œ ì „ë‹¬í•˜ì—¬ ìµœì¢… ì‘ë‹µ ì–»ê¸°
final_response = model_with_tools.invoke(messages)
print(final_response.text)
# "The current weather in Boston is 72Â°F and sunny."
```
Toolì—ì„œ ë°˜í™˜ëœ ê° `ToolMessage`ì—ëŠ” ì›ë³¸ Tool í˜¸ì¶œê³¼ ì¼ì¹˜í•˜ëŠ” `tool_call_id`ê°€ í¬í•¨ë˜ì–´ ìˆì–´ì„œ, ëª¨ë¸ì´ ê²°ê³¼ë¥¼ ìš”ì²­ê³¼ ì—°ê²°í•˜ë„ë¡ ë•ìŠµë‹ˆë‹¤.

</details>

<details>
<summary>Tool í˜¸ì¶œ ê°•ì œí•˜ê¸°</summary>

ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë¸ì€ ì‚¬ìš©ì ì…ë ¥ì— ë”°ë¼ ë°”ì¸ë“œëœ Tool ì¤‘ ì–´ëŠ ê²ƒì„ ì‚¬ìš©í• ì§€ ì„ íƒí•  ììœ ê°€ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ Tool ì‚¬ìš©ì„ ê°•ì œí•˜ê±°ë‚˜, ëª¨ë¸ì´ íŠ¹ì • Tool ë˜ëŠ” ì£¼ì–´ì§„ ëª©ë¡ì˜ **ì–´ë–¤** Toolì„ ì‚¬ìš©í•˜ë„ë¡ ë³´ì¥í•˜ë ¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

**íƒ­: ì–´ë–¤ Toolì˜ ì‚¬ìš© ê°•ì œ**
```python
model_with_tools = model.bind_tools([tool_1], tool_choice="any")
```

**íƒ­: íŠ¹ì • Toolì˜ ì‚¬ìš© ê°•ì œ**
```python
model_with_tools = model.bind_tools([tool_1], tool_choice="tool_1")
```

</details>

<details>
<summary>ë³‘ë ¬ Tool í˜¸ì¶œ</summary>

ë§ì€ ëª¨ë¸ì´ ì ì ˆí•  ë•Œ ì—¬ëŸ¬ Toolì„ ë³‘ë ¬ë¡œ í˜¸ì¶œí•˜ëŠ” ê²ƒì„ ì§€ì›í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì´ ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ì •ë³´ë¥¼ ë™ì‹œì— ìˆ˜ì§‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
model_with_tools = model.bind_tools([get_weather])

response = model_with_tools.invoke(
    "What's the weather in Boston and Tokyo?"
)


# ëª¨ë¸ì€ ì—¬ëŸ¬ Tool í˜¸ì¶œì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
print(response.tool_calls)
# [
#   {'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'call_1'},
#   {'name': 'get_weather', 'args': {'location': 'Tokyo'}, 'id': 'call_2'},
# ]


# ëª¨ë“  Tool ì‹¤í–‰(ë¹„ë™ê¸°ë¡œ ë³‘ë ¬ë¡œ ìˆ˜í–‰ ê°€ëŠ¥)
results = []
for tool_call in response.tool_calls:
    if tool_call['name'] == 'get_weather':
        result = get_weather.invoke(tool_call)
    ...
    results.append(result)
```

ëª¨ë¸ì€ ìš”ì²­ëœ ì‘ì—…ì˜ ë…ë¦½ì„±ì— ë”°ë¼ ë³‘ë ¬ ì‹¤í–‰ì´ ì ì ˆí•œ ì‹œê¸°ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ê²°ì •í•©ë‹ˆë‹¤.

> [!TIP]
> Tool í˜¸ì¶œì„ ì§€ì›í•˜ëŠ” ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ ë³‘ë ¬ Tool í˜¸ì¶œì„ í™œì„±í™”í•©ë‹ˆë‹¤. ì¼ë¶€(OpenAI ë° Anthropic í¬í•¨)ëŠ” ì´ ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•˜ë„ë¡ í—ˆìš©í•©ë‹ˆë‹¤. ì´ë¥¼ ìˆ˜í–‰í•˜ë ¤ë©´ `parallel_tool_calls=False`ë¥¼ ì„¤ì •í•˜ì„¸ìš”:
>
> ```python
> model.bind_tools([get_weather], parallel_tool_calls=False)
> ```

</details>

<details>
<summary>Tool í˜¸ì¶œ ìŠ¤íŠ¸ë¦¬ë°</summary>

ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°í•  ë•Œ, Tool í˜¸ì¶œì€ ToolCallChunkë¥¼ í†µí•´ ì ì§„ì ìœ¼ë¡œ êµ¬ì¶•ë©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì™„ì „í•œ ì‘ë‹µì„ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  Tool í˜¸ì¶œì´ ìƒì„±ë˜ëŠ” ë™ì•ˆ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
for chunk in model_with_tools.stream(
    "What's the weather in Boston and Tokyo?"
):
    # Tool í˜¸ì¶œ ì²­í¬ê°€ ì ì§„ì ìœ¼ë¡œ ë„ì°©í•©ë‹ˆë‹¤
    for tool_chunk in chunk.tool_call_chunks:
        if name := tool_chunk.get("name"):
            print(f"Tool: {name}")
        if id_ := tool_chunk.get("id"):
            print(f"ID: {id_}")
        if args := tool_chunk.get("args"):
            print(f"Args: {args}")

# ì¶œë ¥:
# Tool: get_weather
# ID: call_SvMlU1TVIZugrFLckFE2ceRE
# Args: {"lo
# Args: catio
# Args: n": "B
# Args: osto
# Args: n"}
# Tool: get_weather
# ID: call_QMZdy6qInx13oWKE7KhuhOLR
# Args: {"lo
# Args: catio
# Args: n": "T
# Args: okyo
# Args: "}
```

ì²­í¬ë¥¼ ëˆ„ì í•˜ì—¬ ì™„ì „í•œ Tool í˜¸ì¶œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

**Tool í˜¸ì¶œ ëˆ„ì **

```python
gathered = None
for chunk in model_with_tools.stream("What's the weather in Boston?"):
    gathered = chunk if gathered is None else gathered + chunk
    print(gathered.tool_calls)
```

</details>

## êµ¬ì¡°í™”ëœ ì¶œë ¥

ëª¨ë¸ì€ ì£¼ì–´ì§„ ìŠ¤í‚¤ë§ˆì™€ ì¼ì¹˜í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ì‘ë‹µì„ ì œê³µí•˜ë„ë¡ ìš”ì²­ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì¶œë ¥ì„ ì‰½ê²Œ êµ¬ë¬¸ ë¶„ì„í•˜ê³  í›„ì† ì²˜ë¦¬ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤. LangChainì€ êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ì ìš©í•˜ê¸° ìœ„í•œ ì—¬ëŸ¬ ìŠ¤í‚¤ë§ˆ ìœ í˜• ë° ë©”ì„œë“œë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

êµ¬ì¡°í™”ëœ ì¶œë ¥ì— ëŒ€í•´ ì•Œì•„ë³´ë ¤ë©´ [êµ¬ì¡°í™”ëœ ì¶œë ¥](https://python.langchain.com/docs/how_to/structured_output/)ì„ ì°¸ì¡°í•˜ì„¸ìš”.

#### Pydantic

Pydantic ëª¨ë¸ì€ í•„ë“œ ìœ íš¨ì„± ê²€ì‚¬, ì„¤ëª… ë° ì¤‘ì²© êµ¬ì¡°ë¡œ ê°€ì¥ í’ë¶€í•œ ê¸°ëŠ¥ ì§‘í•©ì„ ì œê³µí•©ë‹ˆë‹¤.

```python
from pydantic import BaseModel, Field

class Movie(BaseModel):
    """A movie with details."""
    title: str = Field(..., description="The title of the movie")
    year: int = Field(..., description="The year the movie was released")
    director: str = Field(..., description="The director of the movie")
    rating: float = Field(..., description="The movie's rating out of 10")

model_with_structure = model.with_structured_output(Movie)
response = model_with_structure.invoke("Provide details about the movie Inception")
print(response)  # Movie(title="Inception", year=2010, director="Christopher Nolan", rating=8.8)
```

#### TypedDict

Pythonì˜ TypedDictëŠ” ëŸ°íƒ€ì„ ìœ íš¨ì„± ê²€ì‚¬ê°€ í•„ìš”í•˜ì§€ ì•Šì„ ë•Œ Pydantic ëª¨ë¸ì— ë” ê°„ë‹¨í•œ ëŒ€ì•ˆì„ ì œê³µí•©ë‹ˆë‹¤.

```python
from typing_extensions import TypedDict, Annotated

class MovieDict(TypedDict):
    """A movie with details."""
    title: Annotated[str, "The title of the movie"]
    year: Annotated[int, "The year the movie was released"]
    director: Annotated[str, "The director of the movie"]
    rating: Annotated[float, "The movie's rating out of 10"]

model_with_structure = model.with_structured_output(MovieDict)
response = model_with_structure.invoke("Provide details about the movie Inception")
print(response)  # {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}
```

#### JSON ìŠ¤í‚¤ë§ˆ

ìµœëŒ€ ì œì–´ ë° ìƒí˜¸ ìš´ìš©ì„±ì„ ìœ„í•´ JSON ìŠ¤í‚¤ë§ˆë¥¼ ì œê³µí•©ë‹ˆë‹¤.

```python
import json

json_schema = {
  "title": "Movie",
  "description": "A movie with details",
  "type": "object",
  "properties": {
    "title": { "type": "string", "description": "The title of the movie" },
    "year": { "type": "integer", "description": "The year the movie was released" },
    "director": { "type": "string", "description": "The director of the movie" },
    "rating": { "type": "number", "description": "The movie's rating out of 10" }
  },
  "required": ["title", "year", "director", "rating"]
}

model_with_structure = model.with_structured_output(
  json_schema,
  method="json_schema",
)
response = model_with_structure.invoke("Provide details about the movie Inception")
print(response)  # {'title': 'Inception', 'year': 2010, ...}
```

> [!INFO]
> **êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ìœ„í•œ ì£¼ìš” ê³ ë ¤ ì‚¬í•­**
>
> *   **ë©”ì„œë“œ ë§¤ê°œë³€ìˆ˜**: ì¼ë¶€ ì œê³µìëŠ” êµ¬ì¡°í™”ëœ ì¶œë ¥ì— ëŒ€í•´ ë‹¤ì–‘í•œ ë©”ì„œë“œë¥¼ ì§€ì›í•©ë‹ˆë‹¤:
>     *   `'json_schema'` : ì œê³µìê°€ ì œê³µí•˜ëŠ” ì „ìš© êµ¬ì¡°í™”ëœ ì¶œë ¥ ê¸°ëŠ¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
>     *   `'function_calling'` : ì£¼ì–´ì§„ ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¥´ëŠ” [Tool í˜¸ì¶œ](https://docs.langchain.com/oss/python/langchain/models#tool-calling)ì„ ê°•ì œí•˜ì—¬ êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ë„ì¶œí•©ë‹ˆë‹¤.
>     *   `'json_mode'` : ì¼ë¶€ ì œê³µìê°€ ì œê³µí•˜ëŠ” `'json_schema'`ì˜ ì„ êµ¬ìì…ë‹ˆë‹¤. ìœ íš¨í•œ JSONì„ ìƒì„±í•˜ì§€ë§Œ ìŠ¤í‚¤ë§ˆëŠ” í”„ë¡¬í”„íŠ¸ì— ì„¤ëª…ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
> *   **Raw í¬í•¨**: `include_raw=True`ë¡œ ì„¤ì •í•˜ë©´ íŒŒì‹±ëœ ì¶œë ¥ê³¼ ì›ë³¸ `AIMessage`ë¥¼ ëª¨ë‘ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> *   **ìœ íš¨ì„± ê²€ì‚¬**: Pydantic ëª¨ë¸ì€ ìë™ ìœ íš¨ì„± ê²€ì‚¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤. `TypedDict` ë° JSON ìŠ¤í‚¤ë§ˆëŠ” ìˆ˜ë™ ìœ íš¨ì„± ê²€ì‚¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.
>
> [ì œê³µìì˜ í†µí•© í˜ì´ì§€](https://docs.langchain.com/oss/python/integrations/providers/overview)ì—ì„œ ì§€ì›ë˜ëŠ” ë©”ì„œë“œ ë° êµ¬ì„± ì˜µì…˜ì„ ì°¸ì¡°í•˜ì„¸ìš”.

<details>
<summary>ì˜ˆì‹œ: íŒŒì‹±ëœ êµ¬ì¡°ì™€ í•¨ê»˜ ë©”ì‹œì§€ ì¶œë ¥</summary>

ì›ë³¸ AIMessage ê°ì²´ë¥¼ íŒŒì‹±ëœ í‘œí˜„ê³¼ í•¨ê»˜ ë°˜í™˜í•˜ë©´ í† í° ìˆ˜ì™€ ê°™ì€ ì‘ë‹µ ë©”íƒ€ë°ì´í„°ì— ì ‘ê·¼í•˜ëŠ” ê²ƒì´ ìœ ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ìˆ˜í–‰í•˜ë ¤ë©´ with_structured_outputì„ í˜¸ì¶œí•  ë•Œ `include_raw=True`ë¥¼ ì„¤ì •í•˜ì„¸ìš”:

```python
from pydantic import BaseModel, Field

class Movie(BaseModel):
    """A movie with details."""
    title: str = Field(..., description="The title of the movie")
    year: int = Field(..., description="The year the movie was released")
    director: str = Field(..., description="The director of the movie")
    rating: float = Field(..., description="The movie's rating out of 10")

model_with_structure = model.with_structured_output(Movie, include_raw=True)
response = model_with_structure.invoke("Provide details about the movie Inception")
response
# {
#     "raw": AIMessage(...),
#     "parsed": Movie(title=..., year=..., ...),
#     "parsing_error": None,
# }
```

</details>

<details>
<summary>ì˜ˆì‹œ: ì¤‘ì²© êµ¬ì¡°</summary>

ìŠ¤í‚¤ë§ˆëŠ” ì¤‘ì²©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

**íƒ­: Pydantic**
```python
from pydantic import BaseModel, Field

class Actor(BaseModel):
    name: str
    role: str

class MovieDetails(BaseModel):
    title: str
    year: int
    cast: list[Actor]
    genres: list[str]
    budget: float | None = Field(None, description="Budget in millions USD")

model_with_structure = model.with_structured_output(MovieDetails)
```

**íƒ­: TypedDict**
```python
from typing_extensions import Annotated, TypedDict

class Actor(TypedDict):
    name: str
    role: str

class MovieDetails(TypedDict):
    title: str
    year: int
    cast: list[Actor]
    genres: list[str]
    budget: Annotated[float | None, ..., "Budget in millions USD"]

model_with_structure = model.with_structured_output(MovieDetails)
```

</details>

## ê³ ê¸‰ ì£¼ì œ

### ëª¨ë¸ í”„ë¡œí•„

> ëª¨ë¸ í”„ë¡œí•„ì€ `langchain>=1.1`ì´ í•„ìš”í•©ë‹ˆë‹¤.

LangChain Chat ëª¨ë¸ì€ `.profile` ì†ì„±ì„ í†µí•´ ì§€ì›ë˜ëŠ” ê¸°ëŠ¥ê³¼ ëŠ¥ë ¥ì˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ë…¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
model.profile
# {
#   "max_input_tokens": 400000,
#   "image_inputs": True,
#   "reasoning_output": True,
#   "tool_calling": True,
#   ...
# }
```

[API ì°¸ì¡°](https://api.python.langchain.com/en/latest/chat_models/langchain_core.chat_models.base.BaseChatModel.html#langchain_core.chat_models.base.BaseChatModel.profile)ì—ì„œ í•„ë“œì˜ ì „ì²´ ì§‘í•©ì„ ì°¸ì¡°í•˜ì„¸ìš”.

ëª¨ë¸ í”„ë¡œí•„ ë°ì´í„°ì˜ ëŒ€ë¶€ë¶„ì€ ëª¨ë¸ ê¸°ëŠ¥ ë°ì´í„°ë¥¼ ì œê³µí•˜ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ ì´ë‹ˆì…”í‹°ë¸Œì¸ [models.dev](https://models.dev/) í”„ë¡œì íŠ¸ë¡œ êµ¬ë™ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°ì´í„°ëŠ” LangChain ì‚¬ìš© ëª©ì ì„ ìœ„í•´ ì¶”ê°€ í•„ë“œë¡œ ë³´ê°•ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë³´ê°•ì€ ì—…ìŠ¤íŠ¸ë¦¼ í”„ë¡œì íŠ¸ê°€ ì§„í™”í•¨ì— ë”°ë¼ ì •ë ¬ë©ë‹ˆë‹¤.

ëª¨ë¸ í”„ë¡œí•„ ë°ì´í„°ëŠ” ì‘ìš© í”„ë¡œê·¸ë¨ì´ ëª¨ë¸ ê¸°ëŠ¥ì„ ë™ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´:
1. ìš”ì•½ MiddlewareëŠ” ëª¨ë¸ì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸°ì— ë”°ë¼ ìš”ì•½ì„ íŠ¸ë¦¬ê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. `create_agent`ì˜ êµ¬ì¡°í™”ëœ ì¶œë ¥ ì „ëµì€ ìë™ìœ¼ë¡œ ì¶”ë¡ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì˜ˆ: ê¸°ë³¸ êµ¬ì¡°í™”ëœ ì¶œë ¥ ê¸°ëŠ¥ ì§€ì›ì„ í™•ì¸í•˜ì—¬).
3. ëª¨ë¸ ì…ë ¥ì€ ì§€ì›ë˜ëŠ” ì–‘ì‹ ë° ìµœëŒ€ ì…ë ¥ í† í°ì— ë”°ë¼ ê²Œì´íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<details>
<summary>í”„ë¡œí•„ ë°ì´í„° ì—…ë°ì´íŠ¸ ë˜ëŠ” ë®ì–´ì“°ê¸°</summary>

ëª¨ë¸ í”„ë¡œí•„ ë°ì´í„°ê°€ ëˆ„ë½ë˜ì—ˆê±°ë‚˜ ì˜¤ë˜ë˜ì—ˆê±°ë‚˜ ë¶€ì •í™•í•œ ê²½ìš° ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì˜µì…˜ 1(ë¹ ë¥¸ ìˆ˜ì •)**

ëª¨ë“  ìœ íš¨í•œ í”„ë¡œí•„ë¡œ Chat ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
custom_profile = {
    "max_input_tokens": 100_000,
    "tool_calling": True,
    "structured_output": True,
    # ...
}
model = init_chat_model("...", profile=custom_profile)
```
í”„ë¡œí•„ì€ ë˜í•œ ì¼ë°˜ ë”•ì…”ë„ˆë¦¬ì´ë©° ì œìë¦¬ì—ì„œ ì—…ë°ì´íŠ¸ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ê°€ ê³µìœ ë˜ëŠ” ê²½ìš°, ê³µìœ  ìƒíƒœë¥¼ ë³€ê²½í•˜ì§€ ì•Šê¸° ìœ„í•´ model_copy ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”.

```python
new_profile = model.profile | {"key": "value"}
model.model_copy(update={"profile": new_profile})
```

**ì˜µì…˜ 2(ì—…ìŠ¤íŠ¸ë¦¼ì—ì„œ ë°ì´í„° ìˆ˜ì •)**
ë°ì´í„°ì˜ ê¸°ë³¸ ì†ŒìŠ¤ëŠ” **models.dev** í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ëŠ” ì¶”ê°€ í•„ë“œì™€ LangChain **í†µí•© íŒ¨í‚¤ì§€**ì˜ ì¬ì •ì˜ì™€ ë³‘í•©ë˜ë©° ì´ëŸ¬í•œ íŒ¨í‚¤ì§€ì™€ í•¨ê»˜ ì œê³µë©ë‹ˆë‹¤.

ëª¨ë¸ í”„ë¡œí•„ ë°ì´í„°ëŠ” ë‹¤ìŒ í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ì—…ë°ì´íŠ¸ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
1. (í•„ìš”í•œ ê²½ìš°) **models.dev** GitHub **ë¦¬í¬ì§€í† ë¦¬**ì— ëŒ€í•œ ëŒì–´ì˜¤ê¸° ìš”ì²­ì„ í†µí•´ ì†ŒìŠ¤ ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
2. (í•„ìš”í•œ ê²½ìš°) LangChain **í†µí•© íŒ¨í‚¤ì§€**ì— ëŒ€í•œ ëŒì–´ì˜¤ê¸° ìš”ì²­ì„ í†µí•´ `langchain_<package>/data/profile_augmentations.toml`ì˜ ì¶”ê°€ í•„ë“œì™€ ì¬ì •ì˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
3. `langchain-model-profiles` CLI ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ **models.dev**ì—ì„œ ìµœì‹  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê³ , ë³´ê°•ì„ ë³‘í•©í•˜ê³  í”„ë¡œí•„ ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤:

```bash
pip install langchain-model-profiles
langchain-profiles refresh --provider <provider> --data-dir <data_dir>
```

ì´ ëª…ë ¹:
- models.devì—ì„œ <provider>ì— ëŒ€í•œ ìµœì‹  ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤
- <data_dir>ì˜ profile_augmentations.tomlì—ì„œ ë³´ê°•ì„ ë³‘í•©í•©ë‹ˆë‹¤
- ë³‘í•©ëœ í”„ë¡œí•„ì„ <data_dir>ì˜ profiles.pyì— ì‘ì„±í•©ë‹ˆë‹¤

ì˜ˆ: LangChain ëª¨ë†€ë¦¬ì‹ ë¦¬í¬ì—ì„œ libs/partners/anthropic:

```bash
uv run --with langchain-model-profiles --provider anthropic --data-dir langchain_anthropic/data
```

</details>

> ëª¨ë¸ í”„ë¡œí•„ì€ ë² íƒ€ ê¸°ëŠ¥ì…ë‹ˆë‹¤. í”„ë¡œí•„ í˜•ì‹ì€ ë³€ê²½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ë©€í‹°ëª¨ë‹¬

íŠ¹ì • ëª¨ë¸ì€ ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë° ë¹„ë””ì˜¤ì™€ ê°™ì€ ë¹„í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  ë°˜í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì½˜í…ì¸  ë¸”ë¡ì„ ì œê³µí•˜ì—¬ ë¹„í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ëª¨ë¸ë¡œ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> ê¸°ë³¸ ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ì´ ìˆëŠ” ëª¨ë“  LangChain Chat ëª¨ë¸ì€ ë‹¤ìŒì„ ì§€ì›í•©ë‹ˆë‹¤:
> - êµì°¨ ì œê³µì í‘œì¤€ í˜•ì‹ì˜ ë°ì´í„°(ìš°ë¦¬ì˜ [ë©”ì‹œì§€ ê°€ì´ë“œ](https://docs.langchain.com/oss/python/langchain/models/messages) ì°¸ì¡°)
> - OpenAI Chat ì™„ë£Œ í˜•ì‹
> - í•´ë‹¹ íŠ¹ì • ì œê³µìì— ê¸°ë³¸ì´ ë˜ëŠ” ëª¨ë“  í˜•ì‹(ì˜ˆ: Anthropic ëª¨ë¸ì€ Anthropic ê¸°ë³¸ í˜•ì‹ì„ ìˆ˜ìš©í•©ë‹ˆë‹¤)

ë©”ì‹œì§€ ê°€ì´ë“œì˜ [ë©€í‹°ëª¨ë‹¬ ì„¹ì…˜](https://docs.langchain.com/oss/python/langchain/models/messages#multimodal)ì—ì„œ ìì„¸í•œ ë‚´ìš©ì„ ì°¸ì¡°í•˜ì„¸ìš”.

ì¼ë¶€ ëª¨ë¸ì€ ì‘ë‹µì˜ ì¼ë¶€ë¡œ ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ë¥¼ ë°˜í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í˜¸ì¶œë˜ë„ë¡ í•œ ê²½ìš°, ê²°ê³¼ `AIMessage`ëŠ” ë©€í‹°ëª¨ë‹¬ ìœ í˜•ì˜ ì½˜í…ì¸  ë¸”ë¡ì„ ê°€ì§ˆ ê²ƒì…ë‹ˆë‹¤.

#### ë©€í‹°ëª¨ë‹¬ ì¶œë ¥

```python
response = model.invoke("Create a picture of a cat")
print(response.content_blocks)
# [
#     {"type": "text", "text": "Here's a picture of a cat"},
#     {"type": "image", "base64": "...", "mime_type": "image/jpeg"},
# ]
```

íŠ¹ì • ì œê³µìì— ëŒ€í•œ ì‚¬ìš© ê°€ëŠ¥í•œ Tool ë° ì‚¬ìš© ì„¸ë¶€ ì •ë³´ëŠ” [í†µí•© í˜ì´ì§€](https://docs.langchain.com/oss/python/integrations/providers/overview)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

### ì¶”ë¡ 

ë§ì€ ëª¨ë¸ì´ ë³µì¡í•œ ë¬¸ì œë¥¼ ë” ì‘ê³  ê´€ë¦¬ ê°€ëŠ¥í•œ ë‹¨ê³„ë¡œ ë¶„í•´í•˜ê¸° ìœ„í•´ ë‹¤ë‹¨ê³„ ì¶”ë¡ ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ëª¨ë¸ì— ì˜í•´ ì§€ì›ë˜ëŠ” ê²½ìš°, ì´ ì¶”ë¡  í”„ë¡œì„¸ìŠ¤ë¥¼ í‘œì‹œí•˜ì—¬ ëª¨ë¸ì´ ìµœì¢… ë‹µë³€ì— ë„ë‹¬í•œ ë°©ë²•ì„ ë” ì˜ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### ì¶”ë¡  ì¶œë ¥ ìŠ¤íŠ¸ë¦¬ë°

```python
for chunk in model.stream("Why do parrots have colorful feathers?"):
    reasoning_steps = [r for r in chunk.content_blocks if r["type"] == "reasoning"]
    print(reasoning_steps if reasoning_steps else chunk.text)
```

#### ì™„ì „í•œ ì¶”ë¡  ì¶œë ¥

```python
response = model.invoke("Why do parrots have colorful feathers?")
reasoning_steps = [b for b in response.content_blocks if b["type"] == "reasoning"]
print(" ".join(step["reasoning"] for step in reasoning_steps))
```

ëª¨ë¸ì— ë”°ë¼, ë•Œë•Œë¡œ ì¶”ë¡ ì— íˆ¬ì…í•´ì•¼ í•  ë…¸ë ¥ì˜ ìˆ˜ì¤€ì„ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ ì‚¬í•˜ê²Œ, ëª¨ë¸ì´ ì¶”ë¡ ì„ ì™„ì „íˆ ë¹„í™œì„±í™”í•˜ë„ë¡ ìš”ì²­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë²”ì£¼í˜• ì¶”ë¡  "ê³„ì¸µ"(ì˜ˆ: `'low'` ë˜ëŠ” `'high'`) ë˜ëŠ” ì •ìˆ˜ í† í° ì˜ˆì‚°ì˜ í˜•íƒœë¥¼ ì·¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ìì„¸í•œ ë‚´ìš©ì€ respective Chat ëª¨ë¸ì˜ [í†µí•© í˜ì´ì§€](https://docs.langchain.com/oss/python/integrations/providers/overview) ë˜ëŠ” ì°¸ì¡°ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

### ë¡œì»¬ ëª¨ë¸

LangChainì€ ìì‹ ì˜ í•˜ë“œì›¨ì–´ì—ì„œ ëª¨ë¸ì„ ë¡œì»¬ë¡œ ì‹¤í–‰í•˜ëŠ” ê²ƒì„ ì§€ì›í•©ë‹ˆë‹¤. ì´ëŠ” ë°ì´í„° í”„ë¼ì´ë²„ì‹œê°€ ì¤‘ìš”í•œ ì‹œë‚˜ë¦¬ì˜¤, ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ì„ í˜¸ì¶œí•˜ë ¤ê³  í•  ë•Œ, ë˜ëŠ” í´ë¼ìš°ë“œ ê¸°ë°˜ ëª¨ë¸ì„ ì‚¬ìš©í•  ë•Œ ë°œìƒí•˜ëŠ” ë¹„ìš©ì„ í”¼í•˜ë ¤ê³  í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.

[Ollama](https://docs.langchain.com/oss/python/integrations/chat/ollama)ëŠ” Chat ë° ì„ë² ë”© ëª¨ë¸ì„ ë¡œì»¬ë¡œ ì‹¤í–‰í•˜ëŠ” ê°€ì¥ ì‰¬ìš´ ë°©ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.

### í”„ë¡¬í”„íŠ¸ ìºì‹±

ë§ì€ ì œê³µìëŠ” ë°˜ë³µ ì²˜ë¦¬ì˜ ë™ì¼í•œ í† í°ì—ì„œ ì§€ì—° ì‹œê°„ê³¼ ë¹„ìš©ì„ ì¤„ì´ê¸° ìœ„í•´ í”„ë¡¬í”„íŠ¸ ìºì‹± ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ëŠ¥ì€ ì•”ë¬µì ì´ê±°ë‚˜ ëª…ì‹œì ì…ë‹ˆë‹¤:

- **ì•”ë¬µì  í”„ë¡¬í”„íŠ¸ ìºì‹±**: ì œê³µìëŠ” ìš”ì²­ì´ ìºì‹œì— ë„ë‹¬í•˜ë©´ ìë™ìœ¼ë¡œ ë¹„ìš© ì ˆê°ì„ ì „ë‹¬í•©ë‹ˆë‹¤. ì˜ˆ: OpenAI ë° Gemini.
- **ëª…ì‹œì  ìºì‹±**: ì œê³µìëŠ” ë” í° ì œì–´ ë˜ëŠ” ë¹„ìš© ì ˆê°ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ ìºì‹œ í¬ì¸íŠ¸ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë„ë¡ í—ˆìš©í•©ë‹ˆë‹¤. ì˜ˆ:
    - `ChatOpenAI` (via `prompt_cache_key`)
    - Anthropicì˜ `AnthropicPromptCachingMiddleware`
    - Gemini.
    - AWS Bedrock

> í”„ë¡¬í”„íŠ¸ ìºì‹±ì€ ì¢…ì¢… ìµœì†Œ ì…ë ¥ í† í° ì„ê³„ê°’ ìœ„ì—ì„œë§Œ engagedë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ì œê³µì í˜ì´ì§€ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

ìºì‹œ ì‚¬ìš©ì€ ëª¨ë¸ ì‘ë‹µì˜ ì‚¬ìš©ëŸ‰ ë©”íƒ€ë°ì´í„°ì— ë°˜ì˜ë©ë‹ˆë‹¤.

### ì„œë²„ ì¸¡ Tool ì‚¬ìš©

ì¼ë¶€ ì œê³µìëŠ” ì„œë²„ ì¸¡ Tool í˜¸ì¶œ ë£¨í”„ë¥¼ ì§€ì›í•©ë‹ˆë‹¤: ëª¨ë¸ì€ ì›¹ ê²€ìƒ‰, ì½”ë“œ ì¸í„°í”„ë¦¬í„° ë° ê¸°íƒ€ Toolê³¼ ìƒí˜¸ ì‘ìš©í•˜ê³  ë‹¨ì¼ ëŒ€í™” í„´ì—ì„œ ê²°ê³¼ë¥¼ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ëª¨ë¸ì´ Toolì„ ì„œë²„ ì¸¡ì—ì„œ í˜¸ì¶œí•˜ë©´, ì‘ë‹µ ë©”ì‹œì§€ì˜ ì½˜í…ì¸ ì—ëŠ” Toolì˜ í˜¸ì¶œ ë° ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì½˜í…ì¸ ê°€ í¬í•¨ë©ë‹ˆë‹¤. ì‘ë‹µì˜ ì½˜í…ì¸  ë¸”ë¡ì— ì ‘ê·¼í•˜ë©´ ì œê³µìì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” í˜•ì‹ìœ¼ë¡œ ì„œë²„ ì¸¡ Tool í˜¸ì¶œ ë° ê²°ê³¼ê°€ ë°˜í™˜ë©ë‹ˆë‹¤:

#### ì„œë²„ ì¸¡ Tool ì‚¬ìš©ìœ¼ë¡œ í˜¸ì¶œ

```python
from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-4.1-mini")

tool = {"type": "web_search"}
model_with_tools = model.bind_tools([tool])

response = model_with_tools.invoke("What was a positive news story from today?")
print(response.content_blocks)
```

**ê²°ê³¼**

```python
[
    {
        "type": "server_tool_call",
        "name": "web_search",
        "args": {
            "query": "positive news stories today",
            "type": "search"
        },
        "id": "ws_abc123"
    },
    {
        "type": "server_tool_result",
        "tool_call_id": "ws_abc123",
        "status": "success"
    },
    {
        "type": "text",
        "text": "Here are some positive news stories from today...",
        "annotations": [
            {
                "end_index": 410,
                "start_index": 337,
                "title": "article title",
                "type": "citation",
                "url": "..."
            }
        ]
    }
]
```

ì´ê²ƒì€ ë‹¨ì¼ ëŒ€í™” í„´ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. í´ë¼ì´ì–¸íŠ¸ ì¸¡ Tool í˜¸ì¶œê³¼ ê°™ì´ ì „ë‹¬ë˜ì–´ì•¼ í•˜ëŠ” ì—°ê´€ëœ `ToolMessage` ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤.

ì£¼ì–´ì§„ ì œê³µìì— ëŒ€í•œ í†µí•© í˜ì´ì§€ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ Tool ë° ì‚¬ìš© ì„¸ë¶€ ì •ë³´ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

### ì†ë„ ì œí•œ

ë§ì€ Chat ëª¨ë¸ ì œê³µìëŠ” ì£¼ì–´ì§„ ì‹œê°„ ê¸°ê°„ ë‚´ì— ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” í˜¸ì¶œ ìˆ˜ì— ì œí•œì„ ë¶€ê³¼í•©ë‹ˆë‹¤. ì†ë„ ì œí•œì— ë„ë‹¬í•˜ë©´, ì¼ë°˜ì ìœ¼ë¡œ ì œê³µìë¡œë¶€í„° ì†ë„ ì œí•œ ì˜¤ë¥˜ ì‘ë‹µì„ ë°›ì„ ê²ƒì´ë©°, ë” ë§ì€ ìš”ì²­ì„ í•˜ê¸° ì „ì— ê¸°ë‹¤ë ¤ì•¼ í•©ë‹ˆë‹¤.

ì†ë„ ì œí•œì„ ê´€ë¦¬í•˜ëŠ” ê²ƒì„ ë•ê¸° ìœ„í•´, Chat ëª¨ë¸ í†µí•©ì€ ì´ˆê¸°í™” ì¤‘ì— ì œê³µí•  ìˆ˜ ìˆëŠ” `rate_limiter` ë§¤ê°œë³€ìˆ˜ë¥¼ ìˆ˜ìš©í•©ë‹ˆë‹¤. ì´ëŠ” ìš”ì²­ì´ ìˆ˜í–‰ë˜ëŠ” ì†ë„ë¥¼ ì œì–´í•©ë‹ˆë‹¤.

<details>
<summary>ì†ë„ ì œí•œê¸°ë¥¼ ì´ˆê¸°í™”í•˜ê³  ì‚¬ìš©</summary>

LangChainì—ëŠ” (ì„ íƒ ì‚¬í•­) ê¸°ë³¸ ì œê³µ `InMemoryRateLimiter`ê°€ í•¨ê»˜ ì œê³µë©ë‹ˆë‹¤. ì´ ì œí•œê¸°ëŠ” ìŠ¤ë ˆë“œ ì•ˆì „í•˜ë©° ë™ì¼í•œ í”„ë¡œì„¸ìŠ¤ì˜ ì—¬ëŸ¬ ìŠ¤ë ˆë“œì— ì˜í•´ ê³µìœ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from langchain_core.rate_limiters import InMemoryRateLimiter

rate_limiter = InMemoryRateLimiter(
    requests_per_second=0.1,  # 1 request every 10s
    check_every_n_seconds=0.1,  # Check every 100ms whether allowed to make a request
    max_bucket_size=10,  # Controls the maximum burst size.
)

model = init_chat_model(
    model="gpt-5",
    model_provider="openai",
    rate_limiter=rate_limiter
)
```

> [!WARNING]
> ì œê³µëœ ì†ë„ ì œí•œê¸°ëŠ” ë‹¨ìœ„ ì‹œê°„ ë‹¹ ìš”ì²­ ìˆ˜ë¥¼ ì œí•œí•  ìˆ˜ë§Œ ìˆìŠµë‹ˆë‹¤. ìš”ì²­ì˜ í¬ê¸°ì— ë”°ë¼ ì œí•œí•´ì•¼ í•˜ëŠ” ê²½ìš° ë„ì›€ì´ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

</details>

### Base URL ë˜ëŠ” í”„ë¡ì‹œ

ë§ì€ Chat ëª¨ë¸ í†µí•©ì˜ ê²½ìš°, API ìš”ì²­ì— ëŒ€í•´ ê¸°ë³¸ URLì„ êµ¬ì„±í•  ìˆ˜ ìˆìœ¼ë©°, ì´ë¥¼ í†µí•´ OpenAI í˜¸í™˜ APIë¥¼ ê°€ì§„ ëª¨ë¸ ì œê³µìë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ í”„ë¡ì‹œ ì„œë²„ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<details>
<summary>Base URL</summary>

ë§ì€ ëª¨ë¸ ì œê³µìê°€ OpenAI í˜¸í™˜ APIë¥¼ ì œê³µí•©ë‹ˆë‹¤(ì˜ˆ: Together AI, vLLM). ì ì ˆí•œ `base_url` ë§¤ê°œë³€ìˆ˜ë¥¼ ì§€ì •í•˜ì—¬ ì´ëŸ¬í•œ ì œê³µìì™€ `init_chat_model`ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
model = init_chat_model(
    model="MODEL_NAME",
    model_provider="openai",
    base_url="BASE_URL",
    api_key="YOUR_API_KEY",
)
```
> *ì°¸ê³ : ì§ì ‘ Chat ëª¨ë¸ í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤í™”ë¥¼ ì‚¬ìš©í•  ë•Œ, ë§¤ê°œë³€ìˆ˜ ì´ë¦„ì€ ì œê³µìë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì„¸ë¶€ ì‚¬í•­ì€ respective ì°¸ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.*

</details>

<details>
<summary>í”„ë¡ì‹œ êµ¬ì„±</summary>

HTTP í”„ë¡ì‹œê°€ í•„ìš”í•œ ë°°í¬ì˜ ê²½ìš°, ì¼ë¶€ ëª¨ë¸ í†µí•©ì€ í”„ë¡ì‹œ êµ¬ì„±ì„ ì§€ì›í•©ë‹ˆë‹¤:

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(
    model="gpt-4o",
    openai_proxy="http://proxy.example.com:8080"
)
```
> *ì°¸ê³ : í”„ë¡ì‹œ ì§€ì›ì€ í†µí•©ì— ë”°ë¼ ë‹¤ë¦…ë‹ˆë‹¤. í”„ë¡ì‹œ êµ¬ì„± ì˜µì…˜ì€ íŠ¹ì • ëª¨ë¸ ì œê³µìì˜ ì°¸ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.*

</details>

### ë¡œê·¸ í™•ë¥ 

íŠ¹ì • ëª¨ë¸ì€ ëª¨ë¸ ì´ˆê¸°í™” ì‹œ `logprobs` ë§¤ê°œë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì—¬ ì£¼ì–´ì§„ í† í°ì˜ ê°€ëŠ¥ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” í† í° ìˆ˜ì¤€ì˜ ë¡œê·¸ í™•ë¥ ì„ ë°˜í™˜í•˜ë„ë¡ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
model = init_chat_model(
    model="gpt-4o",
    model_provider="openai"
).bind(logprobs=True)

response = model.invoke("Why do parrots talk?")
print(response.response_metadata["logprobs"])
```

### í† í° ì‚¬ìš©ëŸ‰

ë§ì€ ëª¨ë¸ ì œê³µìê°€ í˜¸ì¶œ ì‘ë‹µì˜ ì¼ë¶€ë¡œ í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•  ë•Œ, ì´ ì •ë³´ëŠ” í•´ë‹¹ ëª¨ë¸ì—ì„œ ìƒì„±ëœ `AIMessage` ê°ì²´ì— í¬í•¨ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ë©”ì‹œì§€ ê°€ì´ë“œ](https://docs.langchain.com/oss/python/langchain/models/messages)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

> OpenAI ë° Azure OpenAI Chat ì™„ë£Œì™€ ê°™ì€ ì¼ë¶€ ì œê³µì APIëŠ” ìŠ¤íŠ¸ë¦¬ë° ì»¨í…ìŠ¤íŠ¸ì—ì„œ í† í° ì‚¬ìš©ëŸ‰ ë°ì´í„°ë¥¼ ë°›ìœ¼ë ¤ëŠ” ì‚¬ìš©ì ì˜µíŠ¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ í†µí•© ê°€ì´ë“œì˜ ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš©ëŸ‰ ë©”íƒ€ë°ì´í„° ì„¹ì…˜ì„ ì°¸ì¡°í•˜ì„¸ìš”.

ì•„ë˜ì™€ ê°™ì´ ì½œë°± ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ìš© í”„ë¡œê·¸ë¨ì˜ ëª¨ë¸ ì „ì²´ì—ì„œ ì§‘ê³„ëœ í† í° ìˆ˜ë¥¼ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

**íƒ­: ì½œë°± í•¸ë“¤ëŸ¬**

```python
from langchain.chat_models import init_chat_model
from langchain_core.callbacks import UsageMetadataCallbackHandler

model_1 = init_chat_model(model="gpt-4o-mini")
model_2 = init_chat_model(model="claude-haiku-4-5-20251001")

callback = UsageMetadataCallbackHandler()
result_1 = model_1.invoke("Hello", config={"callbacks": [callback]})
result_2 = model_2.invoke("Hello", config={"callbacks": [callback]})
print(callback.usage_metadata)
```

**íƒ­: ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €**

```python
from langchain.chat_models import init_chat_model
from langchain_core.callbacks import get_usage_metadata_callback

model_1 = init_chat_model(model="gpt-4o-mini")
model_2 = init_chat_model(model="claude-haiku-4-5-20251001")

with get_usage_metadata_callback() as cb:
    model_1.invoke("Hello")
    model_2.invoke("Hello")
    print(cb.usage_metadata)
```

```text
{
    'gpt-4o-mini-2024-07-18': {
        'input_tokens': 8,
        'output_tokens': 10,
        'total_tokens': 18,
        'input_token_details': {'audio': 0, 'cache_read': 0},
        'output_token_details': {'audio': 0, 'reasoning': 0}
    },
    'claude-haiku-4-5-20251001': {
        'input_tokens': 8,
        'output_tokens': 21,
        'total_tokens': 29,
        'input_token_details': {'cache_read': 0, 'cache_creation': 0}
    }
}
```

### í˜¸ì¶œ êµ¬ì„±

ëª¨ë¸ì„ í˜¸ì¶œí•  ë•Œ, `config` ë§¤ê°œë³€ìˆ˜ë¥¼ í†µí•´ `RunnableConfig` ë”•ì…”ë„ˆë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ê°€ êµ¬ì„±ì„ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì‹¤í–‰ ë™ì‘, ì½œë°± ë° ë©”íƒ€ë°ì´í„° ì¶”ì ì— ëŒ€í•œ ëŸ°íƒ€ì„ ì œì–´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì¼ë°˜ì ì¸ êµ¬ì„± ì˜µì…˜ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

#### êµ¬ì„±ìœ¼ë¡œ í˜¸ì¶œ

```python
response = model.invoke(
    "Tell me a joke",
    config={
        "run_name": "joke_generation",      # Custom name for this run
        "tags": ["humor", "demo"],          # Tags for categorization
        "metadata": {"user_id": "123"},     # Custom metadata
        "callbacks": [my_callback_handler], # Callback handlers
    }
)
```

ì´ëŸ¬í•œ êµ¬ì„± ê°’ì€ ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì— íŠ¹íˆ ìœ ìš©í•©ë‹ˆë‹¤:
- LangSmith ì¶”ì ìœ¼ë¡œ ë””ë²„ê¹…í•©ë‹ˆë‹¤
- ì‚¬ìš©ì ì •ì˜ ë¡œê¹… ë˜ëŠ” ëª¨ë‹ˆí„°ë§ì„ êµ¬í˜„í•©ë‹ˆë‹¤
- ìƒì‚°ì˜ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ì„ ì œì–´í•©ë‹ˆë‹¤
- ë³µì¡í•œ íŒŒì´í”„ë¼ì¸ ì „ì²´ì—ì„œ í˜¸ì¶œì„ ì¶”ì í•©ë‹ˆë‹¤

<details>
<summary>ì£¼ìš” êµ¬ì„± ì†ì„±</summary>

*   **run_name** (*ë¬¸ìì—´*): ë¡œê·¸ ë° ì¶”ì ì—ì„œ ì´ íŠ¹ì • í˜¸ì¶œì„ ì‹ë³„í•©ë‹ˆë‹¤. í•˜ìœ„ í˜¸ì¶œì— ì˜í•´ ìƒì†ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
*   **tags** (*ë¬¸ìì—´[]*): ë””ë²„ê¹… ë„êµ¬ì—ì„œ í•„í„°ë§ ë° ì •ë¦¬í•˜ê¸° ìœ„í•´ ëª¨ë“  í•˜ìœ„ í˜¸ì¶œì— ì˜í•´ ìƒì†ë˜ëŠ” ë ˆì´ë¸”ì…ë‹ˆë‹¤.
*   **metadata** (*ê°ì²´*): ëª¨ë“  í•˜ìœ„ í˜¸ì¶œì— ì˜í•´ ìƒì†ë˜ëŠ” ì¶”ì  ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì‚¬ìš©ì ì •ì˜ í‚¤-ê°’ ìŒì…ë‹ˆë‹¤.
*   **max_concurrency** (*ìˆ«ì*): `batch()` ë˜ëŠ” `batch_as_completed()`ë¥¼ ì‚¬ìš©í•  ë•Œ ë³‘ë ¬ í˜¸ì¶œì˜ ìµœëŒ€ ìˆ˜ë¥¼ ì œì–´í•©ë‹ˆë‹¤.
*   **callbacks** (*ë°°ì—´*): ì‹¤í–‰ ì¤‘ ì´ë²¤íŠ¸ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³  ì‘ë‹µí•˜ê¸° ìœ„í•œ í•¸ë“¤ëŸ¬ì…ë‹ˆë‹¤.
*   **recursion_limit** (*ìˆ«ì*): ë³µì¡í•œ íŒŒì´í”„ë¼ì¸ì˜ ë¬´í•œ ë£¨í”„ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•œ ì²´ì¸ì˜ ìµœëŒ€ ì¬ê·€ ê¹Šì´ì…ë‹ˆë‹¤.

</details>

> ì§€ì›ë˜ëŠ” ëª¨ë“  ì†ì„±ì— ëŒ€í•´ì„œëŠ” ì „ì²´ [RunnableConfig ì°¸ì¡°](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.config.RunnableConfig.html)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

### êµ¬ì„± ê°€ëŠ¥í•œ ëª¨ë¸

`configurable_fields`ë¥¼ ì§€ì •í•˜ì—¬ ëŸ°íƒ€ì„ êµ¬ì„± ê°€ëŠ¥ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ ê°’ì„ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´, `'model'` ë° `'model_provider'`ê°€ ê¸°ë³¸ì ìœ¼ë¡œ êµ¬ì„± ê°€ëŠ¥í•©ë‹ˆë‹¤.

```python
from langchain.chat_models import init_chat_model

configurable_model = init_chat_model(temperature=0)

configurable_model.invoke(
    "what's your name",
    config={"configurable": {"model": "gpt-5-nano"}},  # Run with GPT-5-Nano
)
configurable_model.invoke(
    "what's your name",
    config={"configurable": {"model": "claude-sonnet-4-5-20250929"}},  # Run with Claude
)
```

<details>
<summary>ê¸°ë³¸ê°’ì´ ìˆëŠ” êµ¬ì„± ê°€ëŠ¥í•œ ëª¨ë¸</summary>

ìš°ë¦¬ëŠ” ê¸°ë³¸ ëª¨ë¸ ê°’ìœ¼ë¡œ êµ¬ì„± ê°€ëŠ¥í•œ ëª¨ë¸ì„ ë§Œë“¤ê³ , êµ¬ì„± ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì§€ì •í•˜ê³ , êµ¬ì„± ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ì— ì ‘ë‘ì‚¬ë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
first_model = init_chat_model(
        model="gpt-4.1-mini",
        temperature=0,
        configurable_fields=("model", "model_provider", "temperature", "max_tokens"),
        config_prefix="first",
)

first_model.invoke(
    "what's your name",
    config={
        "configurable": {
            "first_model": "claude-sonnet-4-5-20250929",
            "first_temperature": 0.5,
            "first_max_tokens": 100,
        }
    },
)
```

configurable_fields ë° config_prefixì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ init_chat_model ì°¸ì¡°ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

</details>

<details>
<summary>êµ¬ì„± ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì„ ì–¸ì ìœ¼ë¡œ ì‚¬ìš©</summary>

bind_tools, with_structured_output, with_configurable ë“±ê³¼ ê°™ì€ ì„ ì–¸ì  ì‘ì—…ì„ êµ¬ì„± ê°€ëŠ¥í•œ ëª¨ë¸ì—ì„œ í˜¸ì¶œí•˜ê³  ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ êµ¬ì„± ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì²´ì¸í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ê°€ ì •ê¸°ì ìœ¼ë¡œ ì¸ìŠ¤í„´ìŠ¤í™”ëœ Chat ëª¨ë¸ ê°ì²´ë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.

```python
from pydantic import BaseModel, Field


class GetWeather(BaseModel):
    """Get the current weather in a given location"""

        location: str = Field(..., description="The city and state, e.g. San Francisco, CA")


class GetPopulation(BaseModel):
    """Get the current population in a given location"""

        location: str = Field(..., description="The city and state, e.g. San Francisco, CA")


model = init_chat_model(temperature=0)
model_with_tools = model.bind_tools([GetWeather, GetPopulation])

model_with_tools.invoke(
    "what's bigger in 2024 LA or NYC", config={"configurable": {"model": "gpt-4.1-mini"}}
).tool_calls
```
```
[
    {
        'name': 'GetPopulation',
        'args': {'location': 'Los Angeles, CA'},
        'id': 'call_Ga9m8FAArIyEjItHmztPYA22',
        'type': 'tool_call'
    },
    {
        'name': 'GetPopulation',
        'args': {'location': 'New York, NY'},
        'id': 'call_jh2dEvBaAHRaw5JUDthOs7rt',
        'type': 'tool_call'
    }
]
```
```python
model_with_tools.invoke(
    "what's bigger in 2024 LA or NYC",
    config={"configurable": {"model": "claude-sonnet-4-5-20250929"}},
).tool_calls
```
```
[
    {
        'name': 'GetPopulation',
        'args': {'location': 'Los Angeles, CA'},
        'id': 'toolu_01JMufPf4F4t2zLj7miFeqXp',
        'type': 'tool_call'
    },
    {
        'name': 'GetPopulation',
        'args': {'location': 'New York City, NY'},
        'id': 'toolu_01RQBHcE8kEEbYTuuS8WqY1u',
        'type': 'tool_call'
    }
]
```

</details>
