# Agent Chat UI

[**Agent Chat UI**](https://agentchat.vercel.app/) is a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking. Agent Chat UI works seamlessly with agents created using `create_agent` and provides interactive experiences for your agents with minimal setup, whether you're running locally or in a deployed context (such as [LangSmith](https://smith.langchain.com/)).

Agent Chat UI is open source and can be adapted to your application needs.

[![Introducing Agent Chat UI](https://img.youtube.com/vi/lInrwVnZ83o/maxresdefault.jpg)](https://www.youtube.com/watch?v=lInrwVnZ83o)

*▶️ Watch: Introducing Agent Chat UI*

> [!TIP]
> You can use generative UI in the Agent Chat UI. For more information, see [Implement generative user interfaces with LangGraph](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-ui/).

## Quick start

The fastest way to get started is using the hosted version:

1. **Visit [Agent Chat UI](https://agentchat.vercel.app/)**
2. **Connect your agent** by entering your deployment URL or local server address
3. **Start chatting** - the UI will automatically detect and render tool calls and interrupts

## Local development

For customization or local development, you can run Agent Chat UI locally:

#### Use npx

```bash
# Create a new Agent Chat UI project
npx create-agent-chat-app --project-name my-chat-ui
cd my-chat-ui

# Install dependencies and start
pnpm install
pnpm dev
```

#### Clone repository

```bash
# Clone the repository
git clone https://github.com/langchain-ai/agent-chat-ui.git
cd agent-chat-ui

# Install dependencies and start
pnpm install
pnpm dev
```

## Connect to your agent

Agent Chat UI can connect to both **local** and [**deployed agents**](https://docs.langchain.com/oss/python/langchain/deploy).

After starting Agent Chat UI, you'll need to configure it to connect to your agent:

1. **Graph ID**: Enter your graph name (find this under `graphs` in your `langgraph.json` file)
2. **Deployment URL**: Your Agent server's endpoint (e.g., `http://localhost:2024` for local development, or your deployed agent's URL)
3. **LangSmith API key (optional)**: Add your LangSmith API key (not required if you're using a local Agent server)

Once configured, Agent Chat UI will automatically fetch and display any interrupted threads from your agent.

> [!TIP]
> Agent Chat UI has out-of-the-box support for rendering tool calls and tool result messages. To customize what messages are shown, see [Hiding Messages in the Chat](https://github.com/langchain-ai/agent-chat-ui#hiding-messages-in-the-chat).
