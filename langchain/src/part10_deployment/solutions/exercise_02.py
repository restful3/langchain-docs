"""
================================================================================
LangChain AI Agent ë§ˆìŠ¤í„° êµì•ˆ
Part 10: Deployment - ì‹¤ìŠµ ê³¼ì œ 2 í•´ë‹µ
================================================================================

ê³¼ì œ: í‰ê°€ ì‹œìŠ¤í…œ
ë‚œì´ë„: â­â­â­â­â˜† (ê³ ê¸‰)

ìš”êµ¬ì‚¬í•­:
1. Agent ì„±ëŠ¥ ìë™ í‰ê°€
2. ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹
3. ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¶„ì„

í•™ìŠµ ëª©í‘œ:
- Agent í‰ê°€ ë°©ë²•ë¡ 
- ë©”íŠ¸ë¦­ ì •ì˜ ë° ì¸¡ì •
- ì§€ì†ì  ê°œì„ 

================================================================================
"""

from typing import List, Dict, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import statistics
import json

# ============================================================================
# í‰ê°€ ë°ì´í„°ì…‹
# ============================================================================

@dataclass
class EvaluationCase:
    """í‰ê°€ ì¼€ì´ìŠ¤"""
    id: str
    question: str
    expected_answer: str
    category: str
    difficulty: str  # easy, medium, hard

# í‰ê°€ ë°ì´í„°ì…‹
EVALUATION_DATASET = [
    EvaluationCase(
        id="calc_01",
        question="2 + 2ëŠ” ì–¼ë§ˆì¸ê°€ìš”?",
        expected_answer="4",
        category="ê³„ì‚°",
        difficulty="easy"
    ),
    EvaluationCase(
        id="calc_02",
        question="(10 + 5) * 2ë¥¼ ê³„ì‚°í•´ì£¼ì„¸ìš”",
        expected_answer="30",
        category="ê³„ì‚°",
        difficulty="medium"
    ),
    EvaluationCase(
        id="info_01",
        question="Pythonì˜ ì°½ì‹œìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",
        expected_answer="Guido van Rossum",
        category="ì§€ì‹",
        difficulty="easy"
    ),
    EvaluationCase(
        id="reason_01",
        question="ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì£¼ìš” ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        expected_answer="ë³€ê²½ ê°€ëŠ¥ì„± (mutable vs immutable)",
        category="ì¶”ë¡ ",
        difficulty="medium"
    ),
]

# ============================================================================
# í‰ê°€ ë©”íŠ¸ë¦­
# ============================================================================

@dataclass
class EvaluationMetrics:
    """í‰ê°€ ë©”íŠ¸ë¦­"""
    accuracy: float = 0.0
    avg_response_time: float = 0.0
    success_rate: float = 0.0
    category_scores: Dict[str, float] = field(default_factory=dict)
    difficulty_scores: Dict[str, float] = field(default_factory=dict)
    total_cost: float = 0.0

@dataclass
class EvaluationResult:
    """ê°œë³„ í‰ê°€ ê²°ê³¼"""
    case_id: str
    question: str
    expected: str
    actual: str
    passed: bool
    response_time: float
    error: str = ""

# ============================================================================
# í‰ê°€ì (Evaluator)
# ============================================================================

class AgentEvaluator:
    """Agent í‰ê°€ ì‹œìŠ¤í…œ"""
    
    def __init__(self, agent, dataset: List[EvaluationCase]):
        self.agent = agent
        self.dataset = dataset
        self.results: List[EvaluationResult] = []
    
    def evaluate_single(self, case: EvaluationCase) -> EvaluationResult:
        """ë‹¨ì¼ ì¼€ì´ìŠ¤ í‰ê°€"""
        import time
        from langchain_core.messages import HumanMessage
        
        print(f"  í‰ê°€ ì¤‘: {case.id}...")
        
        try:
            start = time.time()
            result = self.agent.invoke({
                "messages": [HumanMessage(content=case.question)]
            })
            elapsed = time.time() - start
            
            actual_answer = result["messages"][-1].content
            
            # ì •ë‹µ í™•ì¸ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
            passed = self._check_answer(case.expected_answer, actual_answer)
            
            return EvaluationResult(
                case_id=case.id,
                question=case.question,
                expected=case.expected_answer,
                actual=actual_answer,
                passed=passed,
                response_time=elapsed
            )
            
        except Exception as e:
            return EvaluationResult(
                case_id=case.id,
                question=case.question,
                expected=case.expected_answer,
                actual="",
                passed=False,
                response_time=0.0,
                error=str(e)
            )
    
    def _check_answer(self, expected: str, actual: str) -> bool:
        """ë‹µë³€ í™•ì¸ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)"""
        expected_lower = expected.lower()
        actual_lower = actual.lower()
        
        # ìˆ«ìëŠ” ì •í™•íˆ ë§¤ì¹­
        if expected.strip().isdigit():
            return expected.strip() in actual_lower
        
        # ë¬¸ìì—´ì€ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€
        keywords = expected_lower.split()
        return any(kw in actual_lower for kw in keywords)
    
    def evaluate_all(self) -> EvaluationMetrics:
        """ì „ì²´ í‰ê°€ ì‹¤í–‰"""
        print("\n" + "=" * 70)
        print("ğŸ“Š í‰ê°€ ì‹œì‘")
        print("=" * 70)
        
        self.results = []
        
        for case in self.dataset:
            result = self.evaluate_single(case)
            self.results.append(result)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = self._calculate_metrics()
        
        return metrics
    
    def _calculate_metrics(self) -> EvaluationMetrics:
        """ë©”íŠ¸ë¦­ ê³„ì‚°"""
        total = len(self.results)
        passed = sum(1 for r in self.results if r.passed)
        
        metrics = EvaluationMetrics(
            accuracy=passed / total if total > 0 else 0,
            avg_response_time=statistics.mean([r.response_time for r in self.results]),
            success_rate=passed / total if total > 0 else 0
        )
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜
        categories = {}
        for case in self.dataset:
            if case.category not in categories:
                categories[case.category] = {"total": 0, "passed": 0}
            
            categories[case.category]["total"] += 1
            
            result = next((r for r in self.results if r.case_id == case.id), None)
            if result and result.passed:
                categories[case.category]["passed"] += 1
        
        metrics.category_scores = {
            cat: data["passed"] / data["total"]
            for cat, data in categories.items()
        }
        
        # ë‚œì´ë„ë³„ ì ìˆ˜
        difficulties = {}
        for case in self.dataset:
            if case.difficulty not in difficulties:
                difficulties[case.difficulty] = {"total": 0, "passed": 0}
            
            difficulties[case.difficulty]["total"] += 1
            
            result = next((r for r in self.results if r.case_id == case.id), None)
            if result and result.passed:
                difficulties[case.difficulty]["passed"] += 1
        
        metrics.difficulty_scores = {
            diff: data["passed"] / data["total"]
            for diff, data in difficulties.items()
        }
        
        return metrics
    
    def print_report(self, metrics: EvaluationMetrics):
        """í‰ê°€ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        print("\n" + "=" * 70)
        print("ğŸ“ˆ í‰ê°€ ë¦¬í¬íŠ¸")
        print("=" * 70)
        
        print(f"\nì „ì²´ ì„±ëŠ¥:")
        print(f"  ì •í™•ë„: {metrics.accuracy:.1%}")
        print(f"  ì„±ê³µë¥ : {metrics.success_rate:.1%}")
        print(f"  í‰ê·  ì‘ë‹µ ì‹œê°„: {metrics.avg_response_time:.2f}ì´ˆ")
        
        print(f"\nì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜:")
        for category, score in metrics.category_scores.items():
            print(f"  {category}: {score:.1%}")
        
        print(f"\në‚œì´ë„ë³„ ì ìˆ˜:")
        for difficulty, score in metrics.difficulty_scores.items():
            print(f"  {difficulty}: {score:.1%}")
        
        print(f"\nê°œë³„ ê²°ê³¼:")
        for result in self.results:
            status = "âœ…" if result.passed else "âŒ"
            print(f"  {status} {result.case_id}: {result.response_time:.2f}s")
            if not result.passed and result.error:
                print(f"     ì˜¤ë¥˜: {result.error}")
        
        print("\n" + "=" * 70)
    
    def export_results(self, filename: str = "evaluation_results.json"):
        """ê²°ê³¼ ë‚´ë³´ë‚´ê¸°"""
        data = {
            "timestamp": datetime.now().isoformat(),
            "total_cases": len(self.results),
            "results": [
                {
                    "case_id": r.case_id,
                    "passed": r.passed,
                    "response_time": r.response_time,
                    "question": r.question,
                    "expected": r.expected,
                    "actual": r.actual[:200],  # ì²˜ìŒ 200ìë§Œ
                    "error": r.error
                }
                for r in self.results
            ]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ ê²°ê³¼ ì €ì¥: {filename}")

# ============================================================================
# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
# ============================================================================

def run_benchmark():
    """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    print("=" * 70)
    print("ğŸ Agent ë²¤ì¹˜ë§ˆí¬")
    print("=" * 70)
    
    # í…ŒìŠ¤íŠ¸ìš© Agent (ì‹¤ì œë¡œëŠ” ì œëŒ€ë¡œ ëœ Agent ì‚¬ìš©)
    from langchain_core.tools import tool
    from langchain_openai import ChatOpenAI
    from langgraph.prebuilt import create_react_agent
    
    @tool
    def calculator(expr: str) -> str:
        """ê³„ì‚°"""
        try:
            return str(eval(expr))
        except:
            return "ì˜¤ë¥˜"
    
    model = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    agent = create_react_agent(model, [calculator])
    
    # í‰ê°€ ì‹¤í–‰
    evaluator = AgentEvaluator(agent, EVALUATION_DATASET)
    metrics = evaluator.evaluate_all()
    
    # ë¦¬í¬íŠ¸ ì¶œë ¥
    evaluator.print_report(metrics)
    
    # ê²°ê³¼ ì €ì¥
    evaluator.export_results()
    
    # ì„±ëŠ¥ ê¸°ì¤€ í™•ì¸
    print("\n" + "=" * 70)
    print("ğŸ¯ ì„±ëŠ¥ ê¸°ì¤€")
    print("=" * 70)
    
    thresholds = {
        "ì •í™•ë„": (metrics.accuracy, 0.8),
        "ì‘ë‹µ ì‹œê°„": (metrics.avg_response_time, 3.0),
    }
    
    all_passed = True
    for metric_name, (value, threshold) in thresholds.items():
        if metric_name == "ì‘ë‹µ ì‹œê°„":
            passed = value <= threshold
            symbol = "âœ…" if passed else "âŒ"
            print(f"  {symbol} {metric_name}: {value:.2f} (ê¸°ì¤€: â‰¤{threshold})")
        else:
            passed = value >= threshold
            symbol = "âœ…" if passed else "âŒ"
            print(f"  {symbol} {metric_name}: {value:.1%} (ê¸°ì¤€: â‰¥{threshold:.1%})")
        
        if not passed:
            all_passed = False
    
    print("\n" + "=" * 70)
    if all_passed:
        print("âœ… ëª¨ë“  ì„±ëŠ¥ ê¸°ì¤€ í†µê³¼!")
    else:
        print("âŒ ì¼ë¶€ ì„±ëŠ¥ ê¸°ì¤€ ë¯¸ë‹¬")
    print("=" * 70)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n" + "=" * 70)
    print("ğŸ“Š Part 10: í‰ê°€ ì‹œìŠ¤í…œ - ì‹¤ìŠµ ê³¼ì œ 2 í•´ë‹µ")
    print("=" * 70)
    
    try:
        run_benchmark()
        
        print("\nğŸ’¡ í•™ìŠµ í¬ì¸íŠ¸:")
        print("  1. í‰ê°€ ë°ì´í„°ì…‹ êµ¬ì¶•")
        print("  2. ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ ì •ì˜")
        print("  3. ìë™í™”ëœ í‰ê°€ íŒŒì´í”„ë¼ì¸")
        print("  4. ì„±ëŠ¥ ê¸°ì¤€ ì„¤ì •")
        
        print("\nğŸ’¡ ì¶”ê°€ ê°œì„ :")
        print("  1. LLM ê¸°ë°˜ í‰ê°€ (ì •ë‹µ íŒì •)")
        print("  2. ë” í° ë°ì´í„°ì…‹")
        print("  3. A/B í…ŒìŠ¤íŒ…")
        print("  4. ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ ì¶”ì ")
    except Exception as e:
        print(f"âš ï¸ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
