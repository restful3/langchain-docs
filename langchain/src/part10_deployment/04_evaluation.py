"""
================================================================================
LangChain AI Agent ë§ˆìŠ¤í„° êµì•ˆ
Part 10: ë°°í¬ì™€ ê´€ì¸¡ì„± (Deployment & Observability)
================================================================================

íŒŒì¼ëª…: 04_evaluation.py
ë‚œì´ë„: â­â­â­â­â­ (ì „ë¬¸ê°€)
ì˜ˆìƒ ì‹œê°„: 30ë¶„

ğŸ“š í•™ìŠµ ëª©í‘œ:
  - í‰ê°€ ë©”íŠ¸ë¦­ ì´í•´ ë° ì ìš©
  - í‰ê°€ ë°ì´í„°ì…‹ ê´€ë¦¬
  - ë²¤ì¹˜ë§ˆí‚¹ ìˆ˜í–‰
  - A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰
  - ì»¤ìŠ¤í…€ í‰ê°€ì ì‘ì„±

ğŸ“– ê³µì‹ ë¬¸ì„œ:
  â€¢ LangSmith: /official/30-langsmith-studio.md
  â€¢ Testing: /official/31-test.md

ğŸ“„ êµì•ˆ ë¬¸ì„œ:
  â€¢ Part 10 ê°œìš”: /docs/part10_deployment.md

ğŸ”§ í•„ìš”í•œ íŒ¨í‚¤ì§€:
  pip install langchain langchain-openai langsmith

ğŸ”‘ í•„ìš”í•œ í™˜ê²½ë³€ìˆ˜:
  - OPENAI_API_KEY
  - LANGSMITH_API_KEY (ì„ íƒ)

ğŸš€ ì‹¤í–‰ ë°©ë²•:
  python 04_evaluation.py

================================================================================
"""

# ============================================================================
# Imports
# ============================================================================

import os
import sys
import time
import statistics
from typing import Dict, Any, List
from datetime import datetime
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain.tools import tool

# ============================================================================
# í™˜ê²½ ì„¤ì •
# ============================================================================

load_dotenv()

if not os.getenv("OPENAI_API_KEY"):
    print("âŒ ì˜¤ë¥˜: OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    sys.exit(1)

# ============================================================================
# ì˜ˆì œ 1: í‰ê°€ ë©”íŠ¸ë¦­ ì†Œê°œ
# ============================================================================

def example_1_evaluation_metrics():
    """í‰ê°€ ë©”íŠ¸ë¦­ ì†Œê°œ"""
    print("=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 1: í‰ê°€ ë©”íŠ¸ë¦­ ì†Œê°œ")
    print("=" * 70)

    print("""
ğŸ“Š í‰ê°€ ë©”íŠ¸ë¦­ (Evaluation Metrics)ì´ë€?

ì •ì˜:
  AI Agentì˜ ì„±ëŠ¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •í•˜ëŠ” ì§€í‘œ

ì™œ ì¤‘ìš”í•œê°€?
  â€¢ ê°ê´€ì ì¸ ì„±ëŠ¥ í‰ê°€
  â€¢ ë²„ì „ ê°„ ë¹„êµ
  â€¢ ê°œì„  íš¨ê³¼ ê²€ì¦
  â€¢ í”„ë¡œë•ì…˜ ì¤€ë¹„ë„ íŒë‹¨

ì£¼ìš” ë©”íŠ¸ë¦­ ì¹´í…Œê³ ë¦¬:

1ï¸âƒ£ ì •í™•ë„ (Accuracy Metrics)
   â€¢ Correctness: ë‹µë³€ì´ ì •í™•í•œê°€?
   â€¢ Relevance: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ”ê°€?
   â€¢ Completeness: í•„ìš”í•œ ì •ë³´ë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ”ê°€?

2ï¸âƒ£ ì„±ëŠ¥ (Performance Metrics)
   â€¢ Latency: ì‘ë‹µ ì‹œê°„
   â€¢ Throughput: ì²˜ë¦¬ëŸ‰ (QPS)
   â€¢ Token Usage: í† í° ì‚¬ìš©ëŸ‰

3ï¸âƒ£ í’ˆì§ˆ (Quality Metrics)
   â€¢ Coherence: ì¼ê´€ì„±
   â€¢ Fluency: ìì—°ìŠ¤ëŸ¬ì›€
   â€¢ Helpfulness: ìœ ìš©ì„±

4ï¸âƒ£ ì‹ ë¢°ì„± (Reliability Metrics)
   â€¢ Success Rate: ì„±ê³µë¥ 
   â€¢ Error Rate: ì˜¤ë¥˜ìœ¨
   â€¢ Consistency: ì¼ê´€ì„± (ê°™ì€ ì…ë ¥, ê°™ì€ ì¶œë ¥)

5ï¸âƒ£ ë¹„ìš© (Cost Metrics)
   â€¢ Cost per Query: ì¿¼ë¦¬ë‹¹ ë¹„ìš©
   â€¢ Token Efficiency: í† í° íš¨ìœ¨ì„±
    """)

    print("\nğŸ”¹ ê¸°ë³¸ ë©”íŠ¸ë¦­ ì¸¡ì • ì˜ˆì œ:")
    print("-" * 70)

    @tool
    def get_capital(country: str) -> str:
        """êµ­ê°€ì˜ ìˆ˜ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        capitals = {
            "ëŒ€í•œë¯¼êµ­": "ì„œìš¸",
            "ì¼ë³¸": "ë„ì¿„",
            "ë¯¸êµ­": "ì›Œì‹±í„´ D.C.",
            "í”„ë‘ìŠ¤": "íŒŒë¦¬",
            "ì˜êµ­": "ëŸ°ë˜"
        }
        return capitals.get(country, f"{country}ì˜ ìˆ˜ë„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # Agent ìƒì„±
    agent = create_agent(
        model="gpt-4o-mini",
        tools=[get_capital],
    )

    # í‰ê°€ ë°ì´í„°ì…‹
    eval_dataset = [
        {
            "question": "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?",
            "expected_answer": "ì„œìš¸",
            "category": "simple"
        },
        {
            "question": "ì¼ë³¸ ìˆ˜ë„ ì•Œë ¤ì¤˜",
            "expected_answer": "ë„ì¿„",
            "category": "simple"
        },
        {
            "question": "ë¯¸êµ­ê³¼ í”„ë‘ìŠ¤ì˜ ìˆ˜ë„ë¥¼ ëª¨ë‘ ì•Œë ¤ì£¼ì„¸ìš”",
            "expected_answer": ["ì›Œì‹±í„´", "íŒŒë¦¬"],
            "category": "multi"
        },
        {
            "question": "ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‚˜ë¼ xyzì˜ ìˆ˜ë„ëŠ”?",
            "expected_answer": None,
            "category": "edge"
        }
    ]

    print("\ní‰ê°€ ì‹¤í–‰:")
    results = []

    for i, case in enumerate(eval_dataset, 1):
        print(f"\n[{i}] {case['question']}")

        start_time = time.time()
        try:
            response = agent.invoke({
                "messages": [{"role": "user", "content": case['question']}]
            })
            answer = response['messages'][-1].content
            latency = time.time() - start_time

            print(f"  ì‘ë‹µ: {answer[:80]}...")
            print(f"  ì§€ì—°ì‹œê°„: {latency:.2f}ì´ˆ")

            # ì •í™•ë„ í‰ê°€
            if case['expected_answer'] is None:
                # ì—ëŸ¬ ì¼€ì´ìŠ¤: "ì •ë³´ê°€ ì—†ë‹¤"ëŠ” ë©”ì‹œì§€ í™•ì¸
                is_correct = any(word in answer for word in ["ì—†", "ì •ë³´ê°€ ì—†", "ëª¨ë¥´"])
            elif isinstance(case['expected_answer'], list):
                # ë‹¤ì¤‘ ë‹µë³€: ëª¨ë“  í‚¤ì›Œë“œ í¬í•¨ í™•ì¸
                is_correct = all(exp in answer for exp in case['expected_answer'])
            else:
                # ë‹¨ì¼ ë‹µë³€: í‚¤ì›Œë“œ í¬í•¨ í™•ì¸
                is_correct = case['expected_answer'] in answer

            status = "âœ…" if is_correct else "âŒ"
            print(f"  ì •í™•ë„: {status}")

            results.append({
                "question": case['question'],
                "category": case['category'],
                "is_correct": is_correct,
                "latency": latency,
                "answer_length": len(answer)
            })

        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜: {e}")
            results.append({
                "question": case['question'],
                "category": case['category'],
                "is_correct": False,
                "latency": time.time() - start_time,
                "answer_length": 0
            })

    # ë©”íŠ¸ë¦­ ì§‘ê³„
    print("\n" + "-" * 70)
    print("\nğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½:")

    accuracy = sum(1 for r in results if r['is_correct']) / len(results) * 100
    avg_latency = statistics.mean(r['latency'] for r in results)
    avg_answer_length = statistics.mean(r['answer_length'] for r in results)

    print(f"   ì •í™•ë„ (Accuracy): {accuracy:.1f}%")
    print(f"   í‰ê·  ì§€ì—°ì‹œê°„ (Latency): {avg_latency:.2f}ì´ˆ")
    print(f"   í‰ê·  ì‘ë‹µ ê¸¸ì´: {avg_answer_length:.0f}ì")

    # ì¹´í…Œê³ ë¦¬ë³„ ì •í™•ë„
    print("\n   ì¹´í…Œê³ ë¦¬ë³„ ì •í™•ë„:")
    categories = {}
    for r in results:
        cat = r['category']
        if cat not in categories:
            categories[cat] = []
        categories[cat].append(r['is_correct'])

    for cat, correctness in categories.items():
        cat_accuracy = sum(correctness) / len(correctness) * 100
        print(f"     â€¢ {cat}: {cat_accuracy:.1f}%")

    print("\nğŸ’¡ ë©”íŠ¸ë¦­ ì„ íƒ ê°€ì´ë“œ:")
    print("   â€¢ ë„ë©”ì¸ì— ë§ëŠ” ë©”íŠ¸ë¦­ ì„ íƒ")
    print("   â€¢ ì—¬ëŸ¬ ë©”íŠ¸ë¦­ì„ ì¡°í•©í•˜ì—¬ ì¢…í•© í‰ê°€")
    print("   â€¢ ìë™í™”ëœ ë©”íŠ¸ë¦­ + ì‚¬ëŒ í‰ê°€")
    print("   â€¢ ì‹œê°„ì— ë”°ë¥¸ ë©”íŠ¸ë¦­ ì¶”ì´ ëª¨ë‹ˆí„°ë§")


# ============================================================================
# ì˜ˆì œ 2: í‰ê°€ ë°ì´í„°ì…‹ ê´€ë¦¬
# ============================================================================

def example_2_evaluation_datasets():
    """í‰ê°€ ë°ì´í„°ì…‹ ê´€ë¦¬"""
    print("\n" + "=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 2: í‰ê°€ ë°ì´í„°ì…‹ ê´€ë¦¬")
    print("=" * 70)

    print("""
ğŸ“‚ í‰ê°€ ë°ì´í„°ì…‹ ê´€ë¦¬:

í‰ê°€ ë°ì´í„°ì…‹ì˜ ì¤‘ìš”ì„±:
  â€¢ ì¼ê´€ëœ ì„±ëŠ¥ ì¸¡ì •
  â€¢ íšŒê·€(regression) ë°©ì§€
  â€¢ ë²„ì „ ê°„ ë¹„êµ ê°€ëŠ¥

ë°ì´í„°ì…‹ êµ¬ì„± ìš”ì†Œ:
  1ï¸âƒ£ ì…ë ¥ (Input/Question)
  2ï¸âƒ£ ì˜ˆìƒ ì¶œë ¥ (Expected Output)
  3ï¸âƒ£ ë©”íƒ€ë°ì´í„° (Category, Difficulty, Tags)
  4ï¸âƒ£ í‰ê°€ ê¸°ì¤€ (Evaluation Criteria)

ë°ì´í„°ì…‹ ì„¤ê³„ ì›ì¹™:
  â€¢ ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€ ë°˜ì˜
  â€¢ ë‚œì´ë„ ë¶„í¬ ê· í˜•
  â€¢ Edge cases í¬í•¨
  â€¢ ì •ê¸°ì  ì—…ë°ì´íŠ¸

LangSmith ë°ì´í„°ì…‹:
  â€¢ UIì—ì„œ ë°ì´í„°ì…‹ ìƒì„±/ê´€ë¦¬
  â€¢ ë²„ì „ ê´€ë¦¬
  â€¢ íŒ€ ê³µìœ 
  â€¢ ìë™ í‰ê°€ ì‹¤í–‰
    """)

    print("\nğŸ”¹ ì²´ê³„ì ì¸ ë°ì´í„°ì…‹ ì˜ˆì œ:")
    print("-" * 70)

    # ê³ ê¸‰ í‰ê°€ ë°ì´í„°ì…‹
    evaluation_dataset = {
        "name": "Calculator Agent v1.0",
        "description": "ê¸°ë³¸ ê³„ì‚° ê¸°ëŠ¥ í‰ê°€",
        "version": "1.0",
        "created": datetime.now().isoformat(),
        "test_cases": [
            {
                "id": "CALC-001",
                "category": "ê¸°ë³¸ê³„ì‚°",
                "difficulty": "easy",
                "input": "10 ë”í•˜ê¸° 20ì€?",
                "expected": "30",
                "tags": ["addition", "basic"],
                "weight": 1.0
            },
            {
                "id": "CALC-002",
                "category": "ê¸°ë³¸ê³„ì‚°",
                "difficulty": "easy",
                "input": "100 ë¹¼ê¸° 25ëŠ”?",
                "expected": "75",
                "tags": ["subtraction", "basic"],
                "weight": 1.0
            },
            {
                "id": "CALC-003",
                "category": "ì‘ìš©ê³„ì‚°",
                "difficulty": "medium",
                "input": "1000ì›ì˜ 15% í• ì¸ê°€ëŠ”?",
                "expected": "850",
                "tags": ["percentage", "discount"],
                "weight": 1.5
            },
            {
                "id": "CALC-004",
                "category": "ë³µí•©ê³„ì‚°",
                "difficulty": "hard",
                "input": "5000ì›ì— 10% í• ì¸ í›„ 10% ì„¸ê¸ˆ ì¶”ê°€í•˜ë©´?",
                "expected": "4950",
                "tags": ["multi-step", "complex"],
                "weight": 2.0
            },
            {
                "id": "CALC-005",
                "category": "ì—£ì§€ì¼€ì´ìŠ¤",
                "difficulty": "edge",
                "input": "0ìœ¼ë¡œ ë‚˜ëˆ„ê¸°",
                "expected": None,  # ì—ëŸ¬ ì²˜ë¦¬ í™•ì¸
                "tags": ["error-handling", "edge"],
                "weight": 1.5
            }
        ]
    }

    # Calculator Tool
    @tool
    def calculate(expression: str) -> str:
        """ìˆ˜ì‹ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        try:
            result = eval(expression, {"__builtins__": {}}, {})
            return str(result)
        except ZeroDivisionError:
            return "0ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        except Exception as e:
            return f"ê³„ì‚° ì˜¤ë¥˜: {e}"

    # Agent ìƒì„±
    agent = create_agent(
        model="gpt-4o-mini",
        tools=[calculate],
    )

    # ë°ì´í„°ì…‹ ì‹¤í–‰
    print(f"\nğŸ“Š ë°ì´í„°ì…‹: {evaluation_dataset['name']}")
    print(f"   ë²„ì „: {evaluation_dataset['version']}")
    print(f"   ì¼€ì´ìŠ¤ ìˆ˜: {len(evaluation_dataset['test_cases'])}")
    print("\n" + "-" * 70)

    results = []
    total_weight = 0
    weighted_score = 0

    for test_case in evaluation_dataset['test_cases']:
        print(f"\n[{test_case['id']}] {test_case['difficulty'].upper()}")
        print(f"  ì§ˆë¬¸: {test_case['input']}")
        print(f"  ì¹´í…Œê³ ë¦¬: {test_case['category']}")
        print(f"  íƒœê·¸: {', '.join(test_case['tags'])}")
        print(f"  ê°€ì¤‘ì¹˜: {test_case['weight']}")

        try:
            response = agent.invoke({
                "messages": [{"role": "user", "content": test_case['input']}]
            })
            answer = response['messages'][-1].content
            print(f"  ì‘ë‹µ: {answer[:80]}...")

            # í‰ê°€
            if test_case['expected'] is None:
                # ì—ëŸ¬ ì²˜ë¦¬ í™•ì¸
                is_correct = any(word in answer for word in ["ì˜¤ë¥˜", "ë‚˜ëˆŒ ìˆ˜ ì—†", "ë¶ˆê°€ëŠ¥"])
            else:
                is_correct = test_case['expected'] in answer

            score = test_case['weight'] if is_correct else 0
            weighted_score += score
            total_weight += test_case['weight']

            status = "âœ… PASS" if is_correct else "âŒ FAIL"
            print(f"  ê²°ê³¼: {status} (ì ìˆ˜: {score}/{test_case['weight']})")

            results.append({
                "id": test_case['id'],
                "passed": is_correct,
                "weight": test_case['weight'],
                "score": score
            })

        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜: {e}")
            total_weight += test_case['weight']
            results.append({
                "id": test_case['id'],
                "passed": False,
                "weight": test_case['weight'],
                "score": 0
            })

    # ìµœì¢… ì ìˆ˜
    print("\n" + "=" * 70)
    print("\nğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼:")

    final_score = (weighted_score / total_weight * 100) if total_weight > 0 else 0
    pass_count = sum(1 for r in results if r['passed'])
    total_count = len(results)

    print(f"   ê°€ì¤‘ ì ìˆ˜: {final_score:.1f}/100")
    print(f"   í†µê³¼ìœ¨: {pass_count}/{total_count} ({pass_count/total_count*100:.1f}%)")

    # ë‚œì´ë„ë³„ ë¶„ì„
    print("\n   ë‚œì´ë„ë³„ ê²°ê³¼:")
    difficulty_map = {tc['id']: tc['difficulty'] for tc in evaluation_dataset['test_cases']}
    difficulty_results = {}

    for r in results:
        diff = difficulty_map[r['id']]
        if diff not in difficulty_results:
            difficulty_results[diff] = {"passed": 0, "total": 0}
        difficulty_results[diff]['total'] += 1
        if r['passed']:
            difficulty_results[diff]['passed'] += 1

    for diff, stats in difficulty_results.items():
        rate = stats['passed'] / stats['total'] * 100
        print(f"     â€¢ {diff}: {stats['passed']}/{stats['total']} ({rate:.1f}%)")

    print("\nğŸ’¡ ë°ì´í„°ì…‹ ê´€ë¦¬ íŒ:")
    print("   â€¢ ë²„ì „ ê´€ë¦¬ë¡œ ë³€ê²½ ì¶”ì ")
    print("   â€¢ ê°€ì¤‘ì¹˜ë¡œ ì¤‘ìš”ë„ ë°˜ì˜")
    print("   â€¢ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ë¥¼ ë°ì´í„°ì…‹ì— ì¶”ê°€")
    print("   â€¢ ì •ê¸°ì ìœ¼ë¡œ ë°ì´í„°ì…‹ ê²€í†  ë° ì—…ë°ì´íŠ¸")
    print("   â€¢ LangSmithì— ì €ì¥í•˜ì—¬ íŒ€ê³¼ ê³µìœ ")


# ============================================================================
# ì˜ˆì œ 3: ë²¤ì¹˜ë§ˆí‚¹
# ============================================================================

def example_3_benchmarking():
    """ë²¤ì¹˜ë§ˆí‚¹"""
    print("\n" + "=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 3: ë²¤ì¹˜ë§ˆí‚¹")
    print("=" * 70)

    print("""
âš¡ ë²¤ì¹˜ë§ˆí‚¹ (Benchmarking)ì´ë€?

ì •ì˜:
  ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ ì²´ê³„ì ìœ¼ë¡œ ì¸¡ì •í•˜ê³  ë¹„êµí•˜ëŠ” í”„ë¡œì„¸ìŠ¤

ë²¤ì¹˜ë§ˆí‚¹ ëª©ì :
  1ï¸âƒ£ ì„±ëŠ¥ ê¸°ì¤€ì„ (baseline) ìˆ˜ë¦½
  2ï¸âƒ£ ìµœì í™” íš¨ê³¼ ì¸¡ì •
  3ï¸âƒ£ ëª¨ë¸/ë²„ì „ ê°„ ë¹„êµ
  4ï¸âƒ£ ë³‘ëª© ì§€ì  íŒŒì•…
  5ï¸âƒ£ SLA(Service Level Agreement) ê²€ì¦

ì¸¡ì • í•­ëª©:
  â€¢ ì²˜ë¦¬ëŸ‰ (Throughput): QPS (Queries Per Second)
  â€¢ ì§€ì—°ì‹œê°„ (Latency): p50, p95, p99
  â€¢ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰: CPU, ë©”ëª¨ë¦¬
  â€¢ ë¹„ìš©: í† í° ì‚¬ìš©ëŸ‰, API ë¹„ìš©
  â€¢ ì •í™•ë„: Accuracy, F1 Score

ë²¤ì¹˜ë§ˆí¬ ì‹œë‚˜ë¦¬ì˜¤:
  â€¢ Cold Start: ì²« ìš”ì²­
  â€¢ Warm: ìºì‹œ ì ìš©
  â€¢ Peak Load: ìµœëŒ€ ë¶€í•˜
  â€¢ Sustained Load: ì§€ì† ë¶€í•˜
    """)

    print("\nğŸ”¹ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì˜ˆì œ:")
    print("-" * 70)

    @tool
    def search_docs(query: str) -> str:
        """ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        time.sleep(0.1)  # ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜
        return f"{query}ì— ëŒ€í•œ ë¬¸ì„œ 3ê°œ ë°œê²¬"

    # Agent ìƒì„±
    agent = create_agent(
        model="gpt-4o-mini",
        tools=[search_docs],
    )

    # ë²¤ì¹˜ë§ˆí¬ ì¿¼ë¦¬
    benchmark_queries = [
        "ì¸ê³µì§€ëŠ¥ì´ë€?",
        "LangChain ì‚¬ìš©ë²•",
        "Python íŠœí† ë¦¬ì–¼",
        "ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜",
        "ë°ì´í„° ë¶„ì„ ë°©ë²•"
    ]

    print("\në²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (5ê°œ ì¿¼ë¦¬ x 3íšŒ ë°˜ë³µ):")
    print("-" * 70)

    all_latencies = []
    all_token_counts = []

    for iteration in range(1, 4):
        print(f"\nğŸ”„ Iteration {iteration}/3:")

        iteration_latencies = []

        for i, query in enumerate(benchmark_queries, 1):
            start = time.time()

            try:
                response = agent.invoke({
                    "messages": [{"role": "user", "content": query}]
                })
                latency = time.time() - start

                # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì • (ì‹¤ì œë¡œëŠ” responseì—ì„œ ê°€ì ¸ì˜´)
                answer = response['messages'][-1].content
                estimated_tokens = len(answer.split()) * 1.3  # ëŒ€ëµì ì¸ ì¶”ì •

                iteration_latencies.append(latency)
                all_latencies.append(latency)
                all_token_counts.append(estimated_tokens)

                print(f"  [{i}] {query[:30]:30s} | {latency:5.2f}s | ~{int(estimated_tokens):3d} tokens")

            except Exception as e:
                print(f"  [{i}] {query[:30]:30s} | ERROR: {e}")

        avg_latency = statistics.mean(iteration_latencies)
        print(f"  â†’ í‰ê·  ì§€ì—°: {avg_latency:.2f}s")

    # í†µê³„ ë¶„ì„
    print("\n" + "=" * 70)
    print("\nğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„:")

    latencies_sorted = sorted(all_latencies)
    n = len(latencies_sorted)

    p50 = latencies_sorted[int(n * 0.50)]
    p95 = latencies_sorted[int(n * 0.95)]
    p99 = latencies_sorted[int(n * 0.99)] if n > 100 else latencies_sorted[-1]

    print(f"\n   ì§€ì—°ì‹œê°„ (Latency):")
    print(f"     â€¢ í‰ê·  (Mean): {statistics.mean(all_latencies):.2f}s")
    print(f"     â€¢ ì¤‘ì•™ê°’ (Median): {statistics.median(all_latencies):.2f}s")
    print(f"     â€¢ ìµœì†Œ (Min): {min(all_latencies):.2f}s")
    print(f"     â€¢ ìµœëŒ€ (Max): {max(all_latencies):.2f}s")
    print(f"     â€¢ p50: {p50:.2f}s")
    print(f"     â€¢ p95: {p95:.2f}s")
    print(f"     â€¢ p99: {p99:.2f}s")

    if len(all_latencies) > 1:
        print(f"     â€¢ í‘œì¤€í¸ì°¨: {statistics.stdev(all_latencies):.2f}s")

    print(f"\n   ì²˜ë¦¬ëŸ‰ (Throughput):")
    total_time = sum(all_latencies)
    qps = len(all_latencies) / total_time if total_time > 0 else 0
    print(f"     â€¢ QPS (Queries/Second): {qps:.2f}")

    print(f"\n   í† í° ì‚¬ìš©ëŸ‰:")
    print(f"     â€¢ í‰ê· : {statistics.mean(all_token_counts):.0f} tokens")
    print(f"     â€¢ ì´í•©: {sum(all_token_counts):.0f} tokens")

    # ì„±ëŠ¥ ë“±ê¸‰ íŒì •
    print(f"\n   ì„±ëŠ¥ ë“±ê¸‰:")
    if p95 < 2.0:
        grade = "ğŸŸ¢ ìš°ìˆ˜ (Excellent)"
    elif p95 < 5.0:
        grade = "ğŸŸ¡ ì–‘í˜¸ (Good)"
    else:
        grade = "ğŸ”´ ê°œì„  í•„ìš” (Needs Improvement)"
    print(f"     {grade}")

    print("\nğŸ’¡ ë²¤ì¹˜ë§ˆí‚¹ ëª¨ë²” ì‚¬ë¡€:")
    print("   â€¢ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µ ì¸¡ì • (í†µê³„ì  ìœ ì˜ì„±)")
    print("   â€¢ Cold startì™€ warm start êµ¬ë¶„")
    print("   â€¢ ë‹¤ì–‘í•œ ì¿¼ë¦¬ íŒ¨í„´ í…ŒìŠ¤íŠ¸")
    print("   â€¢ ì‹œê°„ëŒ€ë³„, ë‚ ì§œë³„ ì¶”ì´ ëª¨ë‹ˆí„°ë§")
    print("   â€¢ p95, p99 ê°™ì€ ë°±ë¶„ìœ„ìˆ˜ ì¤‘ìš”")
    print("   â€¢ ê¸°ì¤€ì„  ëŒ€ë¹„ íšŒê·€ ë°©ì§€")


# ============================================================================
# ì˜ˆì œ 4: A/B í…ŒìŠ¤íŠ¸
# ============================================================================

def example_4_ab_testing():
    """A/B í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 4: A/B í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    print("""
ğŸ”¬ A/B í…ŒìŠ¤íŠ¸ë€?

ì •ì˜:
  ë‘ ê°€ì§€ ì´ìƒì˜ ë²„ì „ì„ ë¹„êµí•˜ì—¬ ì–´ëŠ ê²ƒì´ ë” ë‚˜ì€ì§€ ê²€ì¦

ì‚¬ìš© ì‚¬ë¡€:
  â€¢ í”„ë¡¬í”„íŠ¸ ìµœì í™”
  â€¢ ëª¨ë¸ ì„ íƒ (GPT-4 vs GPT-3.5)
  â€¢ Tool êµ¬ì„± ë³€ê²½
  â€¢ íŒŒë¼ë¯¸í„° íŠœë‹

A/B í…ŒìŠ¤íŠ¸ í”„ë¡œì„¸ìŠ¤:
  1ï¸âƒ£ ê°€ì„¤ ìˆ˜ë¦½
  2ï¸âƒ£ ë³€í˜• A, B ì •ì˜
  3ï¸âƒ£ í‰ê°€ ë©”íŠ¸ë¦­ ì„ ì •
  4ï¸âƒ£ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
  5ï¸âƒ£ í†µê³„ì  ìœ ì˜ì„± ê²€ì¦
  6ï¸âƒ£ ì˜ì‚¬ê²°ì •

í†µê³„ì  ìœ ì˜ì„±:
  â€¢ ì¶©ë¶„í•œ ìƒ˜í”Œ ìˆ˜
  â€¢ p-value < 0.05
  â€¢ ì‹¤ì§ˆì  ì°¨ì´ (practical significance)
    """)

    print("\nğŸ”¹ A/B í…ŒìŠ¤íŠ¸ ì˜ˆì œ:")
    print("-" * 70)

    @tool
    def get_info(topic: str) -> str:
        """ì£¼ì œì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."""
        return f"{topic}ì€(ëŠ”) ì¤‘ìš”í•œ ì£¼ì œì…ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."

    # ë²„ì „ A: ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸
    print("\në²„ì „ A: ê°„ê²°í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸")
    agent_a = create_agent(
        model="gpt-4o-mini",
        tools=[get_info],
    )

    # ë²„ì „ B: ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸ (ë™ì¼ agent ì‚¬ìš©, ì‹¤ì œë¡œëŠ” ë‹¤ë¥¸ ì„¤ì •)
    print("ë²„ì „ B: ìƒì„¸í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸")
    agent_b = create_agent(
        model="gpt-4o-mini",
        tools=[get_info],
    )

    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_queries = [
        "ì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ ì•Œë ¤ì¤˜",
        "ë¨¸ì‹ ëŸ¬ë‹ì´ ë­ì•¼?",
        "Python ì¥ì ì€?",
        "ë°ì´í„° ë¶„ì„ ë°©ë²•",
        "í´ë¼ìš°ë“œ ì»´í“¨íŒ… ì„¤ëª…"
    ]

    print("\n" + "-" * 70)
    print("A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰:")
    print("-" * 70)

    results_a = []
    results_b = []

    for i, query in enumerate(test_queries, 1):
        print(f"\n[í…ŒìŠ¤íŠ¸ {i}] {query}")

        # ë²„ì „ A ì‹¤í–‰
        print("  ğŸ…°ï¸  ë²„ì „ A:")
        start = time.time()
        try:
            response_a = agent_a.invoke({
                "messages": [{"role": "user", "content": query}]
            })
            answer_a = response_a['messages'][-1].content
            latency_a = time.time() - start
            length_a = len(answer_a)

            print(f"     ì‘ë‹µ: {answer_a[:60]}...")
            print(f"     ì§€ì—°: {latency_a:.2f}s | ê¸¸ì´: {length_a}ì")

            results_a.append({
                "query": query,
                "latency": latency_a,
                "length": length_a,
                "success": True
            })
        except Exception as e:
            print(f"     âŒ ì˜¤ë¥˜: {e}")
            results_a.append({
                "query": query,
                "latency": 0,
                "length": 0,
                "success": False
            })

        # ë²„ì „ B ì‹¤í–‰
        print("  ğŸ…±ï¸  ë²„ì „ B:")
        start = time.time()
        try:
            response_b = agent_b.invoke({
                "messages": [{"role": "user", "content": query}]
            })
            answer_b = response_b['messages'][-1].content
            latency_b = time.time() - start
            length_b = len(answer_b)

            print(f"     ì‘ë‹µ: {answer_b[:60]}...")
            print(f"     ì§€ì—°: {latency_b:.2f}s | ê¸¸ì´: {length_b}ì")

            results_b.append({
                "query": query,
                "latency": latency_b,
                "length": length_b,
                "success": True
            })
        except Exception as e:
            print(f"     âŒ ì˜¤ë¥˜: {e}")
            results_b.append({
                "query": query,
                "latency": 0,
                "length": 0,
                "success": False
            })

    # ê²°ê³¼ ë¹„êµ
    print("\n" + "=" * 70)
    print("\nğŸ“Š A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¹„êµ:")

    success_rate_a = sum(1 for r in results_a if r['success']) / len(results_a) * 100
    success_rate_b = sum(1 for r in results_b if r['success']) / len(results_b) * 100

    avg_latency_a = statistics.mean(r['latency'] for r in results_a if r['success'])
    avg_latency_b = statistics.mean(r['latency'] for r in results_b if r['success'])

    avg_length_a = statistics.mean(r['length'] for r in results_a if r['success'])
    avg_length_b = statistics.mean(r['length'] for r in results_b if r['success'])

    print(f"\n   ì„±ê³µë¥ :")
    print(f"     ğŸ…°ï¸  ë²„ì „ A: {success_rate_a:.1f}%")
    print(f"     ğŸ…±ï¸  ë²„ì „ B: {success_rate_b:.1f}%")

    print(f"\n   í‰ê·  ì§€ì—°ì‹œê°„:")
    print(f"     ğŸ…°ï¸  ë²„ì „ A: {avg_latency_a:.2f}s")
    print(f"     ğŸ…±ï¸  ë²„ì „ B: {avg_latency_b:.2f}s")
    latency_diff = ((avg_latency_b - avg_latency_a) / avg_latency_a * 100)
    print(f"     â†’ ì°¨ì´: {latency_diff:+.1f}%")

    print(f"\n   í‰ê·  ì‘ë‹µ ê¸¸ì´:")
    print(f"     ğŸ…°ï¸  ë²„ì „ A: {avg_length_a:.0f}ì")
    print(f"     ğŸ…±ï¸  ë²„ì „ B: {avg_length_b:.0f}ì")
    length_diff = ((avg_length_b - avg_length_a) / avg_length_a * 100)
    print(f"     â†’ ì°¨ì´: {length_diff:+.1f}%")

    # ê¶Œì¥ì‚¬í•­
    print(f"\n   ğŸ’¡ ê¶Œì¥ì‚¬í•­:")
    if abs(latency_diff) < 5:
        print("     â€¢ ì§€ì—°ì‹œê°„ ì°¨ì´ê°€ ë¯¸ë¯¸í•¨ (< 5%)")
    elif latency_diff < 0:
        print(f"     â€¢ ë²„ì „ Aê°€ {abs(latency_diff):.1f}% ë” ë¹ ë¦„ âœ…")
    else:
        print(f"     â€¢ ë²„ì „ Bê°€ {abs(latency_diff):.1f}% ë” ëŠë¦¼ âš ï¸")

    print("\nğŸ’¡ A/B í…ŒìŠ¤íŠ¸ ëª¨ë²” ì‚¬ë¡€:")
    print("   â€¢ í•˜ë‚˜ì˜ ë³€ìˆ˜ë§Œ ë³€ê²½")
    print("   â€¢ ì¶©ë¶„í•œ ìƒ˜í”Œ ìˆ˜ í™•ë³´ (ìµœì†Œ 30+)")
    print("   â€¢ ì—¬ëŸ¬ ë©”íŠ¸ë¦­ ì¢…í•© íŒë‹¨")
    print("   â€¢ í†µê³„ì  + ì‹¤ì§ˆì  ìœ ì˜ì„± ê³ ë ¤")
    print("   â€¢ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì ì§„ì  ë¡¤ì•„ì›ƒ")


# ============================================================================
# ì˜ˆì œ 5: ì»¤ìŠ¤í…€ í‰ê°€ì
# ============================================================================

def example_5_custom_evaluators():
    """ì»¤ìŠ¤í…€ í‰ê°€ì"""
    print("\n" + "=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 5: ì»¤ìŠ¤í…€ í‰ê°€ì")
    print("=" * 70)

    print("""
ğŸ¯ ì»¤ìŠ¤í…€ í‰ê°€ì (Custom Evaluator)ë€?

ì •ì˜:
  ë„ë©”ì¸ íŠ¹í™” ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” í‰ê°€ ë¡œì§

ê¸°ë³¸ í‰ê°€ìì˜ í•œê³„:
  â€¢ ë²”ìš©ì , ë„ë©”ì¸ íŠ¹í™” X
  â€¢ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ë°˜ì˜ ì–´ë ¤ì›€
  â€¢ ë³µì¡í•œ í‰ê°€ ê¸°ì¤€ í‘œí˜„ ì œí•œ

ì»¤ìŠ¤í…€ í‰ê°€ì ì‚¬ìš© ì‚¬ë¡€:
  1ï¸âƒ£ í†¤ ì•¤ ë§¤ë„ˆ ê²€ì¦
     â€¢ ì¹œì ˆí•œê°€? ì „ë¬¸ì ì¸ê°€?

  2ï¸âƒ£ ê·œì • ì¤€ìˆ˜ í™•ì¸
     â€¢ ê¸ˆì§€ ë‹¨ì–´ í¬í•¨ ì—¬ë¶€
     â€¢ í•„ìˆ˜ ì •ë³´ í¬í•¨ ì—¬ë¶€

  3ï¸âƒ£ ë„ë©”ì¸ ì •í™•ë„
     â€¢ ì˜ë£Œ: ì •í™•í•œ ì˜í•™ ìš©ì–´ ì‚¬ìš©
     â€¢ ê¸ˆìœµ: ë¦¬ìŠ¤í¬ ê²½ê³  í¬í•¨

  4ï¸âƒ£ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™
     â€¢ íŠ¹ì • ì œí’ˆ ìš°ì„  ì¶”ì²œ
     â€¢ ê°€ê²© ë²”ìœ„ ì¤€ìˆ˜
    """)

    print("\nğŸ”¹ ì»¤ìŠ¤í…€ í‰ê°€ì ì˜ˆì œ:")
    print("-" * 70)

    # í‰ê°€ì ì •ì˜
    class CustomerServiceEvaluator:
        """ê³ ê° ì„œë¹„ìŠ¤ ì‘ë‹µ í‰ê°€ì"""

        def __init__(self):
            self.required_phrases = ["ê°ì‚¬í•©ë‹ˆë‹¤", "ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤"]
            self.forbidden_words = ["ëª¨ë¥´ê² ", "ì•ˆ ë¼", "ë¶ˆê°€ëŠ¥"]
            self.professional_keywords = ["ê³ ê°ë‹˜", "í™•ì¸", "ì²˜ë¦¬"]

        def evaluate(self, answer: str) -> Dict[str, Any]:
            """ì‘ë‹µì„ í‰ê°€í•©ë‹ˆë‹¤."""
            scores = {}

            # 1. ì˜ˆì˜ ì ìˆ˜
            politeness_score = sum(
                1 for phrase in self.required_phrases if phrase in answer
            ) / len(self.required_phrases) * 100
            scores['politeness'] = politeness_score

            # 2. ê¸ˆì§€ì–´ í™•ì¸
            has_forbidden = any(word in answer for word in self.forbidden_words)
            scores['no_forbidden_words'] = 0 if has_forbidden else 100

            # 3. ì „ë¬¸ì„± ì ìˆ˜
            professional_count = sum(
                1 for keyword in self.professional_keywords if keyword in answer
            )
            scores['professionalism'] = min(professional_count / 2 * 100, 100)

            # 4. ê¸¸ì´ ì ì ˆì„±
            length = len(answer)
            if 50 <= length <= 300:
                scores['appropriate_length'] = 100
            elif length < 50:
                scores['appropriate_length'] = 50
            else:
                scores['appropriate_length'] = 70

            # ì¢…í•© ì ìˆ˜
            overall_score = statistics.mean(scores.values())

            return {
                "scores": scores,
                "overall": overall_score,
                "passed": overall_score >= 70,
                "details": {
                    "has_forbidden": has_forbidden,
                    "length": length,
                    "professional_count": professional_count
                }
            }

    @tool
    def handle_complaint(issue: str) -> str:
        """ê³ ê° ë¶ˆë§Œì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        responses = {
            "ë°°ì†¡ ì§€ì—°": "ê³ ê°ë‹˜, ë°°ì†¡ ì§€ì—°ì— ëŒ€í•´ ì‚¬ê³¼ë“œë¦½ë‹ˆë‹¤. ì¦‰ì‹œ í™•ì¸í•˜ì—¬ ì²˜ë¦¬í•˜ê² ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.",
            "ì œí’ˆ ë¶ˆëŸ‰": "ê³ ê°ë‹˜, ë¶ˆí¸ì„ ë¼ì³ ì£„ì†¡í•©ë‹ˆë‹¤. êµí™˜ ë˜ëŠ” í™˜ë¶ˆ ì²˜ë¦¬ë¥¼ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
            "ê¸°íƒ€": "ê³ ê°ë‹˜ì˜ ë¬¸ì˜ì‚¬í•­ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ìµœì„ ì„ ë‹¤í•´ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤."
        }
        return responses.get(issue, responses["ê¸°íƒ€"])

    # Agent ìƒì„±
    agent = create_agent(
        model="gpt-4o-mini",
        tools=[handle_complaint],
    )

    # í‰ê°€ì ì´ˆê¸°í™”
    evaluator = CustomerServiceEvaluator()

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_cases = [
        "ë°°ì†¡ì´ ë„ˆë¬´ ëŠ¦ì–´ìš”. ì–¸ì œ ì˜¤ë‚˜ìš”?",
        "ë°›ì€ ì œí’ˆì´ íŒŒì†ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "í™˜ë¶ˆ ê°€ëŠ¥í•œê°€ìš”?",
        "ì„œë¹„ìŠ¤ê°€ í˜•í¸ì—†ë„¤ìš”!"
    ]

    print("\nì»¤ìŠ¤í…€ í‰ê°€ ì‹¤í–‰:")
    print("-" * 70)

    evaluation_results = []

    for i, user_query in enumerate(test_cases, 1):
        print(f"\n[í…ŒìŠ¤íŠ¸ {i}] ì‚¬ìš©ì: {user_query}")

        try:
            response = agent.invoke({
                "messages": [{"role": "user", "content": user_query}]
            })
            answer = response['messages'][-1].content

            print(f"  ì‘ë‹µ: {answer}")

            # ì»¤ìŠ¤í…€ í‰ê°€ ì‹¤í–‰
            eval_result = evaluator.evaluate(answer)

            print(f"\n  ğŸ“Š í‰ê°€ ê²°ê³¼:")
            print(f"     ì¢…í•© ì ìˆ˜: {eval_result['overall']:.1f}/100")
            print(f"     í†µê³¼ ì—¬ë¶€: {'âœ… PASS' if eval_result['passed'] else 'âŒ FAIL'}")
            print(f"\n     ì„¸ë¶€ ì ìˆ˜:")
            for metric, score in eval_result['scores'].items():
                status = "âœ…" if score >= 70 else "âš ï¸" if score >= 50 else "âŒ"
                print(f"       {status} {metric}: {score:.1f}/100")

            evaluation_results.append({
                "query": user_query,
                "answer": answer,
                "evaluation": eval_result,
                "passed": eval_result['passed']
            })

        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜: {e}")
            evaluation_results.append({
                "query": user_query,
                "passed": False
            })

    # ìµœì¢… ìš”ì•½
    print("\n" + "=" * 70)
    print("\nğŸ“Š ì»¤ìŠ¤í…€ í‰ê°€ ìš”ì•½:")

    pass_count = sum(1 for r in evaluation_results if r['passed'])
    total_count = len(evaluation_results)
    pass_rate = pass_count / total_count * 100

    print(f"   í†µê³¼ìœ¨: {pass_count}/{total_count} ({pass_rate:.1f}%)")

    # í‰ê·  ì ìˆ˜
    avg_scores = {}
    for r in evaluation_results:
        if 'evaluation' in r:
            for metric, score in r['evaluation']['scores'].items():
                if metric not in avg_scores:
                    avg_scores[metric] = []
                avg_scores[metric].append(score)

    print(f"\n   í‰ê·  ë©”íŠ¸ë¦­ ì ìˆ˜:")
    for metric, scores in avg_scores.items():
        avg = statistics.mean(scores)
        print(f"     â€¢ {metric}: {avg:.1f}/100")

    print("\nğŸ’¡ ì»¤ìŠ¤í…€ í‰ê°€ì ì‘ì„± íŒ:")
    print("   â€¢ ëª…í™•í•œ í‰ê°€ ê¸°ì¤€ ì •ì˜")
    print("   â€¢ ë„ë©”ì¸ ì „ë¬¸ê°€ì™€ í˜‘ì—…")
    print("   â€¢ ì ìˆ˜ì™€ í•¨ê»˜ ì„¤ëª… ì œê³µ")
    print("   â€¢ ì •ê¸°ì ìœ¼ë¡œ í‰ê°€ ê¸°ì¤€ ì—…ë°ì´íŠ¸")
    print("   â€¢ ì‚¬ëŒ í‰ê°€ì™€ ë³‘í–‰")


# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "=" * 70)
    print("ğŸ“ LangChain AI Agent ë§ˆìŠ¤í„° êµì•ˆ")
    print("ğŸ“– Part 10: ë°°í¬ì™€ ê´€ì¸¡ì„± - í‰ê°€ ë° ë²¤ì¹˜ë§ˆí¬")
    print("=" * 70 + "\n")

    # ì˜ˆì œ ì‹¤í–‰
    example_1_evaluation_metrics()
    input("\nâ ê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")

    example_2_evaluation_datasets()
    input("\nâ ê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")

    example_3_benchmarking()
    input("\nâ ê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")

    example_4_ab_testing()
    input("\nâ ê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")

    example_5_custom_evaluators()

    # ë§ˆë¬´ë¦¬
    print("\n" + "=" * 70)
    print("ğŸ‰ Part 10-04: í‰ê°€ ë° ë²¤ì¹˜ë§ˆí¬ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!")
    print("=" * 70)
    print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print("  1. 05_deployment.py - ë°°í¬")
    print("  2. 06_observability.py - ê´€ì¸¡ì„±")
    print("\nğŸ“š í•µì‹¬ ìš”ì•½:")
    print("  â€¢ í‰ê°€ ë©”íŠ¸ë¦­ìœ¼ë¡œ ì„±ëŠ¥ ì •ëŸ‰í™”")
    print("  â€¢ í‰ê°€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì²´ê³„ì  ê´€ë¦¬")
    print("  â€¢ ë²¤ì¹˜ë§ˆí‚¹ìœ¼ë¡œ ì„±ëŠ¥ ê¸°ì¤€ ìˆ˜ë¦½")
    print("  â€¢ A/B í…ŒìŠ¤íŠ¸ë¡œ ìµœì  ë²„ì „ ì„ íƒ")
    print("  â€¢ ì»¤ìŠ¤í…€ í‰ê°€ìë¡œ ë„ë©”ì¸ íŠ¹í™” í‰ê°€")
    print("\n" + "=" * 70 + "\n")


# ============================================================================
# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
# ============================================================================

if __name__ == "__main__":
    main()


# ============================================================================
# ğŸ“š ì¶”ê°€ í•™ìŠµ í¬ì¸íŠ¸
# ============================================================================
#
# 1. ê³ ê¸‰ í‰ê°€ ë©”íŠ¸ë¦­:
#    - BLEU, ROUGE (í…ìŠ¤íŠ¸ ìœ ì‚¬ë„)
#    - BERTScore (ì˜ë¯¸ì  ìœ ì‚¬ë„)
#    - Perplexity
#    - Human Evaluation
#
# 2. ìë™í™”ëœ í‰ê°€:
#    - LangSmith Evaluators
#    - Continuous Evaluation
#    - Regression Detection
#    - Alert on Performance Drop
#
# 3. í†µê³„ì  ë¶„ì„:
#    - t-test, ANOVA
#    - Confidence Intervals
#    - Effect Size
#    - Sample Size Calculation
#
# 4. í”„ë¡œë•ì…˜ í‰ê°€:
#    - Online Evaluation
#    - Shadow Mode
#    - Canary Deployment
#    - Blue-Green Deployment
#
# 5. ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­:
#    - User Satisfaction
#    - Task Completion Rate
#    - Time to Resolution
#    - Cost per Interaction
#
# ============================================================================
