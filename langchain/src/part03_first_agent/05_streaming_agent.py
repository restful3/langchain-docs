"""
================================================================================
LangChain AI Agent ë§ˆìŠ¤í„° êµì•ˆ
Part 3: ì²« ë²ˆì§¸ Agent ë§Œë“¤ê¸°
================================================================================

íŒŒì¼ëª…: 05_streaming_agent.py
ë‚œì´ë„: â­â­â­â˜†â˜† (ì¤‘ê¸‰)
ì˜ˆìƒ ì‹œê°„: 25ë¶„

ğŸ“š í•™ìŠµ ëª©í‘œ:
  - Streamingì˜ ê°œë…ê³¼ ì¥ì  ì´í•´
  - invoke() vs stream() ë¹„êµ
  - stream_mode ì¢…ë¥˜ (values, messages, updates)
  - ì‹¤ì‹œê°„ UI ì‹œë®¬ë ˆì´ì…˜ êµ¬í˜„

ğŸ“– ê³µì‹ ë¬¸ì„œ:
  â€¢ Streaming: /official/06-agents.md (ë¼ì¸ 461-476)
  â€¢ LangChain Streaming: /oss/python/langchain/streaming

ğŸ“„ êµì•ˆ ë¬¸ì„œ:
  â€¢ Part 3 ê°œìš”: /docs/part03_first_agent.md (ì„¹ì…˜ 5)

ğŸ”§ í•„ìš”í•œ íŒ¨í‚¤ì§€:
  pip install langchain langchain-openai python-dotenv

ğŸ”‘ í•„ìš”í•œ í™˜ê²½ë³€ìˆ˜:
  - OPENAI_API_KEY

ğŸš€ ì‹¤í–‰ ë°©ë²•:
  python 05_streaming_agent.py

================================================================================
"""

# ============================================================================
# Imports
# ============================================================================

import os
import sys
import time
from dotenv import load_dotenv
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langchain.tools import tool

# ============================================================================
# í™˜ê²½ ì„¤ì •
# ============================================================================

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í‚¤ í™•ì¸
if not os.getenv("OPENAI_API_KEY"):
    print("âŒ ì˜¤ë¥˜: OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ğŸ“ .env íŒŒì¼ì„ í™•ì¸í•˜ê³  API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    exit(1)

# ============================================================================
# ê³µí†µ ë„êµ¬ ì •ì˜
# ============================================================================

@tool
def get_weather(city: str) -> str:
    """ì£¼ì–´ì§„ ë„ì‹œì˜ í˜„ì¬ ë‚ ì”¨ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    Args:
        city: ë„ì‹œ ì´ë¦„ (ì˜ˆ: ì„œìš¸, ë¶€ì‚°)
    """
    time.sleep(1)  # API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
    weather_data = {
        "ì„œìš¸": "ë§‘ìŒ, 22Â°C, ìŠµë„ 60%",
        "ë¶€ì‚°": "íë¦¼, 20Â°C, ìŠµë„ 70%",
        "ì œì£¼": "ë¹„, 18Â°C, ìŠµë„ 85%",
    }
    return weather_data.get(city, f"{city}ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")


@tool
def get_forecast(city: str, days: int = 3) -> str:
    """ë©°ì¹ ê°„ì˜ ë‚ ì”¨ ì˜ˆë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    Args:
        city: ë„ì‹œ ì´ë¦„
        days: ì˜ˆë³´ ì¼ìˆ˜ (ê¸°ë³¸ 3ì¼)
    """
    time.sleep(1.5)  # API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
    forecasts = {
        "ì„œìš¸": "ë§‘ìŒ â†’ íë¦¼ â†’ ë¹„",
        "ë¶€ì‚°": "íë¦¼ â†’ ë¹„ â†’ ë§‘ìŒ",
        "ì œì£¼": "ë¹„ â†’ ë¹„ â†’ íë¦¼",
    }
    forecast = forecasts.get(city, "ì˜ˆë³´ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    return f"{city}ì˜ {days}ì¼ ì˜ˆë³´: {forecast}"


# ============================================================================
# ì˜ˆì œ 1: invoke() vs stream() ë¹„êµ
# ============================================================================

def example_1_invoke_vs_stream():
    """invoke()ì™€ stream()ì˜ ì°¨ì´ ì²´ê°í•˜ê¸°"""
    print("=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 1: invoke() vs stream() ë¹„êµ")
    print("=" * 70)

    model = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    agent = create_agent(
        model=model,
        tools=[get_weather, get_forecast],
        system_prompt="ë‹¹ì‹ ì€ ë‚ ì”¨ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì¹œì ˆí•œ Agentì…ë‹ˆë‹¤.",
    )

    question = {"messages": [{"role": "user", "content": "ì„œìš¸ì˜ í˜„ì¬ ë‚ ì”¨ì™€ 3ì¼ ì˜ˆë³´ë¥¼ ì•Œë ¤ì¤˜"}]}

    # ë°©ë²• 1: invoke() - ì™„ë£Œ í›„ ë°˜í™˜
    print("\nğŸ”¹ ë°©ë²• 1: invoke() - ì™„ë£Œ í›„ í•œ ë²ˆì— ë°˜í™˜")
    print("ğŸ‘¤ ì‚¬ìš©ì: ì„œìš¸ì˜ í˜„ì¬ ë‚ ì”¨ì™€ 3ì¼ ì˜ˆë³´ë¥¼ ì•Œë ¤ì¤˜")
    print("â³ Agentê°€ ì‘ì—… ì¤‘... (ëŒ€ê¸°)")

    start_time = time.time()
    result = agent.invoke(question)
    elapsed = time.time() - start_time

    print(f"âœ… ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ)")
    print(f"ğŸ¤– Agent: {result['messages'][-1].content}")

    print("\n" + "-" * 70)

    # ë°©ë²• 2: stream() - ì‹¤ì‹œê°„ ë°˜í™˜
    print("\nğŸ”¹ ë°©ë²• 2: stream() - ì‹¤ì‹œê°„ìœ¼ë¡œ ì¤‘ê°„ ê³¼ì • í‘œì‹œ")
    print("ğŸ‘¤ ì‚¬ìš©ì: ì„œìš¸ì˜ í˜„ì¬ ë‚ ì”¨ì™€ 3ì¼ ì˜ˆë³´ë¥¼ ì•Œë ¤ì¤˜")
    print("ğŸ¤– Agent: (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°)\n")

    start_time = time.time()
    final_answer = ""

    for chunk in agent.stream(question, stream_mode="values"):
        latest_message = chunk["messages"][-1]

        # ë„êµ¬ í˜¸ì¶œ í‘œì‹œ
        if hasattr(latest_message, "tool_calls") and latest_message.tool_calls:
            for tc in latest_message.tool_calls:
                print(f"   [ë„êµ¬ í˜¸ì¶œ] {tc['name']}({tc['args']}) ...")

        # ë„êµ¬ ê²°ê³¼ í‘œì‹œ
        elif latest_message.__class__.__name__ == "ToolMessage":
            print(f"   [ë„êµ¬ ê²°ê³¼] {latest_message.content[:50]}...")

        # ìµœì¢… ë‹µë³€
        elif hasattr(latest_message, "content") and latest_message.content and not hasattr(latest_message, "tool_calls"):
            final_answer = latest_message.content

    elapsed = time.time() - start_time
    print(f"\n   [ìµœì¢… ë‹µë³€] {final_answer}")
    print(f"\nâœ… ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ)")

    print("\nğŸ’¡ í•µì‹¬ ì°¨ì´:")
    print("  - invoke(): ëª¨ë“  ì‘ì—…ì´ ëë‚œ í›„ ê²°ê³¼ë§Œ ë°˜í™˜")
    print("  - stream(): ì‘ì—… ì§„í–‰ ì¤‘ ì¤‘ê°„ ìƒíƒœë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°˜í™˜")
    print("  - stream()ì€ ì‚¬ìš©ìì—ê²Œ 'ëŒ€ê¸° ì¤‘'ì´ ì•„ë‹ˆë¼ 'ì§„í–‰ ì¤‘'ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤\n")


# ============================================================================
# ì˜ˆì œ 2: stream_mode="values" - ì „ì²´ ìƒíƒœ ìŠ¤íŠ¸ë¦¬ë°
# ============================================================================

def example_2_stream_values_mode():
    """values ëª¨ë“œ: ë§¤ë²ˆ ì „ì²´ ìƒíƒœ ë°˜í™˜"""
    print("=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 2: stream_mode='values' - ì „ì²´ ìƒíƒœ")
    print("=" * 70)

    model = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    agent = create_agent(
        model=model,
        tools=[get_weather],
        system_prompt="ë‹¹ì‹ ì€ ê°„ê²°í•œ ë‚ ì”¨ Agentì…ë‹ˆë‹¤.",
    )

    print("\nğŸ‘¤ ì‚¬ìš©ì: ì„œìš¸ ë‚ ì”¨ëŠ”?")
    print("\nğŸ”„ stream_mode='values' ì‹¤í–‰:\n")

    chunk_count = 0
    for chunk in agent.stream(
        {"messages": [{"role": "user", "content": "ì„œìš¸ ë‚ ì”¨ëŠ”?"}]},
        stream_mode="values"  # ê¸°ë³¸ê°’
    ):
        chunk_count += 1
        messages = chunk["messages"]
        latest = messages[-1]

        print(f"[Chunk {chunk_count}]")
        print(f"  ì „ì²´ ë©”ì‹œì§€ ìˆ˜: {len(messages)}")
        print(f"  ìµœì‹  ë©”ì‹œì§€ íƒ€ì…: {latest.__class__.__name__}")
        if hasattr(latest, "content") and latest.content:
            print(f"  ë‚´ìš©: {latest.content[:50]}...")
        if hasattr(latest, "tool_calls") and latest.tool_calls:
            print(f"  ë„êµ¬ í˜¸ì¶œ: {[tc['name'] for tc in latest.tool_calls]}")
        print()

    print("ğŸ’¡ values ëª¨ë“œì˜ íŠ¹ì§•:")
    print("  - ë§¤ë²ˆ ì „ì²´ ìƒíƒœ(ëª¨ë“  ë©”ì‹œì§€)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤")
    print("  - ì „ì²´ ì»¨í…ìŠ¤íŠ¸ë¥¼ í•­ìƒ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    print("  - ë‹¨ì : ì¤‘ë³µ ë°ì´í„°ê°€ ë§ì•„ ë„¤íŠ¸ì›Œí¬ ë¶€ë‹´ì´ ìˆìŠµë‹ˆë‹¤\n")


# ============================================================================
# ì˜ˆì œ 3: stream_mode="messages" - ë©”ì‹œì§€ë§Œ ìŠ¤íŠ¸ë¦¬ë°
# ============================================================================

def example_3_stream_messages_mode():
    """messages ëª¨ë“œ: ë©”ì‹œì§€ë§Œ ë°˜í™˜"""
    print("=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 3: stream_mode='messages' - ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¼")
    print("=" * 70)

    model = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    agent = create_agent(
        model=model,
        tools=[get_weather],
        system_prompt="ë‹¹ì‹ ì€ ê°„ê²°í•œ ë‚ ì”¨ Agentì…ë‹ˆë‹¤.",
    )

    print("\nğŸ‘¤ ì‚¬ìš©ì: ë¶€ì‚° ë‚ ì”¨ëŠ”?")
    print("ğŸ¤– Agent: ", end="", flush=True)

    for msg_tuple in agent.stream(
        {"messages": [{"role": "user", "content": "ë¶€ì‚° ë‚ ì”¨ëŠ”?"}]},
        stream_mode="messages"
    ):
        # msg_tupleì€ (message, metadata) í˜•íƒœ
        message, metadata = msg_tuple

        # AIMessageì˜ contentë§Œ ì¶œë ¥ (ì‹¤ì‹œê°„ íƒ€ì´í•‘ íš¨ê³¼)
        if hasattr(message, "content") and message.content:
            print(message.content, end="", flush=True)

    print("\n")

    print("\nğŸ’¡ messages ëª¨ë“œì˜ íŠ¹ì§•:")
    print("  - ë©”ì‹œì§€ ê°ì²´ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤")
    print("  - UIì— ì§ì ‘ í‘œì‹œí•˜ê¸° ì‰½ìŠµë‹ˆë‹¤")
    print("  - íƒ€ì´í•‘ íš¨ê³¼ êµ¬í˜„ì— ì í•©í•©ë‹ˆë‹¤\n")


# ============================================================================
# ì˜ˆì œ 4: stream_mode="updates" - ë³€ê²½ì‚¬í•­ë§Œ ìŠ¤íŠ¸ë¦¬ë°
# ============================================================================

def example_4_stream_updates_mode():
    """updates ëª¨ë“œ: ê° ë‹¨ê³„ì˜ ë³€ê²½ì‚¬í•­ë§Œ ë°˜í™˜"""
    print("=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 4: stream_mode='updates' - ë³€ê²½ì‚¬í•­ë§Œ")
    print("=" * 70)

    model = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    agent = create_agent(
        model=model,
        tools=[get_weather],
        system_prompt="ë‹¹ì‹ ì€ ê°„ê²°í•œ ë‚ ì”¨ Agentì…ë‹ˆë‹¤.",
    )

    print("\nğŸ‘¤ ì‚¬ìš©ì: ì œì£¼ ë‚ ì”¨ëŠ”?")
    print("\nğŸ”„ stream_mode='updates' ì‹¤í–‰:\n")

    update_count = 0
    for update in agent.stream(
        {"messages": [{"role": "user", "content": "ì œì£¼ ë‚ ì”¨ëŠ”?"}]},
        stream_mode="updates"
    ):
        update_count += 1
        print(f"[Update {update_count}]")
        print(f"  ë…¸ë“œ: {list(update.keys())}")

        for node_name, node_data in update.items():
            if "messages" in node_data:
                messages = node_data["messages"]
                print(f"  ì¶”ê°€ëœ ë©”ì‹œì§€ ìˆ˜: {len(messages)}")
                for msg in messages:
                    print(f"    - {msg.__class__.__name__}")
        print()

    print("ğŸ’¡ updates ëª¨ë“œì˜ íŠ¹ì§•:")
    print("  - ê° ë‹¨ê³„ì—ì„œ ì¶”ê°€ëœ ë‚´ìš©ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤")
    print("  - ë„¤íŠ¸ì›Œí¬ íš¨ìœ¨ì ì…ë‹ˆë‹¤ (ì¤‘ë³µ ì—†ìŒ)")
    print("  - ì „ì²´ ì»¨í…ìŠ¤íŠ¸ëŠ” ì§ì ‘ ê´€ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤\n")


# ============================================================================
# ì˜ˆì œ 5: ì‹¤ì‹œê°„ UI ì‹œë®¬ë ˆì´ì…˜
# ============================================================================

def example_5_realtime_ui_simulation():
    """ì‹¤ì œ ì±—ë´‡ì²˜ëŸ¼ ì‹¤ì‹œê°„ ì‘ë‹µ í‘œì‹œ"""
    print("=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 5: ì‹¤ì‹œê°„ UI ì‹œë®¬ë ˆì´ì…˜ (ì±—ë´‡)")
    print("=" * 70)

    model = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    agent = create_agent(
        model=model,
        tools=[get_weather, get_forecast],
        system_prompt="ë‹¹ì‹ ì€ ì¹œì ˆí•œ ë‚ ì”¨ ë¹„ì„œì…ë‹ˆë‹¤. ë‹µë³€ì€ ê°„ê²°í•˜ê²Œ ì œê³µí•˜ì„¸ìš”.",
    )

    print("\n" + "=" * 70)
    print("ğŸ’¬ ë‚ ì”¨ ì±—ë´‡ (ì‹¤ì‹œê°„ ëª¨ë“œ)")
    print("=" * 70)

    # ì‹œë®¬ë ˆì´ì…˜í•  ëŒ€í™”
    conversations = [
        "ì„œìš¸ ë‚ ì”¨ ì•Œë ¤ì¤˜",
        "ë¶€ì‚°ê³¼ ì œì£¼ ì¤‘ ì–´ë””ê°€ ë” ë”°ëœ»í•´?",
    ]

    for query in conversations:
        print(f"\nğŸ‘¤ ì‚¬ìš©ì: {query}")
        print("ğŸ¤– Agent: ", end="", flush=True)

        current_status = ""
        final_content = ""

        for chunk in agent.stream(
            {"messages": [{"role": "user", "content": query}]},
            stream_mode="values"
        ):
            latest_message = chunk["messages"][-1]
            msg_type = latest_message.__class__.__name__

            # ìƒíƒœ í‘œì‹œ ì—…ë°ì´íŠ¸
            if msg_type == "AIMessage" and hasattr(latest_message, "tool_calls") and latest_message.tool_calls:
                # ë„êµ¬ í˜¸ì¶œ ì¤‘
                if current_status != "tool_calling":
                    print("\n   [ì •ë³´ ì¡°íšŒ ì¤‘", end="", flush=True)
                    current_status = "tool_calling"
                else:
                    print(".", end="", flush=True)

            elif msg_type == "ToolMessage":
                # ë„êµ¬ ê²°ê³¼ ìˆ˜ì‹ 
                if current_status == "tool_calling":
                    print("]", end="", flush=True)
                    current_status = "tool_received"

            elif msg_type == "AIMessage" and hasattr(latest_message, "content") and latest_message.content:
                # ìµœì¢… ë‹µë³€
                if current_status in ["tool_calling", "tool_received"]:
                    print("\n   ", end="", flush=True)
                    current_status = "answering"

                # íƒ€ì´í•‘ íš¨ê³¼ (ìƒˆë¡œìš´ ë‚´ìš©ë§Œ ì¶œë ¥)
                new_content = latest_message.content[len(final_content):]
                if new_content:
                    for char in new_content:
                        print(char, end="", flush=True)
                        time.sleep(0.02)  # íƒ€ì´í•‘ íš¨ê³¼
                    final_content = latest_message.content

        print("\n")

    print("\n" + "=" * 70)

    print("\nğŸ’¡ ì‹¤ì „ UI êµ¬í˜„ í¬ì¸íŠ¸:")
    print("  1. ë„êµ¬ í˜¸ì¶œ ì¤‘: ë¡œë”© ì¸ë””ì¼€ì´í„° í‘œì‹œ")
    print("  2. ë„êµ¬ ì™„ë£Œ: ì²´í¬ë§ˆí¬ ë˜ëŠ” ì™„ë£Œ ë©”ì‹œì§€")
    print("  3. ìµœì¢… ë‹µë³€: íƒ€ì´í•‘ íš¨ê³¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í‘œì‹œ")
    print("  4. ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‚¬ìš©ì ëŒ€ê¸° ì‹œê°„ ì²´ê° ê°ì†Œ\n")


# ============================================================================
# ë³´ë„ˆìŠ¤: ì§„í–‰ ìƒí™© ë°” í‘œì‹œ
# ============================================================================

def bonus_progress_bar():
    """ì§„í–‰ ìƒí™© ë°”ì™€ í•¨ê»˜ ìŠ¤íŠ¸ë¦¬ë°"""
    print("=" * 70)
    print("ğŸ“Œ ë³´ë„ˆìŠ¤: ì§„í–‰ ìƒí™© ë°” í‘œì‹œ")
    print("=" * 70)

    model = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    agent = create_agent(
        model=model,
        tools=[get_weather, get_forecast],
        system_prompt="ë‹¹ì‹ ì€ ë‚ ì”¨ Agentì…ë‹ˆë‹¤.",
    )

    print("\nğŸ‘¤ ì‚¬ìš©ì: ì„œìš¸ì˜ ë‚ ì”¨ì™€ ì˜ˆë³´ë¥¼ ì•Œë ¤ì¤˜")
    print()

    steps = []
    total_steps = 0

    # 1ë‹¨ê³„: ìŠ¤íŠ¸ë¦¼ ìˆ˜ì§‘ ë° ë‹¨ê³„ íŒŒì•…
    print("ğŸ” ì‘ì—… ë¶„ì„ ì¤‘...", end="", flush=True)
    for chunk in agent.stream(
        {"messages": [{"role": "user", "content": "ì„œìš¸ì˜ ë‚ ì”¨ì™€ ì˜ˆë³´ë¥¼ ì•Œë ¤ì¤˜"}]},
        stream_mode="values"
    ):
        latest = chunk["messages"][-1]
        if hasattr(latest, "tool_calls") and latest.tool_calls:
            for tc in latest.tool_calls:
                steps.append(tc["name"])
                total_steps += 1

    print(f" ì™„ë£Œ! (ì´ {total_steps}ê°œ ì‘ì—…)\n")

    # 2ë‹¨ê³„: ì¬ì‹¤í–‰í•˜ë©° ì§„í–‰ ë°” í‘œì‹œ
    current_step = 0

    def print_progress(current, total, message=""):
        percent = int((current / total) * 100)
        bar_length = 30
        filled = int((bar_length * current) / total)
        bar = "â–ˆ" * filled + "â–‘" * (bar_length - filled)
        print(f"\r[{bar}] {percent}% - {message}", end="", flush=True)

    for chunk in agent.stream(
        {"messages": [{"role": "user", "content": "ì„œìš¸ì˜ ë‚ ì”¨ì™€ ì˜ˆë³´ë¥¼ ì•Œë ¤ì¤˜"}]},
        stream_mode="values"
    ):
        latest = chunk["messages"][-1]
        msg_type = latest.__class__.__name__

        if msg_type == "AIMessage" and hasattr(latest, "tool_calls") and latest.tool_calls:
            for tc in latest.tool_calls:
                current_step += 1
                print_progress(current_step, total_steps, f"{tc['name']} ì‹¤í–‰ ì¤‘...")
                time.sleep(0.5)

        elif msg_type == "ToolMessage":
            print_progress(current_step, total_steps, "ì™„ë£Œ")

    print("\n")

    # ìµœì¢… ë‹µë³€
    final_result = agent.invoke({"messages": [{"role": "user", "content": "ì„œìš¸ì˜ ë‚ ì”¨ì™€ ì˜ˆë³´ë¥¼ ì•Œë ¤ì¤˜"}]})
    print(f"ğŸ¤– Agent: {final_result['messages'][-1].content}")

    print("\nğŸ’¡ ì§„í–‰ ë°”ì˜ íš¨ê³¼:")
    print("  - ì‚¬ìš©ìê°€ ì‘ì—… ì§„í–‰ë„ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸")
    print("  - ëŒ€ê¸° ì‹œê°„ì´ ì§§ê²Œ ëŠê»´ì§")
    print("  - ì–´ëŠ ë‹¨ê³„ì—ì„œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ”ì§€ íŒŒì•… ê°€ëŠ¥\n")


# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n")
    print("ğŸ“ LangChain AI Agent ë§ˆìŠ¤í„° êµì•ˆ")
    print("ğŸ“– Part 3: ì²« ë²ˆì§¸ Agent ë§Œë“¤ê¸° - Streaming Agent")
    print("\n")

    # ëª¨ë“  ì˜ˆì œ ì‹¤í–‰
    example_1_invoke_vs_stream()
    input("\nâ ê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")

    example_2_stream_values_mode()
    input("\nâ ê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")

    example_3_stream_messages_mode()
    input("\nâ ê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")

    example_4_stream_updates_mode()
    input("\nâ ê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")

    example_5_realtime_ui_simulation()
    input("\nâ ê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")

    bonus_progress_bar()

    # ë§ˆë¬´ë¦¬
    print("\n" + "=" * 70)
    print("ğŸ‰ Streaming Agent ì˜ˆì œë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!")
    print("=" * 70)
    print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print("  1. Part 4: Memory & Context Management")
    print("  2. ì‹¤ìŠµ ê³¼ì œ: ìŠ¤íŠ¸ë¦¬ë°ì„ í™œìš©í•œ ì±—ë´‡ ë§Œë“¤ê¸°")
    print("\nğŸ“š Streaming í™œìš© íŒ:")
    print("  â€¢ ì›¹ UI: stream_mode='messages'ë¡œ íƒ€ì´í•‘ íš¨ê³¼")
    print("  â€¢ ëª¨ë‹ˆí„°ë§: stream_mode='values'ë¡œ ì „ì²´ ìƒíƒœ ì¶”ì ")
    print("  â€¢ íš¨ìœ¨ì„±: stream_mode='updates'ë¡œ ë„¤íŠ¸ì›Œí¬ ìµœì í™”")
    print("  â€¢ ì‚¬ìš©ì ê²½í—˜: ì§„í–‰ ìƒí™© í‘œì‹œë¡œ ëŒ€ê¸° ì‹œê°„ ì²´ê° ê°ì†Œ")
    print("\n" + "=" * 70 + "\n")


# ============================================================================
# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
# ============================================================================

if __name__ == "__main__":
    main()


# ============================================================================
# ğŸ“š ì¶”ê°€ í•™ìŠµ í¬ì¸íŠ¸
# ============================================================================
#
# 1. Streaming ëª¨ë“œ ì„ íƒ ê°€ì´ë“œ:
#    - stream_mode="values": ë””ë²„ê¹…, ì „ì²´ ìƒíƒœ í•„ìš”
#    - stream_mode="messages": UI êµ¬í˜„, íƒ€ì´í•‘ íš¨ê³¼
#    - stream_mode="updates": ë„¤íŠ¸ì›Œí¬ ìµœì í™”, íš¨ìœ¨ì„±
#
# 2. íƒ€ì´í•‘ íš¨ê³¼ êµ¬í˜„:
#    for msg_tuple in agent.stream(..., stream_mode="messages"):
#        message, metadata = msg_tuple
#        if hasattr(message, "content"):
#            print(message.content, end="", flush=True)
#
# 3. ì›¹ í”„ë ˆì„ì›Œí¬ ì—°ë™:
#    - Streamlit: st.write_stream()
#    - FastAPI: StreamingResponse()
#    - Gradio: gr.ChatInterface()
#
# 4. ì—ëŸ¬ ì²˜ë¦¬:
#    try:
#        for chunk in agent.stream(...):
#            process(chunk)
#    except Exception as e:
#        print(f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
#        # í´ë°± ì²˜ë¦¬
#
# 5. ì„±ëŠ¥ ìµœì í™”:
#    - í•„ìš”í•œ ì •ë³´ë§Œ ìŠ¤íŠ¸ë¦¬ë°
#    - ë²„í¼ë§ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ íš¨ìœ¨ í–¥ìƒ
#    - íƒ€ì„ì•„ì›ƒ ì„¤ì •
#
# ============================================================================
# ğŸ› ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ
# ============================================================================
#
# ë¬¸ì œ: ìŠ¤íŠ¸ë¦¬ë°ì´ ë©ˆì¶¤ (ì‘ë‹µ ì—†ìŒ)
# í•´ê²°:
#   - timeout ì„¤ì • í™•ì¸
#   - ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ í™•ì¸
#   - ë„êµ¬ê°€ ë¬´í•œ ë£¨í”„ì— ë¹ ì§€ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
#
# ë¬¸ì œ: ì¶œë ¥ì´ ë²„í¼ë§ë˜ì–´ í•œ ë²ˆì— ë‚˜ì˜´
# í•´ê²°:
#   - flush=True ì˜µì…˜ ì‚¬ìš©
#   - sys.stdout.flush() ëª…ì‹œì  í˜¸ì¶œ
#   - í™˜ê²½ë³€ìˆ˜ PYTHONUNBUFFERED=1 ì„¤ì •
#
# ë¬¸ì œ: stream_mode="messages"ì—ì„œ ë„êµ¬ í˜¸ì¶œ ì •ë³´ê°€ ì•ˆ ë³´ì„
# í•´ê²°:
#   - stream_mode="values" ì‚¬ìš©
#   - ë˜ëŠ” tool_calls í™•ì¸ ë¡œì§ ì¶”ê°€
#
# ë¬¸ì œ: ì›¹ UIì—ì„œ ìŠ¤íŠ¸ë¦¬ë°ì´ ì‘ë™í•˜ì§€ ì•ŠìŒ
# í•´ê²°:
#   - Server-Sent Events (SSE) ì‚¬ìš©
#   - WebSocket ì—°ê²° ì‚¬ìš©
#   - í”„ë ˆì„ì›Œí¬ë³„ ìŠ¤íŠ¸ë¦¬ë° API í™•ì¸
#
# ============================================================================
