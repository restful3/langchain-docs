"""
================================================================================
LangChain AI Agent ë§ˆìŠ¤í„° êµì•ˆ
Part 8: RAG & MCP
================================================================================

íŒŒì¼ëª…: 02_vector_store.py
ë‚œì´ë„: â­â­â­â­ (ì¤‘ìƒê¸‰)
ì˜ˆìƒ ì‹œê°„: 35ë¶„

ğŸ“š í•™ìŠµ ëª©í‘œ:
  - Chroma vector store ì‚¬ìš©ë²• ë§ˆìŠ¤í„°
  - ë‹¤ì–‘í•œ Document Loader í™œìš©
  - Text chunking ì „ëµ ì´í•´ ë° ì ìš©
  - Embedding ìƒì„± ë° ì €ì¥
  - ì‹¤ì „ ë¬¸ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ êµ¬ì¶•

ğŸ“– ê³µì‹ ë¬¸ì„œ:
  â€¢ Vector Stores: https://python.langchain.com/docs/concepts/vectorstores/
  â€¢ Document Loaders: https://python.langchain.com/docs/concepts/document_loaders/

ğŸ“„ êµì•ˆ ë¬¸ì„œ:
  â€¢ Part 8: /docs/part08_rag_mcp.md

ğŸ”§ í•„ìš”í•œ íŒ¨í‚¤ì§€:
  pip install langchain langchain-openai langchain-community chromadb python-dotenv

ğŸš€ ì‹¤í–‰ ë°©ë²•:
  python 02_vector_store.py

================================================================================
"""

# ============================================================================
# Imports
# ============================================================================

import os
import tempfile
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.schema import Document
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    CharacterTextSplitter,
)

# ============================================================================
# í™˜ê²½ ì„¤ì •
# ============================================================================

load_dotenv()

if not os.getenv("OPENAI_API_KEY"):
    print("âŒ ì˜¤ë¥˜: OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ğŸ“ .env íŒŒì¼ì„ í™•ì¸í•˜ê³  API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    exit(1)

# ============================================================================
# ì˜ˆì œ 1: Chroma Vector Store ê¸°ë³¸ ì‚¬ìš©ë²•
# ============================================================================

def example_1_chroma_basics():
    """Chroma vector store ê¸°ë³¸ ì‚¬ìš©ë²•"""
    print("=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 1: Chroma Vector Store ê¸°ë³¸ ì‚¬ìš©ë²•")
    print("=" * 70)

    print("""
ğŸ’¡ Chromaë€?
   - ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”© ë°ì´í„°ë² ì´ìŠ¤
   - ë¡œì»¬/ì„œë²„ ëª¨ë“œ ì§€ì›
   - ë©”íƒ€ë°ì´í„° í•„í„°ë§ ê°•ë ¥
   - ì‚¬ìš©ì´ ê°„ë‹¨í•˜ê³  ì§ê´€ì 

ì¥ì :
   â€¢ ë¡œì»¬ ì‹¤í–‰ ê°€ëŠ¥ (í”„ë¼ì´ë²„ì‹œ)
   â€¢ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§
   â€¢ ìë™ ì˜êµ¬ ì €ì¥
   â€¢ ë¹ ë¥¸ ê²€ìƒ‰ ì„±ëŠ¥
    """)

    # ìƒ˜í”Œ í…ìŠ¤íŠ¸
    texts = [
        "íŒŒì´ì¬ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
        "ìë°”ìŠ¤í¬ë¦½íŠ¸ëŠ” ì›¹ ê°œë°œì— ì‚¬ìš©ë©ë‹ˆë‹¤.",
        "SQLì€ ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì–¸ì–´ì…ë‹ˆë‹¤.",
        "DockerëŠ” ì»¨í…Œì´ë„ˆ í”Œë«í¼ì…ë‹ˆë‹¤.",
        "KubernetesëŠ” ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë„êµ¬ì…ë‹ˆë‹¤.",
    ]

    print("\nğŸ“š ì €ì¥í•  í…ìŠ¤íŠ¸:")
    for i, text in enumerate(texts, 1):
        print(f"  {i}. {text}")

    # Embeddings ì´ˆê¸°í™”
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # Chroma ìƒì„± (ì„ì‹œ ë””ë ‰í† ë¦¬)
    with tempfile.TemporaryDirectory() as temp_dir:
        print(f"\nğŸ”§ Chroma ì´ˆê¸°í™” ì¤‘...")

        vectorstore = Chroma.from_texts(
            texts=texts,
            embedding=embeddings,
            persist_directory=temp_dir,
            collection_name="tech_docs"
        )

        print("âœ… Vector Store ìƒì„± ì™„ë£Œ!")

        # ê¸°ë³¸ ê²€ìƒ‰
        query = "í”„ë¡œê·¸ë˜ë°"
        print(f"\nğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{query}'")

        results = vectorstore.similarity_search(query, k=3)

        print("\nğŸ“„ ê²€ìƒ‰ ê²°ê³¼:")
        for i, doc in enumerate(results, 1):
            print(f"  {i}. {doc.page_content}")

        # ì»¬ë ‰ì…˜ ì •ë³´
        print("\nğŸ“Š ì»¬ë ‰ì…˜ í†µê³„:")
        collection = vectorstore._collection
        print(f"  â€¢ ì´ ë¬¸ì„œ ìˆ˜: {collection.count()}")
        print(f"  â€¢ ì»¬ë ‰ì…˜ ì´ë¦„: {collection.name}")

    print("\n" + "=" * 70)


# ============================================================================
# ì˜ˆì œ 2: ë¬¸ì„œ ë¡œë”© - TextLoaderì™€ CSVLoader
# ============================================================================

def example_2_document_loaders():
    """ë‹¤ì–‘í•œ Document Loader ì‚¬ìš©ë²•"""
    print("\n" + "=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 2: Document Loaders")
    print("=" * 70)

    print("""
ğŸ’¡ Document Loaderë€?
   - ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë¬¸ì„œë¥¼ ë¡œë“œ
   - ìë™ìœ¼ë¡œ Document ê°ì²´ ìƒì„±
   - ë©”íƒ€ë°ì´í„° í¬í•¨

ì£¼ìš” Loader:
   â€¢ TextLoader: í…ìŠ¤íŠ¸ íŒŒì¼
   â€¢ CSVLoader: CSV íŒŒì¼
   â€¢ PDFLoader: PDF íŒŒì¼
   â€¢ WebBaseLoader: ì›¹ í˜ì´ì§€
    """)

    # ì„ì‹œ íŒŒì¼ ìƒì„± ë° ë¡œë“œ
    with tempfile.TemporaryDirectory() as temp_dir:
        # 1. í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
        txt_path = os.path.join(temp_dir, "sample.txt")
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write("""LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
AgentëŠ” ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
RAGëŠ” ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€ í’ˆì§ˆì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
Vector StoreëŠ” ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ì €ì¥í•˜ê³  ê²€ìƒ‰í•©ë‹ˆë‹¤.""")

        print("\n1ï¸âƒ£ TextLoader ì‚¬ìš©")
        print("-" * 70)

        from langchain_community.document_loaders import TextLoader

        loader = TextLoader(txt_path, encoding='utf-8')
        docs = loader.load()

        print(f"âœ… ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")
        print(f"\nğŸ“„ ë¬¸ì„œ ë‚´ìš©:\n{docs[0].page_content}")
        print(f"\nğŸ·ï¸  ë©”íƒ€ë°ì´í„°: {docs[0].metadata}")

        # 2. CSV íŒŒì¼ ìƒì„± ë° ë¡œë“œ
        csv_path = os.path.join(temp_dir, "products.csv")
        with open(csv_path, 'w', encoding='utf-8') as f:
            f.write("""product_id,name,category,price,description
1,ë…¸íŠ¸ë¶,ì „ìì œí’ˆ,1200000,ê³ ì„±ëŠ¥ í”„ë¡œê·¸ë˜ë° ë…¸íŠ¸ë¶
2,ë§ˆìš°ìŠ¤,ì „ìì œí’ˆ,35000,ë¬´ì„  ë§ˆìš°ìŠ¤
3,í‚¤ë³´ë“œ,ì „ìì œí’ˆ,89000,ê¸°ê³„ì‹ í‚¤ë³´ë“œ
4,ëª¨ë‹ˆí„°,ì „ìì œí’ˆ,350000,27ì¸ì¹˜ 4K ëª¨ë‹ˆí„°
5,ì±…ìƒ,ê°€êµ¬,150000,ë†’ì´ ì¡°ì ˆ ì±…ìƒ""")

        print("\n2ï¸âƒ£ CSVLoader ì‚¬ìš©")
        print("-" * 70)

        from langchain_community.document_loaders import CSVLoader

        loader = CSVLoader(
            file_path=csv_path,
            encoding='utf-8',
            csv_args={'delimiter': ','}
        )
        docs = loader.load()

        print(f"âœ… ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")
        print("\nğŸ“„ ìƒ˜í”Œ ë¬¸ì„œ (ì²˜ìŒ 2ê°œ):")
        for i, doc in enumerate(docs[:2], 1):
            print(f"\n  ë¬¸ì„œ {i}:")
            print(f"  ë‚´ìš©: {doc.page_content[:100]}...")
            print(f"  ë©”íƒ€ë°ì´í„°: {doc.metadata}")

        # 3. Vector Storeì— ì €ì¥
        print("\n3ï¸âƒ£ Vector Storeì— ì €ì¥")
        print("-" * 70)

        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        vectorstore = Chroma.from_documents(
            documents=docs,
            embedding=embeddings,
            collection_name="products"
        )

        print("âœ… Vector Store ìƒì„± ì™„ë£Œ!")

        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        query = "í”„ë¡œê·¸ë˜ë°ì— í•„ìš”í•œ ì œí’ˆ"
        print(f"\nğŸ” ê²€ìƒ‰: '{query}'")

        results = vectorstore.similarity_search(query, k=2)

        print("\nğŸ“„ ê²€ìƒ‰ ê²°ê³¼:")
        for i, doc in enumerate(results, 1):
            print(f"\n  {i}. {doc.page_content[:80]}...")

    print("\n" + "=" * 70)


# ============================================================================
# ì˜ˆì œ 3: Text Chunking ì „ëµ
# ============================================================================

def example_3_text_chunking():
    """ë‹¤ì–‘í•œ Text Splitting ì „ëµ"""
    print("\n" + "=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 3: Text Chunking ì „ëµ")
    print("=" * 70)

    print("""
ğŸ’¡ Text Chunkingì´ë€?
   - ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë¶„í• 
   - ê²€ìƒ‰ í’ˆì§ˆê³¼ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì— ì¤‘ìš”
   - ì ì ˆí•œ í¬ê¸°ì™€ ê²¹ì¹¨ì´ í•µì‹¬

ì „ëµ:
   â€¢ RecursiveCharacterTextSplitter: ì¬ê·€ì  ë¶„í•  (ê¶Œì¥)
   â€¢ CharacterTextSplitter: ë‹¨ìˆœ ë¬¸ì ê¸°ë°˜ ë¶„í• 
   â€¢ ì ì ˆí•œ chunk_sizeì™€ overlap ì„¤ì •
    """)

    # ê¸´ ë¬¸ì„œ ìƒ˜í”Œ
    long_text = """
ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ë¡œ, ì»´í“¨í„°ê°€ ëª…ì‹œì ìœ¼ë¡œ í”„ë¡œê·¸ë˜ë°ë˜ì§€ ì•Šê³ ë„ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

ì§€ë„ í•™ìŠµì€ ë ˆì´ë¸”ì´ ìˆëŠ” ë°ì´í„°ë¡œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë¶„ë¥˜ì™€ íšŒê·€ê°€ ëŒ€í‘œì ì¸ ì˜ˆì…ë‹ˆë‹¤. ë¶„ë¥˜ëŠ” ë°ì´í„°ë¥¼ ë²”ì£¼ë¡œ ë‚˜ëˆ„ê³ , íšŒê·€ëŠ” ì—°ì†ì ì¸ ê°’ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

ë¹„ì§€ë„ í•™ìŠµì€ ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ ì°¾ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„°ë§ê³¼ ì°¨ì› ì¶•ì†Œê°€ ì£¼ìš” ê¸°ë²•ì…ë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„°ë§ì€ ìœ ì‚¬í•œ ë°ì´í„°ë¥¼ ê·¸ë£¹í™”í•˜ê³ , ì°¨ì› ì¶•ì†ŒëŠ” ë°ì´í„°ì˜ íŠ¹ì§•ì„ ì••ì¶•í•©ë‹ˆë‹¤.

ê°•í™” í•™ìŠµì€ ì—ì´ì „íŠ¸ê°€ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° ë³´ìƒì„ ìµœëŒ€í™”í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤. ê²Œì„ AIì™€ ë¡œë´‡ ì œì–´ì— ë§ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.

ë”¥ëŸ¬ë‹ì€ ì¸ê³µì‹ ê²½ë§ì„ ì—¬ëŸ¬ ì¸µìœ¼ë¡œ ìŒ“ì•„ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ ì¸ì‹, ìì—°ì–´ ì²˜ë¦¬, ìŒì„± ì¸ì‹ ë“±ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
    """.strip()

    print(f"\nğŸ“„ ì›ë³¸ ë¬¸ì„œ ê¸¸ì´: {len(long_text)} ë¬¸ì")

    # 1. RecursiveCharacterTextSplitter
    print("\n1ï¸âƒ£ RecursiveCharacterTextSplitter")
    print("-" * 70)

    splitter1 = RecursiveCharacterTextSplitter(
        chunk_size=200,
        chunk_overlap=50,
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )

    chunks1 = splitter1.split_text(long_text)

    print(f"âœ… ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks1)}")
    print(f"ì„¤ì •: chunk_size=200, overlap=50")

    print("\nğŸ“¦ ì²­í¬ ë¯¸ë¦¬ë³´ê¸°:")
    for i, chunk in enumerate(chunks1[:3], 1):
        print(f"\n  ì²­í¬ {i} (ê¸¸ì´: {len(chunk)}):")
        print(f"  {chunk[:100]}...")

    # 2. CharacterTextSplitter
    print("\n2ï¸âƒ£ CharacterTextSplitter")
    print("-" * 70)

    splitter2 = CharacterTextSplitter(
        chunk_size=200,
        chunk_overlap=50,
        separator="\n\n"
    )

    chunks2 = splitter2.split_text(long_text)

    print(f"âœ… ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks2)}")
    print(f"ì„¤ì •: chunk_size=200, overlap=50, separator='\\n\\n'")

    # 3. ë‹¤ì–‘í•œ í¬ê¸° ë¹„êµ
    print("\n3ï¸âƒ£ ì²­í¬ í¬ê¸°ë³„ ë¹„êµ")
    print("-" * 70)

    sizes = [100, 300, 500]

    for size in sizes:
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=size,
            chunk_overlap=50
        )
        chunks = splitter.split_text(long_text)
        avg_length = sum(len(c) for c in chunks) / len(chunks)

        print(f"\n  chunk_size={size}: {len(chunks)}ê°œ ì²­í¬, í‰ê·  ê¸¸ì´={avg_length:.0f}")

    # 4. Documentë¡œ ë¶„í• 
    print("\n4ï¸âƒ£ Document ê°ì²´ë¡œ ë¶„í• ")
    print("-" * 70)

    doc = Document(
        page_content=long_text,
        metadata={"source": "ml_guide.txt", "chapter": 1}
    )

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=250,
        chunk_overlap=50
    )

    split_docs = splitter.split_documents([doc])

    print(f"âœ… ë¶„í• ëœ ë¬¸ì„œ ìˆ˜: {len(split_docs)}")
    print(f"\nğŸ“„ ì²« ë²ˆì§¸ ë¶„í•  ë¬¸ì„œ:")
    print(f"  ë‚´ìš©: {split_docs[0].page_content[:100]}...")
    print(f"  ë©”íƒ€ë°ì´í„°: {split_docs[0].metadata}")

    print("\nğŸ’¡ ì²­í‚¹ ê°€ì´ë“œ:")
    print("  â€¢ FAQ, ì§§ì€ ë‹µë³€: 200-500")
    print("  â€¢ ì¼ë°˜ ë¬¸ì„œ: 1000-1500")
    print("  â€¢ ê¸´ ê¸°ìˆ  ë¬¸ì„œ: 1500-2000")
    print("  â€¢ overlapì€ chunk_sizeì˜ 10-20% ê¶Œì¥")

    print("\n" + "=" * 70)


# ============================================================================
# ì˜ˆì œ 4: Embedding ìƒì„± ë° ì €ì¥
# ============================================================================

def example_4_embeddings():
    """Embedding ìƒì„± ë° Vector Store ì €ì¥"""
    print("\n" + "=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 4: Embedding ìƒì„± ë° ì €ì¥")
    print("=" * 70)

    print("""
ğŸ’¡ Embeddingì´ë€?
   - í…ìŠ¤íŠ¸ë¥¼ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
   - ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° ê°€ëŠ¥
   - Vector Storeì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ

ëª¨ë¸ ì„ íƒ:
   â€¢ text-embedding-3-small: ë¹ ë¥´ê³  ì €ë ´ (ê¶Œì¥)
   â€¢ text-embedding-3-large: ìµœê³  í’ˆì§ˆ
   â€¢ text-embedding-ada-002: êµ¬ë²„ì „
    """)

    # ë¬¸ì„œ ì¤€ë¹„
    documents = [
        Document(
            page_content="Pythonì€ ë°ì´í„° ê³¼í•™ê³¼ ì›¹ ê°œë°œì— ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
            metadata={"category": "programming", "language": "python"}
        ),
        Document(
            page_content="JavaScriptëŠ” í”„ë¡ íŠ¸ì—”ë“œì™€ ë°±ì—”ë“œ ì›¹ ê°œë°œì„ ìœ„í•œ ì–¸ì–´ì…ë‹ˆë‹¤.",
            metadata={"category": "programming", "language": "javascript"}
        ),
        Document(
            page_content="TensorFlowëŠ” êµ¬ê¸€ì´ ê°œë°œí•œ ë¨¸ì‹ ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.",
            metadata={"category": "ml", "framework": "tensorflow"}
        ),
        Document(
            page_content="PyTorchëŠ” í˜ì´ìŠ¤ë¶ì´ ê°œë°œí•œ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.",
            metadata={"category": "ml", "framework": "pytorch"}
        ),
        Document(
            page_content="DockerëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì»¨í…Œì´ë„ˆë¡œ íŒ¨í‚¤ì§•í•˜ëŠ” í”Œë«í¼ì…ë‹ˆë‹¤.",
            metadata={"category": "devops", "tool": "docker"}
        ),
    ]

    print(f"\nğŸ“š ë¬¸ì„œ ìˆ˜: {len(documents)}")

    # 1. Embeddings ëª¨ë¸ ì´ˆê¸°í™”
    print("\n1ï¸âƒ£ Embeddings ëª¨ë¸ ì´ˆê¸°í™”")
    print("-" * 70)

    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        chunk_size=1000  # ë°°ì¹˜ í¬ê¸°
    )

    print("âœ… OpenAI Embeddings ì´ˆê¸°í™” ì™„ë£Œ")
    print("  â€¢ ëª¨ë¸: text-embedding-3-small")
    print("  â€¢ ì°¨ì›: 1536")

    # 2. Vector Store ìƒì„± ë° ì €ì¥
    print("\n2ï¸âƒ£ Vector Store ìƒì„±")
    print("-" * 70)

    with tempfile.TemporaryDirectory() as temp_dir:
        persist_dir = os.path.join(temp_dir, "chroma_db")

        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            persist_directory=persist_dir,
            collection_name="tech_kb"
        )

        print(f"âœ… Vector Store ìƒì„± ì™„ë£Œ!")
        print(f"  â€¢ ì»¬ë ‰ì…˜: tech_kb")
        print(f"  â€¢ ë¬¸ì„œ ìˆ˜: {vectorstore._collection.count()}")

        # 3. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print("\n3ï¸âƒ£ ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
        print("-" * 70)

        queries = [
            "í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ì¶”ì²œ",
            "ë¨¸ì‹ ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬",
            "ì»¨í…Œì´ë„ˆ ê¸°ìˆ "
        ]

        for query in queries:
            print(f"\nğŸ” ì¿¼ë¦¬: '{query}'")
            results = vectorstore.similarity_search(query, k=2)

            print("  ê²°ê³¼:")
            for i, doc in enumerate(results, 1):
                print(f"    {i}. {doc.page_content[:60]}...")
                print(f"       ë©”íƒ€ë°ì´í„°: {doc.metadata}")

        # 4. ë©”íƒ€ë°ì´í„° í•„í„°ë§
        print("\n4ï¸âƒ£ ë©”íƒ€ë°ì´í„° í•„í„°ë§ ê²€ìƒ‰")
        print("-" * 70)

        print("\nğŸ” ì¹´í…Œê³ ë¦¬ê°€ 'ml'ì¸ ë¬¸ì„œë§Œ ê²€ìƒ‰")
        results = vectorstore.similarity_search(
            "ì¶”ì²œí•´ì£¼ì„¸ìš”",
            k=5,
            filter={"category": "ml"}
        )

        print(f"  ê²°ê³¼ ìˆ˜: {len(results)}")
        for i, doc in enumerate(results, 1):
            print(f"  {i}. {doc.page_content[:50]}...")

        # 5. Vector Store ë¡œë“œ (ì¬ì‚¬ìš©)
        print("\n5ï¸âƒ£ ì €ì¥ëœ Vector Store ë¡œë“œ")
        print("-" * 70)

        # ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ë¡œ ë¡œë“œ
        loaded_vectorstore = Chroma(
            persist_directory=persist_dir,
            embedding_function=embeddings,
            collection_name="tech_kb"
        )

        print("âœ… Vector Store ë¡œë“œ ì™„ë£Œ!")
        print(f"  â€¢ ë¬¸ì„œ ìˆ˜: {loaded_vectorstore._collection.count()}")

        # ë¡œë“œëœ storeë¡œ ê²€ìƒ‰
        results = loaded_vectorstore.similarity_search("Python", k=1)
        print(f"\n  ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: {results[0].page_content[:60]}...")

    print("\n" + "=" * 70)


# ============================================================================
# ì˜ˆì œ 5: ì‹¤ì „ - ë¬¸ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ êµ¬ì¶• ë° ê²€ìƒ‰
# ============================================================================

def example_5_document_library():
    """ì‹¤ì „ ë¬¸ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‹œìŠ¤í…œ"""
    print("\n" + "=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 5: ì‹¤ì „ ë¬¸ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ êµ¬ì¶•")
    print("=" * 70)

    print("""
ğŸ’¡ ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤:
   - ê¸°ìˆ  ë¬¸ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ êµ¬ì¶•
   - ë¬¸ì„œ ì¶”ê°€, ê²€ìƒ‰, ê´€ë¦¬
   - ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§
   - ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜

ê¸°ëŠ¥:
   1. ì—¬ëŸ¬ ë¬¸ì„œ ë¡œë“œ ë° ì²­í‚¹
   2. Vector Store êµ¬ì¶•
   3. ê³ ê¸‰ ê²€ìƒ‰ (í•„í„°ë§, ì ìˆ˜)
   4. í†µê³„ ë° ê´€ë¦¬
    """)

    # ê¸°ìˆ  ë¬¸ì„œ ë°ì´í„°
    tech_articles = [
        {
            "title": "Python ì‹œì‘í•˜ê¸°",
            "content": """Pythonì€ 1991ë…„ ê·€ë„ ë°˜ ë¡œì„¬ì´ ë§Œë“  ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.
ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ë²•ìœ¼ë¡œ ì´ˆë³´ìì—ê²Œ ì í•©í•©ë‹ˆë‹¤. ì›¹ ê°œë°œ, ë°ì´í„° ê³¼í•™,
ë¨¸ì‹ ëŸ¬ë‹, ìë™í™” ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤. pipë¥¼ í†µí•œ íŒ¨í‚¤ì§€ ê´€ë¦¬ê°€
í¸ë¦¬í•˜ë©°, í’ë¶€í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœê³„ë¥¼ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤.""",
            "category": "programming",
            "level": "beginner",
            "tags": ["python", "tutorial"]
        },
        {
            "title": "Docker ì»¨í…Œì´ë„ˆ ê°€ì´ë“œ",
            "content": """DockerëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì»¨í…Œì´ë„ˆë¡œ íŒ¨í‚¤ì§•í•˜ì—¬ ì–´ë””ì„œë“  ë™ì¼í•˜ê²Œ
ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” í”Œë«í¼ì…ë‹ˆë‹¤. ê°€ìƒë¨¸ì‹ ë³´ë‹¤ ê°€ë³ê³  ë¹ ë¥´ë©°, ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤
ì•„í‚¤í…ì²˜ì— ì í•©í•©ë‹ˆë‹¤. Dockerfileë¡œ ì´ë¯¸ì§€ë¥¼ ì •ì˜í•˜ê³ , docker-composeë¡œ
ì—¬ëŸ¬ ì»¨í…Œì´ë„ˆë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. DevOps ì›Œí¬í”Œë¡œìš°ì— í•„ìˆ˜ì ì¸ ë„êµ¬ì…ë‹ˆë‹¤.""",
            "category": "devops",
            "level": "intermediate",
            "tags": ["docker", "container"]
        },
        {
            "title": "React ì»´í¬ë„ŒíŠ¸ ì„¤ê³„",
            "content": """ReactëŠ” Facebookì´ ê°œë°œí•œ ì„ ì–¸ì  UI ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
ì»´í¬ë„ŒíŠ¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜ë¡œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ UIë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Hooksë¥¼ í†µí•´
í•¨ìˆ˜í˜• ì»´í¬ë„ŒíŠ¸ì—ì„œ ìƒíƒœ ê´€ë¦¬ê°€ ê°€ëŠ¥í•˜ë©°, Virtual DOMìœ¼ë¡œ íš¨ìœ¨ì ì¸ ë Œë”ë§ì„
ì œê³µí•©ë‹ˆë‹¤. useState, useEffect ë“±ì˜ Hookì„ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.""",
            "category": "frontend",
            "level": "intermediate",
            "tags": ["react", "javascript"]
        },
        {
            "title": "ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ",
            "content": """ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì…ë‹ˆë‹¤.
ì§€ë„í•™ìŠµ, ë¹„ì§€ë„í•™ìŠµ, ê°•í™”í•™ìŠµìœ¼ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤. scikit-learnì€ ì „í†µì ì¸ ML
ì•Œê³ ë¦¬ì¦˜ì„ ì œê³µí•˜ë©°, ë¶„ë¥˜, íšŒê·€, í´ëŸ¬ìŠ¤í„°ë§ ë“±ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë°ì´í„° ì „ì²˜ë¦¬ì™€ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ì´ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.""",
            "category": "ai",
            "level": "advanced",
            "tags": ["ml", "ai"]
        },
        {
            "title": "SQL ì¿¼ë¦¬ ìµœì í™”",
            "content": """SQLì€ ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì¡°ì‘í•˜ëŠ” í‘œì¤€ ì–¸ì–´ì…ë‹ˆë‹¤.
ì¿¼ë¦¬ ìµœì í™”ëŠ” ì¸ë±ìŠ¤ ì‚¬ìš©, JOIN ìˆœì„œ, WHERE ì¡°ê±´ ìµœì í™”ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.
EXPLAINì„ í†µí•´ ì‹¤í–‰ ê³„íšì„ ë¶„ì„í•˜ê³ , ëŠë¦° ì¿¼ë¦¬ë¥¼ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì •ê·œí™”ì™€ ì—­ì •ê·œí™”ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.""",
            "category": "database",
            "level": "advanced",
            "tags": ["sql", "database"]
        },
        {
            "title": "Git ì›Œí¬í”Œë¡œìš°",
            "content": """Gitì€ ë¶„ì‚° ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œìœ¼ë¡œ ì½”ë“œ ë³€ê²½ ì´ë ¥ì„ ì¶”ì í•©ë‹ˆë‹¤.
ë¸Œëœì¹˜ë¥¼ í†µí•´ ë…ë¦½ì ì¸ ì‘ì—…ì´ ê°€ëŠ¥í•˜ë©°, mergeì™€ rebaseë¡œ ì½”ë“œë¥¼ í†µí•©í•©ë‹ˆë‹¤.
Git Flow, GitHub Flow ë“±ì˜ ì›Œí¬í”Œë¡œìš° ëª¨ë¸ì´ ìˆìœ¼ë©°, íŒ€ í˜‘ì—…ì— í•„ìˆ˜ì ì…ë‹ˆë‹¤.
commit ë©”ì‹œì§€ë¥¼ ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.""",
            "category": "devops",
            "level": "beginner",
            "tags": ["git", "version-control"]
        },
    ]

    print(f"\nğŸ“š ì´ {len(tech_articles)}ê°œ ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ")

    # 1. ë¬¸ì„œë¥¼ Document ê°ì²´ë¡œ ë³€í™˜ ë° ì²­í‚¹
    print("\n1ï¸âƒ£ ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í‚¹")
    print("-" * 70)

    all_documents = []

    for article in tech_articles:
        doc = Document(
            page_content=f"ì œëª©: {article['title']}\n\n{article['content']}",
            metadata={
                "title": article["title"],
                "category": article["category"],
                "level": article["level"],
                "tags": ",".join(article["tags"])
            }
        )
        all_documents.append(doc)

    # ì²­í‚¹
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=300,
        chunk_overlap=50,
        length_function=len
    )

    split_docs = text_splitter.split_documents(all_documents)

    print(f"âœ… ì›ë³¸ ë¬¸ì„œ: {len(all_documents)}ê°œ")
    print(f"âœ… ì²­í‚¹ í›„: {len(split_docs)}ê°œ")
    print(f"  â€¢ chunk_size: 300")
    print(f"  â€¢ overlap: 50")

    # 2. Vector Store êµ¬ì¶•
    print("\n2ï¸âƒ£ Vector Store êµ¬ì¶•")
    print("-" * 70)

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    with tempfile.TemporaryDirectory() as temp_dir:
        vectorstore = Chroma.from_documents(
            documents=split_docs,
            embedding=embeddings,
            persist_directory=temp_dir,
            collection_name="tech_library"
        )

        print("âœ… ë¬¸ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ êµ¬ì¶• ì™„ë£Œ!")
        print(f"  â€¢ ì´ ì²­í¬ ìˆ˜: {vectorstore._collection.count()}")

        # 3. ê¸°ë³¸ ê²€ìƒ‰
        print("\n3ï¸âƒ£ ê¸°ë³¸ ê²€ìƒ‰")
        print("-" * 70)

        query = "ì´ˆë³´ìë¥¼ ìœ„í•œ í”„ë¡œê·¸ë˜ë°"
        print(f"\nğŸ” ì¿¼ë¦¬: '{query}'")

        results = vectorstore.similarity_search(query, k=3)

        print("\nğŸ“„ ê²€ìƒ‰ ê²°ê³¼:")
        for i, doc in enumerate(results, 1):
            print(f"\n  {i}. ì œëª©: {doc.metadata.get('title', 'Unknown')}")
            print(f"     ì¹´í…Œê³ ë¦¬: {doc.metadata.get('category', 'Unknown')}")
            print(f"     ë ˆë²¨: {doc.metadata.get('level', 'Unknown')}")
            print(f"     ë‚´ìš©: {doc.page_content[:100]}...")

        # 4. ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰
        print("\n4ï¸âƒ£ ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§ ê²€ìƒ‰")
        print("-" * 70)

        categories = ["programming", "devops", "ai"]

        for category in categories:
            results = vectorstore.similarity_search(
                "ì¶”ì²œ",
                k=2,
                filter={"category": category}
            )

            print(f"\nğŸ“‚ ì¹´í…Œê³ ë¦¬: {category}")
            print(f"  ê²°ê³¼ ìˆ˜: {len(results)}")
            if results:
                print(f"  â€¢ {results[0].metadata.get('title', 'Unknown')}")

        # 5. ë ˆë²¨ë³„ ê²€ìƒ‰
        print("\n5ï¸âƒ£ ë‚œì´ë„ë³„ ê²€ìƒ‰")
        print("-" * 70)

        levels = ["beginner", "intermediate", "advanced"]

        for level in levels:
            results = vectorstore.similarity_search(
                "í•™ìŠµ",
                k=1,
                filter={"level": level}
            )

            print(f"\nğŸ¯ ë ˆë²¨: {level}")
            if results:
                print(f"  â€¢ {results[0].metadata.get('title', 'Unknown')}")
                print(f"    {results[0].page_content[:80]}...")

        # 6. ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰
        print("\n6ï¸âƒ£ ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ë°˜ ê²€ìƒ‰")
        print("-" * 70)

        query = "ì»¨í…Œì´ë„ˆì™€ ê°€ìƒí™”"
        print(f"\nğŸ” ì¿¼ë¦¬: '{query}'")

        results_with_scores = vectorstore.similarity_search_with_score(query, k=3)

        print("\nğŸ“Š ì ìˆ˜ë³„ ê²°ê³¼:")
        for i, (doc, score) in enumerate(results_with_scores, 1):
            print(f"\n  {i}. ì ìˆ˜: {score:.4f}")
            print(f"     ì œëª©: {doc.metadata.get('title', 'Unknown')}")
            print(f"     ë‚´ìš©: {doc.page_content[:80]}...")

        # 7. í†µê³„ ì •ë³´
        print("\n7ï¸âƒ£ ë¼ì´ë¸ŒëŸ¬ë¦¬ í†µê³„")
        print("-" * 70)

        print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
        print(f"  â€¢ ì´ ë¬¸ì„œ ìˆ˜: {len(all_documents)}")
        print(f"  â€¢ ì´ ì²­í¬ ìˆ˜: {vectorstore._collection.count()}")

        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_count = {}
        for doc in all_documents:
            cat = doc.metadata["category"]
            category_count[cat] = category_count.get(cat, 0) + 1

        print(f"\n  ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ìˆ˜:")
        for cat, count in sorted(category_count.items()):
            print(f"    â€¢ {cat}: {count}ê°œ")

        # ë ˆë²¨ë³„ í†µê³„
        level_count = {}
        for doc in all_documents:
            level = doc.metadata["level"]
            level_count[level] = level_count.get(level, 0) + 1

        print(f"\n  ë‚œì´ë„ë³„ ë¬¸ì„œ ìˆ˜:")
        for level, count in sorted(level_count.items()):
            print(f"    â€¢ {level}: {count}ê°œ")

        # 8. ì‚¬ìš©ì ê²€ìƒ‰
        print("\n8ï¸âƒ£ ì§ì ‘ ê²€ìƒ‰í•´ë³´ê¸°")
        print("-" * 70)

        user_query = input("\nğŸ” ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (Enterë¡œ ê±´ë„ˆë›°ê¸°): ").strip()

        if user_query:
            print(f"\nê²€ìƒ‰ ì¤‘: '{user_query}'...")

            results = vectorstore.similarity_search_with_score(user_query, k=3)

            print(f"\nğŸ“„ ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):")
            for i, (doc, score) in enumerate(results, 1):
                print(f"\n  {i}. {doc.metadata.get('title', 'Unknown')}")
                print(f"     ìœ ì‚¬ë„: {score:.4f}")
                print(f"     ì¹´í…Œê³ ë¦¬: {doc.metadata.get('category', 'Unknown')}")
                print(f"     ë ˆë²¨: {doc.metadata.get('level', 'Unknown')}")
                print(f"     ë‚´ìš©: {doc.page_content[:120]}...")
        else:
            print("ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

    print("\n" + "=" * 70)
    print("âœ… ë¬¸ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‹œìŠ¤í…œ ì™„ë£Œ!")


# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n")
    print("=" * 70)
    print("Part 8: Vector Store êµ¬ì¶• (02_vector_store.py)")
    print("=" * 70)

    while True:
        print("\nğŸ“š ì‹¤í–‰í•  ì˜ˆì œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        print("  1. Chroma Vector Store ê¸°ë³¸")
        print("  2. Document Loaders (Text, CSV)")
        print("  3. Text Chunking ì „ëµ")
        print("  4. Embedding ìƒì„± ë° ì €ì¥")
        print("  5. ì‹¤ì „ ë¬¸ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ êµ¬ì¶• â­")
        print("  0. ì¢…ë£Œ")

        choice = input("\nì„ íƒ (0-5): ").strip()

        if choice == "1":
            example_1_chroma_basics()
        elif choice == "2":
            example_2_document_loaders()
        elif choice == "3":
            example_3_text_chunking()
        elif choice == "4":
            example_4_embeddings()
        elif choice == "5":
            example_5_document_library()
        elif choice == "0":
            print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        else:
            print("\nâŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main()
