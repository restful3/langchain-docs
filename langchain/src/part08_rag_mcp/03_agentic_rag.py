"""
================================================================================
LangChain AI Agent ë§ˆìŠ¤í„° êµì•ˆ
Part 8: RAG & MCP
================================================================================

íŒŒì¼ëª…: 03_agentic_rag.py
ë‚œì´ë„: â­â­â­â­ (ì¤‘ìƒê¸‰)
ì˜ˆìƒ ì‹œê°„: 35ë¶„

ğŸ“š í•™ìŠµ ëª©í‘œ:
  - Retrieverë¥¼ Toolë¡œ ë³€í™˜
  - Agentì— Retriever tool í†µí•©
  - Query planning (query â†’ subqueries)
  - Self-RAG pattern êµ¬í˜„
  - ì‹¤ì „ ì§€ì‹ ê¸°ë°˜ Q&A Agent

ğŸ“– ê³µì‹ ë¬¸ì„œ:
  â€¢ Retrieval: https://python.langchain.com/docs/concepts/retrieval/
  â€¢ Tools: https://python.langchain.com/docs/concepts/tools/

ğŸ“„ êµì•ˆ ë¬¸ì„œ:
  â€¢ Part 8: /docs/part08_rag_mcp.md

ğŸ”§ í•„ìš”í•œ íŒ¨í‚¤ì§€:
  pip install langchain langchain-openai langchain-community faiss-cpu python-dotenv

ğŸš€ ì‹¤í–‰ ë°©ë²•:
  python 03_agentic_rag.py

================================================================================
"""

# ============================================================================
# Imports
# ============================================================================

import os
from dotenv import load_dotenv
from langchain.agents import create_agent
from langchain.tools import tool
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from langchain.tools.retriever import create_retriever_tool

# ============================================================================
# í™˜ê²½ ì„¤ì •
# ============================================================================

load_dotenv()

if not os.getenv("OPENAI_API_KEY"):
    print("âŒ ì˜¤ë¥˜: OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ğŸ“ .env íŒŒì¼ì„ í™•ì¸í•˜ê³  API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    exit(1)

# ============================================================================
# ì˜ˆì œ 1: Retrieverë¥¼ Toolë¡œ ë³€í™˜
# ============================================================================

def example_1_retriever_as_tool():
    """Retrieverë¥¼ Toolë¡œ ë³€í™˜í•˜ì—¬ Agentì—ì„œ ì‚¬ìš©"""
    print("=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 1: Retrieverë¥¼ Toolë¡œ ë³€í™˜")
    print("=" * 70)

    print("""
ğŸ’¡ Retriever Toolì´ë€?
   - Vector Storeì˜ ê²€ìƒ‰ ê¸°ëŠ¥ì„ Toolë¡œ ë˜í•‘
   - Agentê°€ í•„ìš”í•  ë•Œ ê²€ìƒ‰ í˜¸ì¶œ ê°€ëŠ¥
   - ëª…í™•í•œ ë„êµ¬ ì„¤ëª…ì´ ì¤‘ìš”

ë°©ë²•:
   1. Vector Store ìƒì„±
   2. as_retriever()ë¡œ Retriever ë³€í™˜
   3. create_retriever_tool()ë¡œ Tool ìƒì„±
   4. Agentì— Tool ì¶”ê°€
    """)

    # íšŒì‚¬ ì •ì±… ë¬¸ì„œ
    policy_docs = [
        Document(
            page_content="ì—°ì°¨ íœ´ê°€ëŠ” ì…ì‚¬ 1ë…„ í›„ 15ì¼ì´ ë¶€ì—¬ë©ë‹ˆë‹¤. ë§¤ë…„ ê·¼ì† ì—°ìˆ˜ì— ë”°ë¼ 1ì¼ì”© ì¶”ê°€ë©ë‹ˆë‹¤.",
            metadata={"category": "íœ´ê°€", "doc_id": "POL-001"}
        ),
        Document(
            page_content="ì¬íƒê·¼ë¬´ëŠ” ì£¼ 2íšŒê¹Œì§€ ê°€ëŠ¥í•˜ë©°, ì‚¬ì „ì— íŒ€ì¥ ìŠ¹ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.",
            metadata={"category": "ê·¼ë¬´", "doc_id": "POL-002"}
        ),
        Document(
            page_content="ê²½ì¡°ì‚¬ íœ´ê°€ëŠ” ê²°í˜¼ 5ì¼, ì¶œì‚° 3ì¼, ì§ê³„ ê°€ì¡± ì‚¬ë§ 5ì¼ì…ë‹ˆë‹¤.",
            metadata={"category": "íœ´ê°€", "doc_id": "POL-003"}
        ),
        Document(
            page_content="ì ì‹¬ ì‹ëŒ€ëŠ” 1ì¼ ë§Œì›ì´ ì§€ì›ë˜ë©°, ë²•ì¸ ì¹´ë“œë¡œ ê²°ì œí•©ë‹ˆë‹¤.",
            metadata={"category": "ë³µì§€", "doc_id": "POL-004"}
        ),
        Document(
            page_content="ê±´ê°•ê²€ì§„ì€ ì—° 1íšŒ ì œê³µë˜ë©°, ë¹„ìš©ì€ íšŒì‚¬ì—ì„œ ì „ì•¡ ë¶€ë‹´í•©ë‹ˆë‹¤.",
            metadata={"category": "ë³µì§€", "doc_id": "POL-005"}
        ),
    ]

    print(f"\nğŸ“š ë¬¸ì„œ ìˆ˜: {len(policy_docs)}")

    # Vector Store ìƒì„±
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = FAISS.from_documents(policy_docs, embeddings)

    print("âœ… Vector Store ìƒì„± ì™„ë£Œ")

    # 1. ê¸°ë³¸ Retriever Tool
    print("\n1ï¸âƒ£ ê¸°ë³¸ Retriever Tool ìƒì„±")
    print("-" * 70)

    retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

    retriever_tool = create_retriever_tool(
        retriever=retriever,
        name="search_company_policy",
        description="íšŒì‚¬ ì •ì±… ë° ê·œì •ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤. íœ´ê°€, ê·¼ë¬´, ë³µì§€ ê´€ë ¨ ì§ˆë¬¸ì— ì‚¬ìš©í•˜ì„¸ìš”."
    )

    print("âœ… Retriever Tool ìƒì„± ì™„ë£Œ")
    print(f"  â€¢ ë„êµ¬ ì´ë¦„: {retriever_tool.name}")
    print(f"  â€¢ ì„¤ëª…: {retriever_tool.description}")

    # 2. Agent ìƒì„±
    print("\n2ï¸âƒ£ Agent ìƒì„± ë° í…ŒìŠ¤íŠ¸")
    print("-" * 70)

    agent = create_agent(
        model="gpt-4o-mini",
        tools=[retriever_tool],
        system_prompt="""ë‹¹ì‹ ì€ íšŒì‚¬ HR ë‹´ë‹¹ìì…ë‹ˆë‹¤.
        
ì§ì›ë“¤ì˜ ì§ˆë¬¸ì— ë‹µë³€í•  ë•Œ:
1. search_company_policy ë„êµ¬ë¡œ ê´€ë ¨ ì •ì±… ê²€ìƒ‰
2. ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€
3. ì •ì±… ë¬¸ì„œ ë²ˆí˜¸(doc_id)ë„ í•¨ê»˜ ì•ˆë‚´"""
    )

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    test_questions = [
        "ì—°ì°¨ëŠ” ë©°ì¹  ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?",
        "ì¬íƒê·¼ë¬´ê°€ ê°€ëŠ¥í•œê°€ìš”?",
        "íšŒì‚¬ì—ì„œ ì œê³µí•˜ëŠ” ë³µì§€ëŠ” ë¬´ì—‡ì´ ìˆë‚˜ìš”?"
    ]

    for question in test_questions:
        print(f"\nâ“ ì§ˆë¬¸: {question}")
        print("-" * 70)

        response = agent.invoke({
            "messages": [{"role": "user", "content": question}]
        })

        answer = response['messages'][-1].content
        print(f"ğŸ¤– ë‹µë³€: {answer}\n")

    print("=" * 70)


# ============================================================================
# ì˜ˆì œ 2: Agentì— ì—¬ëŸ¬ Retriever í†µí•©
# ============================================================================

def example_2_multiple_retrievers():
    """ì—¬ëŸ¬ ë„ë©”ì¸ì˜ Retrieverë¥¼ Agentì— í†µí•©"""
    print("\n" + "=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 2: ì—¬ëŸ¬ Retriever í†µí•©")
    print("=" * 70)

    print("""
ğŸ’¡ ë‹¤ì¤‘ Retriever ì „ëµ:
   - ë„ë©”ì¸ë³„ë¡œ ë³„ë„ Vector Store êµ¬ì¶•
   - ê°ê°ì„ ë…ë¦½ëœ Toolë¡œ ì œê³µ
   - Agentê°€ ì§ˆë¬¸ì— ë§ëŠ” Tool ì„ íƒ

ì¥ì :
   â€¢ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ
   â€¢ ëª…í™•í•œ ë„ë©”ì¸ ë¶„ë¦¬
   â€¢ ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œ í˜¼ì… ë°©ì§€
    """)

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # 1. HR ì •ì±… Vector Store
    hr_docs = [
        "ì—°ì°¨ëŠ” ì…ì‚¬ 1ë…„ í›„ 15ì¼ ë¶€ì—¬ë©ë‹ˆë‹¤.",
        "ìœ¡ì•„ íœ´ì§ì€ ìë…€ ë§Œ 8ì„¸ê¹Œì§€ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.",
        "ë³‘ê°€ëŠ” ì—° 10ì¼ê¹Œì§€ ê°€ëŠ¥í•˜ë©°, ì§„ë‹¨ì„œ ì œì¶œì´ í•„ìš”í•©ë‹ˆë‹¤."
    ]
    hr_vectorstore = FAISS.from_texts(hr_docs, embeddings)
    hr_retriever = hr_vectorstore.as_retriever(search_kwargs={"k": 2})

    # 2. ê¸°ìˆ  ë¬¸ì„œ Vector Store
    tech_docs = [
        "Python ê°œë°œ í™˜ê²½ì€ Docker ì»¨í…Œì´ë„ˆë¡œ ì œê³µë©ë‹ˆë‹¤.",
        "Git branchëŠ” feature/ì´ìŠˆë²ˆí˜¸ í˜•ì‹ìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”.",
        "ì½”ë“œ ë¦¬ë·°ëŠ” ìµœì†Œ 2ëª…ì˜ ìŠ¹ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
    ]
    tech_vectorstore = FAISS.from_texts(tech_docs, embeddings)
    tech_retriever = tech_vectorstore.as_retriever(search_kwargs={"k": 2})

    # 3. ì¬ë¬´ ì •ì±… Vector Store
    finance_docs = [
        "êµí†µë¹„ëŠ” ì›” 10ë§Œì›ê¹Œì§€ ì§€ì›ë©ë‹ˆë‹¤.",
        "ì—…ë¬´ ê´€ë ¨ ë„ì„œëŠ” ì›” 5ë§Œì›ê¹Œì§€ êµ¬ë§¤ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
        "íšŒì‹ë¹„ëŠ” íŒ€ë‹¹ ë¶„ê¸°ë³„ 30ë§Œì› í•œë„ì…ë‹ˆë‹¤."
    ]
    finance_vectorstore = FAISS.from_texts(finance_docs, embeddings)
    finance_retriever = finance_vectorstore.as_retriever(search_kwargs={"k": 2})

    print("âœ… 3ê°œ ë„ë©”ì¸ Vector Store ìƒì„± ì™„ë£Œ")

    # Tool ìƒì„±
    hr_tool = create_retriever_tool(
        hr_retriever,
        "search_hr_policy",
        "ì¸ì‚¬ ì •ì±…ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤ (íœ´ê°€, íœ´ì§, ë³‘ê°€ ë“±)"
    )

    tech_tool = create_retriever_tool(
        tech_retriever,
        "search_tech_docs",
        "ê¸°ìˆ  ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤ (ê°œë°œ í™˜ê²½, Git, ì½”ë“œ ë¦¬ë·° ë“±)"
    )

    finance_tool = create_retriever_tool(
        finance_retriever,
        "search_finance_policy",
        "ì¬ë¬´ ì •ì±…ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤ (êµí†µë¹„, ë„ì„œë¹„, íšŒì‹ë¹„ ë“±)"
    )

    print("\nğŸ“¦ ìƒì„±ëœ Tools:")
    for tool in [hr_tool, tech_tool, finance_tool]:
        print(f"  â€¢ {tool.name}: {tool.description}")

    # Agent ìƒì„±
    agent = create_agent(
        model="gpt-4o-mini",
        tools=[hr_tool, tech_tool, finance_tool],
        system_prompt="""ë‹¹ì‹ ì€ íšŒì‚¬ í†µí•© ì •ë³´ ì•ˆë‚´ Agentì…ë‹ˆë‹¤.
        
ì§ˆë¬¸ì˜ ì¹´í…Œê³ ë¦¬ì— ë§ëŠ” ê²€ìƒ‰ ë„êµ¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:
- íœ´ê°€, ì¸ì‚¬ ê´€ë ¨ â†’ search_hr_policy
- ê°œë°œ, ê¸°ìˆ  ê´€ë ¨ â†’ search_tech_docs
- ë¹„ìš©, ì¬ë¬´ ê´€ë ¨ â†’ search_finance_policy"""
    )

    # ë‹¤ì–‘í•œ ë„ë©”ì¸ ì§ˆë¬¸
    test_questions = [
        ("ì—°ì°¨ëŠ” ë©°ì¹ ì¸ê°€ìš”?", "HR"),
        ("Git ë¸Œëœì¹˜ ê·œì¹™ì´ ë­ì£ ?", "Tech"),
        ("ë„ì„œ êµ¬ë§¤ ì˜ˆì‚°ì€ ì–¼ë§ˆì¸ê°€ìš”?", "Finance"),
        ("ì½”ë“œ ë¦¬ë·°ëŠ” ëª‡ ëª…ì´ í•„ìš”í•œê°€ìš”?", "Tech"),
    ]

    print("\n" + "=" * 70)
    print("ğŸ§ª ë‹¤ì¤‘ ë„ë©”ì¸ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    for question, domain in test_questions:
        print(f"\nâ“ ì§ˆë¬¸ ({domain}): {question}")
        print("-" * 70)

        response = agent.invoke({
            "messages": [{"role": "user", "content": question}]
        })

        answer = response['messages'][-1].content
        print(f"ğŸ¤– ë‹µë³€: {answer[:150]}...")

    print("\n" + "=" * 70)


# ============================================================================
# ì˜ˆì œ 3: Query Planning - ì¿¼ë¦¬ ë¶„í•´
# ============================================================================

def example_3_query_planning():
    """ë³µì¡í•œ ì§ˆë¬¸ì„ ì—¬ëŸ¬ ì„œë¸Œ ì¿¼ë¦¬ë¡œ ë¶„í•´"""
    print("\n" + "=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 3: Query Planning")
    print("=" * 70)

    print("""
ğŸ’¡ Query Planningì´ë€?
   - ë³µì¡í•œ ì§ˆë¬¸ì„ ì—¬ëŸ¬ ë‹¨ê³„ë¡œ ë¶„í•´
   - ê° ë‹¨ê³„ë³„ë¡œ ì •ë³´ ìˆ˜ì§‘
   - ìµœì¢… ë‹µë³€ í†µí•©

ì˜ˆì‹œ:
   ì§ˆë¬¸: "ì‘ë…„ê³¼ ì˜¬í•´ ë§¤ì¶œ ë¹„êµí•˜ë©´?"
   â†’ 1ë‹¨ê³„: "ì‘ë…„ ë§¤ì¶œ ê²€ìƒ‰"
   â†’ 2ë‹¨ê³„: "ì˜¬í•´ ë§¤ì¶œ ê²€ìƒ‰"
   â†’ 3ë‹¨ê³„: "ë‘ ê°’ ë¹„êµ"
    """)

    # ì¬ë¬´ ë°ì´í„°
    finance_data = [
        "2023ë…„ Q1 ë§¤ì¶œ: $4.2M, ë¹„ìš©: $3.1M, ìˆœì´ìµ: $1.1M",
        "2023ë…„ Q2 ë§¤ì¶œ: $4.8M, ë¹„ìš©: $3.3M, ìˆœì´ìµ: $1.5M",
        "2023ë…„ Q3 ë§¤ì¶œ: $5.2M, ë¹„ìš©: $3.5M, ìˆœì´ìµ: $1.7M",
        "2023ë…„ Q4 ë§¤ì¶œ: $5.8M, ë¹„ìš©: $3.7M, ìˆœì´ìµ: $2.1M",
        "2024ë…„ Q1 ë§¤ì¶œ: $5.5M, ë¹„ìš©: $3.6M, ìˆœì´ìµ: $1.9M",
        "2024ë…„ Q2 ë§¤ì¶œ: $6.2M, ë¹„ìš©: $3.9M, ìˆœì´ìµ: $2.3M",
        "2024ë…„ Q3 ë§¤ì¶œ: $6.8M, ë¹„ìš©: $4.1M, ìˆœì´ìµ: $2.7M",
    ]

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = FAISS.from_texts(finance_data, embeddings)

    # ê²€ìƒ‰ ë„êµ¬
    @tool
    def search_finance_data(query: str) -> str:
        """ì¬ë¬´ ë°ì´í„° ê²€ìƒ‰"""
        docs = vectorstore.similarity_search(query, k=3)
        return "\n".join([d.page_content for d in docs])

    # Query Planning ë„êµ¬
    @tool
    def plan_complex_query(question: str) -> str:
        """ë³µì¡í•œ ì§ˆë¬¸ì„ ì—¬ëŸ¬ ë‹¨ê³„ë¡œ ë¶„í•´
        
        Args:
            question: ë³µì¡í•œ ì§ˆë¬¸
            
        Returns:
            ë‹¨ê³„ë³„ ê²€ìƒ‰ ê³„íš (JSON í˜•ì‹)
        """
        llm = ChatOpenAI(model="gpt-4o-mini")
        
        prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì„ ì—¬ëŸ¬ ê²€ìƒ‰ ë‹¨ê³„ë¡œ ë¶„í•´í•˜ì„¸ìš”:

ì§ˆë¬¸: {question}

ê° ë‹¨ê³„ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±:
1. [ê²€ìƒ‰ ì¿¼ë¦¬ 1]
2. [ê²€ìƒ‰ ì¿¼ë¦¬ 2]
...

ë‹¨ê³„ë³„ ê²€ìƒ‰ ì¿¼ë¦¬ë§Œ ì‘ì„±í•˜ì„¸ìš”."""

        response = llm.invoke(prompt)
        return response.content

    # Agent ìƒì„±
    agent = create_agent(
        model="gpt-4o-mini",
        tools=[search_finance_data, plan_complex_query],
        system_prompt="""ë‹¹ì‹ ì€ ì¬ë¬´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë³µì¡í•œ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•:
1. plan_complex_queryë¡œ ì§ˆë¬¸ì„ ë‹¨ê³„ë³„ë¡œ ë¶„í•´
2. ê° ë‹¨ê³„ë§ˆë‹¤ search_finance_dataë¡œ ë°ì´í„° ê²€ìƒ‰
3. ëª¨ë“  ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€"""
    )

    # ë³µì¡í•œ ì§ˆë¬¸ë“¤
    complex_questions = [
        "2023ë…„ê³¼ 2024ë…„ Q1 ë§¤ì¶œì„ ë¹„êµí•˜ë©´?",
        "2024ë…„ ìƒë°˜ê¸° í‰ê·  ìˆœì´ìµì€ ì–¼ë§ˆì¸ê°€ìš”?",
    ]

    print("\n" + "=" * 70)
    print("ğŸ§ª ë³µì¡í•œ ì§ˆë¬¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    for question in complex_questions:
        print(f"\nâ“ ì§ˆë¬¸: {question}")
        print("=" * 70)

        response = agent.invoke({
            "messages": [{"role": "user", "content": question}]
        })

        answer = response['messages'][-1].content
        print(f"\nğŸ¤– ë‹µë³€:\n{answer}\n")

    print("=" * 70)


# ============================================================================
# ì˜ˆì œ 4: Self-RAG - ê²€ìƒ‰ ê²°ê³¼ ìê¸° í‰ê°€
# ============================================================================

def example_4_self_rag():
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìŠ¤ìŠ¤ë¡œ í‰ê°€í•˜ê³  ê°œì„ í•˜ëŠ” Self-RAG"""
    print("\n" + "=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 4: Self-RAG (ìê¸° ê²€ì¦ RAG)")
    print("=" * 70)

    print("""
ğŸ’¡ Self-RAGë€?
   - ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆì„ Agentê°€ ìŠ¤ìŠ¤ë¡œ í‰ê°€
   - ë¶€ì¡±í•˜ë©´ ì¿¼ë¦¬ ê°œì„  í›„ ì¬ê²€ìƒ‰
   - ì¶©ë¶„í•  ë•Œê¹Œì§€ ë°˜ë³µ

í”„ë¡œì„¸ìŠ¤:
   1. ì´ˆê¸° ê²€ìƒ‰
   2. ê²°ê³¼ í’ˆì§ˆ í‰ê°€
   3. ë¶€ì¡±í•˜ë©´ â†’ ì¿¼ë¦¬ ê°œì„  â†’ ì¬ê²€ìƒ‰
   4. ì¶©ë¶„í•˜ë©´ â†’ ë‹µë³€ ìƒì„±
    """)

    # ê¸°ìˆ  ë¬¸ì„œ
    tech_docs = [
        "DockerëŠ” ì»¨í…Œì´ë„ˆ ê¸°ë°˜ ê°€ìƒí™” í”Œë«í¼ì…ë‹ˆë‹¤.",
        "Docker ComposeëŠ” ì—¬ëŸ¬ ì»¨í…Œì´ë„ˆë¥¼ ì •ì˜í•˜ê³  ì‹¤í–‰í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.",
        "KubernetesëŠ” ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ í”Œë«í¼ì…ë‹ˆë‹¤.",
        "KubernetesëŠ” ìë™ ìŠ¤ì¼€ì¼ë§ê³¼ ë¡œë“œë°¸ëŸ°ì‹±ì„ ì œê³µí•©ë‹ˆë‹¤.",
        "Helmì€ Kubernetes íŒ¨í‚¤ì§€ ê´€ë¦¬ìì…ë‹ˆë‹¤.",
        "Dockerfileì€ ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ë¥¼ ë¹Œë“œí•˜ê¸° ìœ„í•œ ëª…ë ¹ì„ ì •ì˜í•©ë‹ˆë‹¤.",
        "kubectlì€ Kubernetes í´ëŸ¬ìŠ¤í„°ë¥¼ ê´€ë¦¬í•˜ëŠ” CLI ë„êµ¬ì…ë‹ˆë‹¤.",
    ]

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = FAISS.from_texts(tech_docs, embeddings)

    # ë„êµ¬ ì •ì˜
    @tool
    def search_tech_docs(query: str) -> str:
        """ê¸°ìˆ  ë¬¸ì„œ ê²€ìƒ‰"""
        docs = vectorstore.similarity_search(query, k=3)
        results = "\n".join([f"- {d.page_content}" for d in docs])
        return f"ê²€ìƒ‰ ê²°ê³¼:\n{results}"

    @tool
    def evaluate_search_quality(original_question: str, search_results: str) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ê°€ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ì¶©ë¶„í•œì§€ í‰ê°€
        
        Returns:
            'SUFFICIENT' ë˜ëŠ” 'INSUFFICIENT'ì™€ ì´ìœ 
        """
        llm = ChatOpenAI(model="gpt-4o-mini")
        
        prompt = f"""ì§ˆë¬¸: {original_question}

ê²€ìƒ‰ ê²°ê³¼:
{search_results}

ì´ ê²€ìƒ‰ ê²°ê³¼ê°€ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ì¶©ë¶„í•œê°€ìš”?

ì‘ë‹µ í˜•ì‹:
íŒì •: SUFFICIENT ë˜ëŠ” INSUFFICIENT
ì´ìœ : [ê°„ë‹¨í•œ ì„¤ëª…]"""

        response = llm.invoke(prompt)
        return response.content

    @tool
    def improve_search_query(original_question: str, previous_results: str, feedback: str) -> str:
        """ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ê°œì„ 
        
        Returns:
            ê°œì„ ëœ ê²€ìƒ‰ ì¿¼ë¦¬
        """
        llm = ChatOpenAI(model="gpt-4o-mini")
        
        prompt = f"""ì›ë˜ ì§ˆë¬¸: {original_question}

ì´ì „ ê²€ìƒ‰ ê²°ê³¼:
{previous_results}

í”¼ë“œë°±:
{feedback}

ë” ë‚˜ì€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•œ ìƒˆë¡œìš´ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
ì¿¼ë¦¬ë§Œ ë°˜í™˜í•˜ì„¸ìš”."""

        response = llm.invoke(prompt)
        return response.content

    # Self-RAG Agent
    agent = create_agent(
        model="gpt-4o-mini",
        tools=[search_tech_docs, evaluate_search_quality, improve_search_query],
        system_prompt="""ë‹¹ì‹ ì€ ê¸°ìˆ  ë¬¸ì„œ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

Self-RAG í”„ë¡œì„¸ìŠ¤:
1. search_tech_docsë¡œ ì´ˆê¸° ê²€ìƒ‰
2. evaluate_search_qualityë¡œ ê²°ê³¼ í‰ê°€
3. INSUFFICIENTì´ë©´:
   - improve_search_queryë¡œ ì¿¼ë¦¬ ê°œì„ 
   - ê°œì„ ëœ ì¿¼ë¦¬ë¡œ ì¬ê²€ìƒ‰
   - ë‹¤ì‹œ í‰ê°€
4. SUFFICIENTì´ë©´:
   - ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€

ìµœëŒ€ 2íšŒê¹Œì§€ë§Œ ì¬ê²€ìƒ‰í•˜ì„¸ìš”."""
    )

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    test_questions = [
        "ì»¨í…Œì´ë„ˆ ê´€ë¦¬ ë„êµ¬ëŠ” ë¬´ì—‡ì´ ìˆë‚˜ìš”?",
        "Kubernetesì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    ]

    print("\n" + "=" * 70)
    print("ğŸ§ª Self-RAG í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    for question in test_questions:
        print(f"\nâ“ ì§ˆë¬¸: {question}")
        print("=" * 70)

        response = agent.invoke({
            "messages": [{"role": "user", "content": question}]
        })

        answer = response['messages'][-1].content
        print(f"\nğŸ¤– ìµœì¢… ë‹µë³€:\n{answer}\n")

    print("=" * 70)


# ============================================================================
# ì˜ˆì œ 5: ì‹¤ì „ - ì§€ì‹ ê¸°ë°˜ Q&A Agent
# ============================================================================

def example_5_knowledge_qa_agent():
    """ì‹¤ì „ ì§€ì‹ ê¸°ë°˜ Q&A Agent ì‹œìŠ¤í…œ"""
    print("\n" + "=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 5: ì‹¤ì „ ì§€ì‹ ê¸°ë°˜ Q&A Agent")
    print("=" * 70)

    print("""
ğŸ’¡ ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤:
   - íšŒì‚¬ ì „ì²´ ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰
   - ë‹¤ì¤‘ ë„ë©”ì¸ ì§€ì›
   - ê²€ìƒ‰ ê²°ê³¼ í‰ê°€ ë° ê°œì„ 
   - ì¶œì²˜ ì¶”ì  ë° ì‹ ë¢°ë„ í‘œì‹œ

ê¸°ëŠ¥:
   1. ì—¬ëŸ¬ ì§€ì‹ ë² ì´ìŠ¤ í†µí•©
   2. ìë™ ì¿¼ë¦¬ ê°œì„ 
   3. ë‹µë³€ ì‹ ë¢°ë„ í‰ê°€
   4. ì¶œì²˜ ë¬¸ì„œ í‘œì‹œ
    """)

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•
    knowledge_base = [
        Document(
            page_content="LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. Agent, Chain, Tool ë“±ì˜ ê°œë…ì„ ì œê³µí•©ë‹ˆë‹¤.",
            metadata={"source": "tech_docs", "topic": "langchain", "reliability": "high"}
        ),
        Document(
            page_content="RAGëŠ” Retrieval Augmented Generationì˜ ì•½ìë¡œ, ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ LLM ë‹µë³€ì„ ê°œì„ í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.",
            metadata={"source": "tech_docs", "topic": "rag", "reliability": "high"}
        ),
        Document(
            page_content="Vector StoreëŠ” ì„ë² ë”©ëœ ë¬¸ì„œë¥¼ ì €ì¥í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤. FAISS, Chroma, Pinecone ë“±ì´ ìˆìŠµë‹ˆë‹¤.",
            metadata={"source": "tech_docs", "topic": "vector_store", "reliability": "high"}
        ),
        Document(
            page_content="AgentëŠ” LLMì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” íŒ¨í„´ì…ë‹ˆë‹¤. ReAct íŒ¨í„´ì„ ì£¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.",
            metadata={"source": "tech_docs", "topic": "agent", "reliability": "high"}
        ),
        Document(
            page_content="Embeddingì€ í…ìŠ¤íŠ¸ë¥¼ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. OpenAIì˜ text-embedding-3-small ëª¨ë¸ì„ ê¶Œì¥í•©ë‹ˆë‹¤.",
            metadata={"source": "tech_docs", "topic": "embedding", "reliability": "medium"}
        ),
        Document(
            page_content="íšŒì‚¬ëŠ” ì—° 15ì¼ì˜ ì—°ì°¨ë¥¼ ì œê³µí•˜ë©°, ê·¼ì† ì—°ìˆ˜ì— ë”°ë¼ ì¶”ê°€ ë¶€ì—¬ë©ë‹ˆë‹¤.",
            metadata={"source": "hr_policy", "topic": "vacation", "reliability": "high"}
        ),
        Document(
            page_content="ì¬íƒê·¼ë¬´ëŠ” ì£¼ 2íšŒê¹Œì§€ ê°€ëŠ¥í•˜ë©°, íŒ€ì¥ ìŠ¹ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.",
            metadata={"source": "hr_policy", "topic": "remote_work", "reliability": "high"}
        ),
    ]

    vectorstore = FAISS.from_documents(knowledge_base, embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    print(f"âœ… ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ ({len(knowledge_base)}ê°œ ë¬¸ì„œ)")

    # ê³ ê¸‰ ê²€ìƒ‰ ë„êµ¬
    @tool
    def search_knowledge_base(query: str) -> str:
        """íšŒì‚¬ ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰ (ê¸°ìˆ  ë¬¸ì„œ, HR ì •ì±… ë“±)
        
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ì™€ ë©”íƒ€ë°ì´í„° (ì¶œì²˜, ì‹ ë¢°ë„)
        """
        docs = retriever.invoke(query)
        
        results = []
        for i, doc in enumerate(docs, 1):
            source = doc.metadata.get("source", "unknown")
            topic = doc.metadata.get("topic", "general")
            reliability = doc.metadata.get("reliability", "medium")
            
            results.append(
                f"[ë¬¸ì„œ {i}] (ì¶œì²˜: {source}, ì£¼ì œ: {topic}, ì‹ ë¢°ë„: {reliability})\n"
                f"{doc.page_content}\n"
            )
        
        return "\n".join(results)

    @tool
    def evaluate_answer_confidence(question: str, search_results: str) -> str:
        """ë‹µë³€ ì‹ ë¢°ë„ í‰ê°€
        
        Returns:
            HIGH, MEDIUM, LOWì™€ ì´ìœ 
        """
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ í‰ê°€
        result_count = search_results.count("[ë¬¸ì„œ")
        has_high_reliability = "ì‹ ë¢°ë„: high" in search_results
        
        if result_count >= 2 and has_high_reliability:
            confidence = "HIGH"
            reason = "ì—¬ëŸ¬ ê³ ì‹ ë¢°ë„ ë¬¸ì„œì—ì„œ ì¼ê´€ëœ ì •ë³´ ë°œê²¬"
        elif result_count >= 1:
            confidence = "MEDIUM"
            reason = "ê´€ë ¨ ë¬¸ì„œëŠ” ìˆìœ¼ë‚˜ ì¶”ê°€ í™•ì¸ ê¶Œì¥"
        else:
            confidence = "LOW"
            reason = "ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•¨"
        
        return f"ì‹ ë¢°ë„: {confidence}\nì´ìœ : {reason}"

    # Agent ìƒì„±
    agent = create_agent(
        model="gpt-4o-mini",
        tools=[search_knowledge_base, evaluate_answer_confidence],
        system_prompt="""ë‹¹ì‹ ì€ íšŒì‚¬ì˜ í†µí•© ì§€ì‹ ê²€ìƒ‰ Agentì…ë‹ˆë‹¤.

ì§ˆë¬¸ì— ë‹µë³€í•  ë•Œ:
1. search_knowledge_baseë¡œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
2. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ë‹µë³€ ì‘ì„±
3. evaluate_answer_confidenceë¡œ ë‹µë³€ ì‹ ë¢°ë„ í‰ê°€
4. ë‹µë³€ê³¼ í•¨ê»˜ ì¶œì²˜ì™€ ì‹ ë¢°ë„ë¥¼ ëª…ì‹œ

ë‹µë³€ í˜•ì‹:
[ë‹µë³€ ë‚´ìš©]

ğŸ“š ì¶œì²˜: [ë¬¸ì„œ ì¶œì²˜]
ğŸ¯ ì‹ ë¢°ë„: [HIGH/MEDIUM/LOW]"""
    )

    # ë‹¤ì–‘í•œ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
    test_questions = [
        "RAGê°€ ë¬´ì—‡ì¸ê°€ìš”?",
        "AgentëŠ” ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?",
        "ì—°ì°¨ëŠ” ëª‡ ì¼ì¸ê°€ìš”?",
        "LangChainì˜ ì£¼ìš” ê°œë…ì€?",
    ]

    print("\n" + "=" * 70)
    print("ğŸ§ª ì§€ì‹ ê¸°ë°˜ Q&A í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    for question in test_questions:
        print(f"\n{'=' * 70}")
        print(f"â“ ì§ˆë¬¸: {question}")
        print("=" * 70)

        response = agent.invoke({
            "messages": [{"role": "user", "content": question}]
        })

        answer = response['messages'][-1].content
        print(f"\nğŸ¤– ë‹µë³€:\n{answer}\n")

    # ì‚¬ìš©ì ì…ë ¥
    print("\n" + "=" * 70)
    print("ğŸ’¬ ì§ì ‘ ì§ˆë¬¸í•´ë³´ì„¸ìš” (ì¢…ë£Œ: 'quit' ì…ë ¥)")
    print("=" * 70)

    user_question = input("\nâ“ ì§ˆë¬¸: ").strip()

    if user_question and user_question.lower() != 'quit':
        print("\nğŸ” ê²€ìƒ‰ ì¤‘...\n")

        response = agent.invoke({
            "messages": [{"role": "user", "content": user_question}]
        })

        answer = response['messages'][-1].content
        print(f"ğŸ¤– ë‹µë³€:\n{answer}\n")

    print("=" * 70)
    print("âœ… ì§€ì‹ ê¸°ë°˜ Q&A Agent ì™„ë£Œ!")


# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n")
    print("=" * 70)
    print("Part 8: Agentic RAG (03_agentic_rag.py)")
    print("=" * 70)

    while True:
        print("\nğŸ“š ì‹¤í–‰í•  ì˜ˆì œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        print("  1. Retrieverë¥¼ Toolë¡œ ë³€í™˜")
        print("  2. ì—¬ëŸ¬ Retriever í†µí•©")
        print("  3. Query Planning (ì¿¼ë¦¬ ë¶„í•´)")
        print("  4. Self-RAG (ìê¸° ê²€ì¦)")
        print("  5. ì‹¤ì „ ì§€ì‹ ê¸°ë°˜ Q&A Agent â­")
        print("  0. ì¢…ë£Œ")

        choice = input("\nì„ íƒ (0-5): ").strip()

        if choice == "1":
            example_1_retriever_as_tool()
        elif choice == "2":
            example_2_multiple_retrievers()
        elif choice == "3":
            example_3_query_planning()
        elif choice == "4":
            example_4_self_rag()
        elif choice == "5":
            example_5_knowledge_qa_agent()
        elif choice == "0":
            print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        else:
            print("\nâŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main()
