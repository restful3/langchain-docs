"""
================================================================================
LangChain AI Agent ë§ˆìŠ¤í„° êµì•ˆ
Part 6: Context Engineering
================================================================================

íŒŒì¼ëª…: 04_dynamic_model.py
ë‚œì´ë„: â­â­â­â­ (ê³ ê¸‰)
ì˜ˆìƒ ì‹œê°„: 30ë¶„

ğŸ“š í•™ìŠµ ëª©í‘œ:
  - wrap_model_callë¡œ ëª¨ë¸ ë™ì  ì „í™˜
  - ëŒ€í™” ê¸¸ì´/ë³µì¡ë„ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
  - ë¹„ìš© ì˜ˆì‚° ì œì•½ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
  - í’ˆì§ˆ ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ (haikuâ†’sonnetâ†’opus)
  - Fallback ëª¨ë¸ ì²´ì¸ (ì‹¤íŒ¨ì‹œ ëŒ€ì²´)

ğŸ“– ê³µì‹ ë¬¸ì„œ:
  â€¢ Runtime: /official/18-runtime.md
  â€¢ Context Engineering: /official/19-context-engineering.md

ğŸ“„ êµì•ˆ ë¬¸ì„œ:
  â€¢ Part 6: /docs/part06_context.md

ğŸ”§ í•„ìš”í•œ íŒ¨í‚¤ì§€:
  pip install langchain langchain-openai langgraph python-dotenv

ğŸ”‘ í•„ìš”í•œ í™˜ê²½ë³€ìˆ˜:
  - OPENAI_API_KEY

ğŸš€ ì‹¤í–‰ ë°©ë²•:
  python 04_dynamic_model.py

================================================================================
"""

# ============================================================================
# Imports
# ============================================================================

import os
from dotenv import load_dotenv
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call
from langchain.agents.agent import ModelRequest, ModelResponse
from langchain.tools import tool
from langchain.chat_models import init_chat_model
from langgraph.checkpoint.memory import MemorySaver
from dataclasses import dataclass
from typing import Callable

# ============================================================================
# í™˜ê²½ ì„¤ì •
# ============================================================================

load_dotenv()

if not os.getenv("OPENAI_API_KEY"):
    print("âŒ ì˜¤ë¥˜: OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ğŸ“ .env íŒŒì¼ì„ í™•ì¸í•˜ê³  API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    exit(1)

# ============================================================================
# ì˜ˆì œ 1: wrap_model_callë¡œ ëª¨ë¸ ë™ì  ì „í™˜
# ============================================================================

def example_1_dynamic_model_switching():
    """wrap_model_callì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë™ì ìœ¼ë¡œ ì „í™˜"""
    print("=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 1: wrap_model_callë¡œ ëª¨ë¸ ë™ì  ì „í™˜")
    print("=" * 70)

    print("""
ğŸ’¡ ë™ì  ëª¨ë¸ ì „í™˜ì´ë€?
   - ì‹¤í–‰ ì¤‘ì— ì‚¬ìš©í•  LLM ëª¨ë¸ì„ ë³€ê²½
   - ìƒí™©ì— ë”°ë¼ ìµœì ì˜ ëª¨ë¸ ì„ íƒ
   - ë¹„ìš©ê³¼ ì„±ëŠ¥ì˜ ê· í˜•

ğŸ¯ ì „í™˜ ê¸°ì¤€:
   - ì§ˆë¬¸ ë³µì¡ë„
   - ëŒ€í™” ê¸¸ì´
   - ë¹„ìš© ì˜ˆì‚°
   - ì‘ë‹µ í’ˆì§ˆ ìš”êµ¬ì‚¬í•­
    """)

    # ë„êµ¬ ì •ì˜
    @tool
    def calculate(expression: str) -> str:
        """ìˆ˜í•™ ê³„ì‚° ìˆ˜í–‰"""
        try:
            result = eval(expression)
            return f"ê³„ì‚° ê²°ê³¼: {result}"
        except Exception as e:
            return f"ê³„ì‚° ì˜¤ë¥˜: {e}"

    @tool
    def search_info(topic: str) -> str:
        """ì •ë³´ ê²€ìƒ‰"""
        return f"'{topic}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."

    # ëª¨ë¸ ë¯¸ë¦¬ ì´ˆê¸°í™”
    mini_model = init_chat_model("gpt-4o-mini", model_provider="openai")

    # ë™ì  ëª¨ë¸ ì „í™˜
    @wrap_model_call
    def switch_model_by_length(
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse]
    ) -> ModelResponse:
        """ë©”ì‹œì§€ ê¸¸ì´ì— ë”°ë¼ ëª¨ë¸ ì „í™˜"""

        message_count = len(request.messages)

        print(f"\nğŸ“Š í˜„ì¬ ë©”ì‹œì§€ ìˆ˜: {message_count}ê°œ")

        if message_count > 10:
            # ê¸´ ëŒ€í™”: ê¸°ë³¸ ëª¨ë¸ ìœ ì§€ (gpt-4o-mini)
            print("ğŸ”„ ëª¨ë¸: gpt-4o-mini (ê¸°ë³¸ - ê¸´ ëŒ€í™”)")
            # request.modelì€ ì´ë¯¸ gpt-4o-mini
        elif message_count > 5:
            # ì¤‘ê°„ ëŒ€í™”: gpt-4o-mini ì‚¬ìš©
            print("ğŸ”„ ëª¨ë¸: gpt-4o-mini (ì¤‘ê°„ ëŒ€í™”)")
            request = request.override(model=mini_model)
        else:
            # ì§§ì€ ëŒ€í™”: gpt-4o-mini ì‚¬ìš©
            print("ğŸ”„ ëª¨ë¸: gpt-4o-mini (ì§§ì€ ëŒ€í™”)")
            request = request.override(model=mini_model)

        return handler(request)

    # Agent ìƒì„±
    agent = create_agent(
        model="gpt-4o-mini",
        tools=[calculate, search_info],
        middleware=[switch_model_by_length],
        checkpointer=MemorySaver(),
    )

    # ì—¬ëŸ¬ í„´ ëŒ€í™”
    config = {"configurable": {"thread_id": "model-switch-001"}}

    questions = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "1 + 1ì€?",
        "íŒŒì´ì¬ì— ëŒ€í•´ ì•Œë ¤ì¤˜",
    ]

    for i, question in enumerate(questions, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ’¬ ì§ˆë¬¸ #{i}: {question}")
        print('='*60)

        response = agent.invoke(
            {"messages": [{"role": "user", "content": question}]},
            config=config
        )

        answer = response['messages'][-1].content
        print(f"\nğŸ¤– ì‘ë‹µ: {answer[:100]}...")


# ============================================================================
# ì˜ˆì œ 2: ëŒ€í™” ê¸¸ì´/ë³µì¡ë„ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
# ============================================================================

def example_2_complexity_based_model():
    """ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ëª¨ë¸ ì„ íƒ"""
    print("\n" + "=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 2: ëŒ€í™” ê¸¸ì´/ë³µì¡ë„ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ")
    print("=" * 70)

    print("""
ğŸ§  ë³µì¡ë„ ë¶„ì„ ê¸°ì¤€:
   - ì§ˆë¬¸ ê¸¸ì´ (ë¬¸ì ìˆ˜)
   - í‚¤ì›Œë“œ ("ë¶„ì„", "ì „ë¬¸", "ìƒì„¸")
   - ê¸°ìˆ  ìš©ì–´ í¬í•¨ ì—¬ë¶€
    """)

    @tool
    def analyze_data(data: str) -> str:
        """ë°ì´í„° ë¶„ì„"""
        return f"'{data}' ë¶„ì„ ì™„ë£Œ."

    # ë³µì¡ë„ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
    @wrap_model_call
    def complexity_based_model(
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse]
    ) -> ModelResponse:
        """ì§ˆë¬¸ ë³µì¡ë„ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ"""

        if request.messages:
            last_msg = request.messages[-1].content
        else:
            last_msg = ""

        # ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°
        complexity_score = 0

        # ê¸¸ì´ ì ìˆ˜
        if len(last_msg) > 200:
            complexity_score += 3
        elif len(last_msg) > 100:
            complexity_score += 2
        elif len(last_msg) > 50:
            complexity_score += 1

        # í‚¤ì›Œë“œ ì ìˆ˜
        complex_keywords = ["ë¶„ì„", "ì „ë¬¸", "ìƒì„¸", "ì‹¬ì¸µ", "ë¹„êµ", "í‰ê°€"]
        for keyword in complex_keywords:
            if keyword in last_msg:
                complexity_score += 1

        print(f"\nğŸ’¬ ì§ˆë¬¸ ê¸¸ì´: {len(last_msg)}ì")
        print(f"ğŸ“Š ë³µì¡ë„ ì ìˆ˜: {complexity_score}")

        # ì ìˆ˜ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ
        if complexity_score >= 4:
            # ë§¤ìš° ë³µì¡: gpt-4o-mini ì‚¬ìš©
            print("ğŸ¯ ì„ íƒëœ ëª¨ë¸: gpt-4o-mini (ë³µì¡)")
            model = init_chat_model("gpt-4o-mini", model_provider="openai")
            request = request.override(model=model)
        elif complexity_score >= 2:
            # ì¤‘ê°„ ë³µì¡ë„: gpt-4o-mini
            print("ğŸ¯ ì„ íƒëœ ëª¨ë¸: gpt-4o-mini (ì¤‘ê°„)")
            model = init_chat_model("gpt-4o-mini", model_provider="openai")
            request = request.override(model=model)
        else:
            # ë‹¨ìˆœ: gpt-4o-mini
            print("ğŸ¯ ì„ íƒëœ ëª¨ë¸: gpt-4o-mini (ë‹¨ìˆœ)")
            model = init_chat_model("gpt-4o-mini", model_provider="openai")
            request = request.override(model=model)

        return handler(request)

    agent = create_agent(
        model="gpt-4o-mini",
        tools=[analyze_data],
        middleware=[complexity_based_model],
        checkpointer=MemorySaver(),
    )

    # ë‹¤ì–‘í•œ ë³µì¡ë„ ì§ˆë¬¸
    questions = [
        "ì•ˆë…•",
        "íŒŒì´ì¬ì´ ë­ì•¼?",
        "íŒŒì´ì¬ê³¼ ìë°”ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¹„êµ ë¶„ì„í•´ì„œ ê°ê°ì˜ ì¥ë‹¨ì ì„ ìƒì„¸íˆ ì„¤ëª…í•´ì¤˜",
    ]

    for question in questions:
        print(f"\n{'='*60}")
        print(f"ğŸ’¬ ì§ˆë¬¸: {question}")
        print('='*60)

        response = agent.invoke(
            {"messages": [{"role": "user", "content": question}]},
            config={"configurable": {"thread_id": "complexity-001"}}
        )

        answer = response['messages'][-1].content
        print(f"\nğŸ¤– ì‘ë‹µ: {answer[:150]}...")


# ============================================================================
# ì˜ˆì œ 3: ë¹„ìš© ì˜ˆì‚° ì œì•½ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
# ============================================================================

def example_3_budget_based_model():
    """ì¼ì¼ ì˜ˆì‚°ì„ ì¶”ì í•˜ì—¬ ëª¨ë¸ ì„ íƒ"""
    print("\n" + "=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 3: ë¹„ìš© ì˜ˆì‚° ì œì•½ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ")
    print("=" * 70)

    print("""
ğŸ’° ë¹„ìš© ìµœì í™” ì „ëµ:
   - ì˜ˆì‚° ì¶©ë¶„: ê³ ê¸‰ ëª¨ë¸ (gpt-4o-mini)
   - ì˜ˆì‚° ë¶€ì¡±: ì €ë ´í•œ ëª¨ë¸ (gpt-4o-mini)
   - ì˜ˆì‚° ì´ˆê³¼: ê²½ê³  ë©”ì‹œì§€
    """)

    @dataclass
    class BudgetContext:
        user_id: str
        daily_budget: float  # ë‹¬ëŸ¬
        spent_today: float

    @tool
    def get_recommendation(topic: str) -> str:
        """ì¶”ì²œ ì •ë³´ ì œê³µ"""
        return f"'{topic}' ì¶”ì²œ ì •ë³´ì…ë‹ˆë‹¤."

    # ì˜ˆì‚° ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
    @wrap_model_call
    def budget_based_model(
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse]
    ) -> ModelResponse:
        """ì˜ˆì‚°ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ"""

        ctx = request.runtime.context
        budget = ctx.daily_budget
        spent = ctx.spent_today
        remaining = budget - spent

        print(f"\nğŸ’° ì˜ˆì‚° í˜„í™©:")
        print(f"  - ì¼ì¼ ì˜ˆì‚°: ${budget:.2f}")
        print(f"  - ì‚¬ìš© ê¸ˆì•¡: ${spent:.2f}")
        print(f"  - ë‚¨ì€ ê¸ˆì•¡: ${remaining:.2f}")

        # ì˜ˆì‚° ê¸°ì¤€ ëª¨ë¸ ì„ íƒ
        if remaining >= 1.0:
            # ì˜ˆì‚° ì¶©ë¶„: gpt-4o-mini
            print("ğŸ¯ ì„ íƒ: gpt-4o-mini (ì˜ˆì‚° ì¶©ë¶„)")
            model = init_chat_model("gpt-4o-mini", model_provider="openai")
        elif remaining >= 0.1:
            # ì˜ˆì‚° ì ìŒ: gpt-4o-mini
            print("ğŸ¯ ì„ íƒ: gpt-4o-mini (ì˜ˆì‚° ì ìŒ)")
            model = init_chat_model("gpt-4o-mini", model_provider="openai")
        else:
            # ì˜ˆì‚° ê±°ì˜ ì†Œì§„: gpt-4o-mini (ê°€ì¥ ì €ë ´)
            print("âš ï¸ ì„ íƒ: gpt-4o-mini (ì˜ˆì‚° ê±°ì˜ ì†Œì§„)")
            model = init_chat_model("gpt-4o-mini", model_provider="openai")

        request = request.override(model=model)
        return handler(request)

    agent = create_agent(
        model="gpt-4o-mini",
        tools=[get_recommendation],
        middleware=[budget_based_model],
        context_schema=BudgetContext,
        checkpointer=MemorySaver(),
    )

    # ë‹¤ì–‘í•œ ì˜ˆì‚° ì‹œë‚˜ë¦¬ì˜¤
    budgets = [
        ("user_rich", 10.0, 5.0),   # ì¶©ë¶„í•œ ì˜ˆì‚°
        ("user_low", 5.0, 4.5),     # ì ì€ ì˜ˆì‚°
        ("user_poor", 1.0, 0.95),   # ê±°ì˜ ì†Œì§„
    ]

    for user_id, budget, spent in budgets:
        print(f"\n{'='*60}")
        response = agent.invoke(
            {"messages": [{"role": "user", "content": "ì¶”ì²œ ì¢€ í•´ì¤˜"}]},
            context=BudgetContext(
                user_id=user_id,
                daily_budget=budget,
                spent_today=spent
            ),
            config={"configurable": {"thread_id": f"budget-{user_id}"}}
        )
        print(f"ğŸ’¬ ì‘ë‹µ: {response['messages'][-1].content[:100]}...")


# ============================================================================
# ì˜ˆì œ 4: í’ˆì§ˆ ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
# ============================================================================

def example_4_quality_based_model():
    """ìš”ì²­ì˜ í’ˆì§ˆ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ"""
    print("\n" + "=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 4: í’ˆì§ˆ ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ")
    print("=" * 70)

    print("""
â­ í’ˆì§ˆ ë ˆë²¨:
   - í‘œì¤€: gpt-4o-mini
   - ê³ í’ˆì§ˆ: gpt-4o-mini
   - ìµœê³ í’ˆì§ˆ: gpt-4o-mini
    """)

    @dataclass
    class QualityContext:
        user_id: str
        quality_tier: str  # "standard", "high", "premium"

    @tool
    def create_content(topic: str) -> str:
        """ì½˜í…ì¸  ìƒì„±"""
        return f"'{topic}' ì½˜í…ì¸ ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤."

    # í’ˆì§ˆ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
    @wrap_model_call
    def quality_based_model(
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse]
    ) -> ModelResponse:
        """í’ˆì§ˆ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ"""

        ctx = request.runtime.context
        tier = ctx.quality_tier

        print(f"\nâ­ í’ˆì§ˆ í‹°ì–´: {tier.upper()}")

        # í‹°ì–´ë³„ ëª¨ë¸ ë§¤í•‘
        tier_models = {
            "standard": ("gpt-4o-mini", "í‘œì¤€ ëª¨ë¸"),
            "high": ("gpt-4o-mini", "ê³ í’ˆì§ˆ ëª¨ë¸"),
            "premium": ("gpt-4o-mini", "í”„ë¦¬ë¯¸ì—„ ëª¨ë¸"),
        }

        model_name, description = tier_models.get(tier, ("gpt-4o-mini", "ê¸°ë³¸ ëª¨ë¸"))

        print(f"ğŸ¯ ì„ íƒ: {description}")

        model = init_chat_model(model_name, model_provider="openai")
        request = request.override(model=model)

        return handler(request)

    agent = create_agent(
        model="gpt-4o-mini",
        tools=[create_content],
        middleware=[quality_based_model],
        context_schema=QualityContext,
        checkpointer=MemorySaver(),
    )

    # ë‹¤ì–‘í•œ í’ˆì§ˆ í‹°ì–´
    tiers = ["standard", "high", "premium"]

    for tier in tiers:
        print(f"\n{'='*60}")
        response = agent.invoke(
            {"messages": [{"role": "user", "content": "ê¸°ì‚¬ë¥¼ ì‘ì„±í•´ì¤˜"}]},
            context=QualityContext(
                user_id=f"user_{tier}",
                quality_tier=tier
            ),
            config={"configurable": {"thread_id": f"quality-{tier}"}}
        )
        print(f"ğŸ’¬ ì‘ë‹µ: {response['messages'][-1].content[:100]}...")


# ============================================================================
# ì˜ˆì œ 5: Fallback ëª¨ë¸ ì²´ì¸ (ì‹¤íŒ¨ì‹œ ëŒ€ì²´)
# ============================================================================

def example_5_fallback_model_chain():
    """ëª¨ë¸ ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ëŒ€ì²´ ëª¨ë¸ ì‚¬ìš©"""
    print("\n" + "=" * 70)
    print("ğŸ“Œ ì˜ˆì œ 5: Fallback ëª¨ë¸ ì²´ì¸ (ì‹¤íŒ¨ì‹œ ëŒ€ì²´)")
    print("=" * 70)

    print("""
ğŸ”„ Fallback ì „ëµ:
   1. ë¨¼ì € gpt-4o-mini ì‹œë„
   2. ì‹¤íŒ¨ ì‹œ gpt-4o-minië¡œ ëŒ€ì²´
   3. ëª¨ë‘ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€

ğŸ’¡ ì‹¤íŒ¨ ì›ì¸:
   - API ì˜¤ë¥˜
   - íƒ€ì„ì•„ì›ƒ
   - Rate limit ì´ˆê³¼
   - ê¸°íƒ€ ì˜ˆì™¸
    """)

    @tool
    def process_request(text: str) -> str:
        """ìš”ì²­ ì²˜ë¦¬"""
        return f"'{text}' ì²˜ë¦¬ ì™„ë£Œ."

    # Fallback ëª¨ë¸ ì²´ì¸
    @wrap_model_call
    def fallback_chain(
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse]
    ) -> ModelResponse:
        """Fallback ëª¨ë¸ ì²´ì¸"""

        # 1ì°¨ ì‹œë„: gpt-4o-mini
        print("\nğŸ”µ 1ì°¨ ì‹œë„: gpt-4o-mini")
        primary_model = init_chat_model("gpt-4o-mini", model_provider="openai")
        request = request.override(model=primary_model)

        try:
            response = handler(request)
            print("âœ… ì„±ê³µ!")
            return response
        except Exception as e:
            print(f"âŒ ì‹¤íŒ¨: {e}")

            # 2ì°¨ ì‹œë„: gpt-4o-mini (ëŒ€ì²´)
            print("\nğŸŸ¡ 2ì°¨ ì‹œë„: gpt-4o-mini (ëŒ€ì²´)")
            fallback_model = init_chat_model("gpt-4o-mini", model_provider="openai")
            request = request.override(model=fallback_model)

            try:
                response = handler(request)
                print("âœ… ì„±ê³µ!")
                return response
            except Exception as e2:
                print(f"âŒ ì‹¤íŒ¨: {e2}")

                # 3ì°¨ ì‹œë„: ìµœí›„ ëª¨ë¸
                print("\nğŸ”´ 3ì°¨ ì‹œë„: gpt-4o-mini (ìµœí›„)")
                last_resort = init_chat_model("gpt-4o-mini", model_provider="openai")
                request = request.override(model=last_resort)

                response = handler(request)
                print("âœ… ì„±ê³µ!")
                return response

    agent = create_agent(
        model="gpt-4o-mini",
        tools=[process_request],
        middleware=[fallback_chain],
        checkpointer=MemorySaver(),
    )

    # í…ŒìŠ¤íŠ¸
    print(f"\n{'='*60}")
    response = agent.invoke(
        {"messages": [{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}]},
        config={"configurable": {"thread_id": "fallback-001"}}
    )
    print(f"\nğŸ’¬ ìµœì¢… ì‘ë‹µ: {response['messages'][-1].content}")


# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n")
    print("ğŸ“ LangChain AI Agent ë§ˆìŠ¤í„° êµì•ˆ")
    print("ğŸ“– Part 6: Context Engineering - Dynamic Model")
    print("\n")

    try:
        # ì˜ˆì œ 1: ëª¨ë¸ ë™ì  ì „í™˜
        example_1_dynamic_model_switching()
        input("\nâ ê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")

        # ì˜ˆì œ 2: ë³µì¡ë„ ê¸°ë°˜
        example_2_complexity_based_model()
        input("\nâ ê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")

        # ì˜ˆì œ 3: ì˜ˆì‚° ê¸°ë°˜
        example_3_budget_based_model()
        input("\nâ ê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")

        # ì˜ˆì œ 4: í’ˆì§ˆ ê¸°ë°˜
        example_4_quality_based_model()
        input("\nâ ê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")

        # ì˜ˆì œ 5: Fallback ì²´ì¸
        example_5_fallback_model_chain()

        # ë§ˆë¬´ë¦¬
        print("\n" + "=" * 70)
        print("ğŸ‰ Part 6 - Dynamic Model ì™„ë£Œ!")
        print("=" * 70)
        print("\nğŸ’¡ ë°°ìš´ ë‚´ìš©:")
        print("  âœ… wrap_model_callë¡œ ëª¨ë¸ ë™ì  ì „í™˜")
        print("  âœ… ë³µì¡ë„ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ")
        print("  âœ… ì˜ˆì‚° ì œì•½ ê¸°ë°˜ ì„ íƒ")
        print("  âœ… í’ˆì§ˆ ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ ì„ íƒ")
        print("  âœ… Fallback ëª¨ë¸ ì²´ì¸")
        print("\nğŸ“š ë‹¤ìŒ ë‹¨ê³„:")
        print("  â¡ï¸ 05_tool_runtime.py - ToolRuntime")
        print("\n" + "=" * 70 + "\n")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìê°€ í”„ë¡œê·¸ë¨ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


# ============================================================================
# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
# ============================================================================

if __name__ == "__main__":
    main()


# ============================================================================
# ğŸ“š ì¶”ê°€ í•™ìŠµ í¬ì¸íŠ¸
# ============================================================================
#
# 1. ë™ì  ëª¨ë¸ì˜ ì¥ì :
#    - ë¹„ìš© ìµœì í™”
#    - ì„±ëŠ¥ê³¼ í’ˆì§ˆì˜ ê· í˜•
#    - ìë™ Fallbackìœ¼ë¡œ ì•ˆì •ì„± í–¥ìƒ
#
# 2. ëª¨ë¸ ì„ íƒ ê¸°ì¤€:
#    - ì§ˆë¬¸ ë³µì¡ë„
#    - ëŒ€í™” ê¸¸ì´
#    - ë¹„ìš© ì˜ˆì‚°
#    - í’ˆì§ˆ ìš”êµ¬ì‚¬í•­
#    - ì‘ë‹µ ì‹œê°„ ì œì•½
#
# 3. ì‹¤ì „ íŒ:
#    - í•­ìƒ Fallback ì¤€ë¹„
#    - ë¹„ìš© ì¶”ì  ë° ëª¨ë‹ˆí„°ë§
#    - A/B í…ŒìŠ¤íŒ…ìœ¼ë¡œ ìµœì  ëª¨ë¸ ì°¾ê¸°
#
# ============================================================================
# ğŸ› ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ
# ============================================================================
#
# ë¬¸ì œ: "ëª¨ë¸ì´ ì „í™˜ë˜ì§€ ì•ŠìŒ"
# í•´ê²°: request.override(model=...)ë¡œ ìˆ˜ì •ëœ requestë¥¼ handlerì— ì „ë‹¬
#
# ë¬¸ì œ: "init_chat_modelì´ ì‹¤íŒ¨í•¨"
# í•´ê²°: model_providerë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •
#
# ë¬¸ì œ: "ë¹„ìš©ì´ ì˜ˆìƒë³´ë‹¤ ë§ì´ ë‚˜ì˜´"
# í•´ê²°: ê° ëª¨ë¸ì˜ í† í°ë‹¹ ê°€ê²©ì„ ì •í™•íˆ íŒŒì•…í•˜ê³  ì¶”ì 
#
# ============================================================================
