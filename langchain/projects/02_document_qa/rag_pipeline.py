"""
RAG Pipeline Implementation
RAG (Retrieval-Augmented Generation) íŒŒì´í”„ë¼ì¸ êµ¬í˜„
"""

from typing import List, Optional
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_core.documents import Document


class RAGPipeline:
    """RAG íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤"""

    def __init__(self, documents: Optional[List[Document]] = None):
        """
        ì´ˆê¸°í™”

        Args:
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ìƒˆë¡œ ì¸ë±ì‹±í•  ê²½ìš°)
        """
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        self.vectorstore: Optional[FAISS] = None

        if documents:
            self._build_vectorstore(documents)

    def _build_vectorstore(self, documents: List[Document]):
        """
        ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•

        Args:
            documents: ì¸ë±ì‹±í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ”¨ {len(documents)}ê°œ ë¬¸ì„œë¡œ ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì¤‘...")
        self.vectorstore = FAISS.from_documents(documents, self.embeddings)
        print("âœ… ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ!")

    def create_qa_chain(self, k: int = 3) -> RetrievalQA:
        """
        Q&A ì²´ì¸ ìƒì„±

        Args:
            k: ê²€ìƒ‰í•  ìƒìœ„ ë¬¸ì„œ ê°œìˆ˜

        Returns:
            RetrievalQA: ì§ˆì˜ì‘ë‹µ ì²´ì¸
        """
        if not self.vectorstore:
            raise ValueError("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # í•œêµ­ì–´ ìµœì í™” í”„ë¡¬í”„íŠ¸
        template = """ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ë¬¸ë§¥(context)ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ì§€ì¹¨:
1. ë¬¸ë§¥ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
2. ë‹µë³€ì„ ëª¨ë¥´ë©´ "ì£¼ì–´ì§„ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
3. ì¶”ì¸¡í•˜ê±°ë‚˜ ì™¸ë¶€ ì§€ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
4. ë‹µë³€ì€ ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
5. ê°€ëŠ¥í•œ í•œ ë¬¸ë§¥ì˜ ì›ë¬¸ì„ í™œìš©í•˜ì„¸ìš”.

ë¬¸ë§¥:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

        prompt = PromptTemplate(
            template=template, input_variables=["context", "question"]
        )

        return RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_type="similarity", search_kwargs={"k": k}
            ),
            chain_type_kwargs={"prompt": prompt},
            return_source_documents=True,
        )

    def search_similar_documents(
        self, query: str, k: int = 3
    ) -> List[tuple[Document, float]]:
        """
        ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜

        Returns:
            List[tuple[Document, float]]: (ë¬¸ì„œ, ìœ ì‚¬ë„ ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.vectorstore:
            raise ValueError("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        return self.vectorstore.similarity_search_with_score(query, k=k)

    def add_documents(self, documents: List[Document]):
        """
        ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ì— ë¬¸ì„œ ì¶”ê°€

        Args:
            documents: ì¶”ê°€í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.vectorstore:
            self._build_vectorstore(documents)
        else:
            self.vectorstore.add_documents(documents)
            print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def save_index(self, path: str):
        """
        ë²¡í„° ìŠ¤í† ì–´ ì¸ë±ìŠ¤ ì €ì¥

        Args:
            path: ì €ì¥ ê²½ë¡œ
        """
        if not self.vectorstore:
            raise ValueError("ì €ì¥í•  ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")

        self.vectorstore.save_local(path)
        print(f"âœ… ì¸ë±ìŠ¤ê°€ {path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    @classmethod
    def load_index(cls, path: str) -> "RAGPipeline":
        """
        ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ

        Args:
            path: ì¸ë±ìŠ¤ ê²½ë¡œ

        Returns:
            RAGPipeline: ë¡œë“œëœ íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤
        """
        pipeline = cls()
        pipeline.vectorstore = FAISS.load_local(
            path, pipeline.embeddings, allow_dangerous_deserialization=True
        )
        print(f"âœ… ì¸ë±ìŠ¤ê°€ {path}ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return pipeline

    def get_stats(self) -> dict:
        """
        ë²¡í„° ìŠ¤í† ì–´ í†µê³„ ì •ë³´ ë°˜í™˜

        Returns:
            dict: í†µê³„ ì •ë³´
        """
        if not self.vectorstore:
            return {"status": "empty", "count": 0}

        # FAISS ì¸ë±ìŠ¤ í¬ê¸° í™•ì¸
        index_size = self.vectorstore.index.ntotal

        return {
            "status": "active",
            "count": index_size,
            "embedding_model": "text-embedding-3-small",
            "llm_model": "gpt-4o-mini",
        }


class HybridRAGPipeline(RAGPipeline):
    """
    í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ëŠ” ê³ ê¸‰ RAG íŒŒì´í”„ë¼ì¸
    (ë„ì „ ê³¼ì œìš©)
    """

    def __init__(self, documents: Optional[List[Document]] = None):
        super().__init__(documents)
        self.bm25_retriever = None

    def _build_vectorstore(self, documents: List[Document]):
        """ë²¡í„° ìŠ¤í† ì–´ ë° BM25 ì¸ë±ìŠ¤ êµ¬ì¶•"""
        super()._build_vectorstore(documents)

        # BM25 retriever êµ¬ì¶•
        try:
            from langchain.retrievers import BM25Retriever

            self.bm25_retriever = BM25Retriever.from_documents(documents)
            self.bm25_retriever.k = 3
            print("âœ… BM25 retriever êµ¬ì¶• ì™„ë£Œ!")
        except ImportError:
            print("âš ï¸  BM25Retrieverë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. rank_bm25 ì„¤ì¹˜ í•„ìš”")

    def create_qa_chain(self, k: int = 3) -> RetrievalQA:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ëŠ” Q&A ì²´ì¸ ìƒì„±"""
        if not self.vectorstore:
            raise ValueError("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # BM25ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ RAG ì²´ì¸ ë°˜í™˜
        if not self.bm25_retriever:
            return super().create_qa_chain(k)

        # ì•™ìƒë¸” retriever ìƒì„±
        from langchain.retrievers import EnsembleRetriever

        faiss_retriever = self.vectorstore.as_retriever(search_kwargs={"k": k})

        ensemble_retriever = EnsembleRetriever(
            retrievers=[self.bm25_retriever, faiss_retriever],
            weights=[0.3, 0.7],  # FAISSì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
        )

        template = """ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ë¬¸ë§¥:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

        prompt = PromptTemplate(
            template=template, input_variables=["context", "question"]
        )

        return RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=ensemble_retriever,
            chain_type_kwargs={"prompt": prompt},
            return_source_documents=True,
        )
