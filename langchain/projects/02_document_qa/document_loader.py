"""
Document Loader Module
ë¬¸ì„œ ë¡œë”© ë° ì „ì²˜ë¦¬ ëª¨ë“ˆ
"""

import os
from pathlib import Path
from typing import List
from langchain_community.document_loaders import (
    DirectoryLoader,
    TextLoader,
    UnstructuredMarkdownLoader,
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document


class DocumentLoader:
    """ë¬¸ì„œ ë¡œë” í´ë˜ìŠ¤"""

    def __init__(
        self,
        docs_path: str,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
    ):
        """
        ì´ˆê¸°í™”

        Args:
            docs_path: ë¬¸ì„œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            chunk_size: ì²­í¬ í¬ê¸° (ê¸°ë³¸: 1000)
            chunk_overlap: ì²­í¬ ì¤‘ì²© í¬ê¸° (ê¸°ë³¸: 200)
        """
        self.docs_path = docs_path
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", " ", ""],
        )

    def load_documents(self) -> List[Document]:
        """
        ë¬¸ì„œ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  Markdown íŒŒì¼ ë¡œë“œ

        Returns:
            List[Document]: ì²­í¬ë¡œ ë¶„í• ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not os.path.exists(self.docs_path):
            raise FileNotFoundError(f"ë¬¸ì„œ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.docs_path}")

        print(f"ğŸ“‚ ë¬¸ì„œ ë¡œë“œ ì¤‘: {self.docs_path}")

        # Markdown íŒŒì¼ ë¡œë“œ
        try:
            loader = DirectoryLoader(
                self.docs_path,
                glob="**/*.md",
                loader_cls=TextLoader,
                loader_kwargs={"encoding": "utf-8"},
                show_progress=True,
            )
            documents = loader.load()
        except Exception as e:
            print(f"âš ï¸  TextLoader ì‹¤íŒ¨, UnstructuredMarkdownLoader ì‹œë„: {e}")
            documents = self._load_with_unstructured()

        if not documents:
            print("âš ï¸  ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")

        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for doc in documents:
            self._enhance_metadata(doc)

        # ë¬¸ì„œ ë¶„í• 
        print(f"âœ‚ï¸  ë¬¸ì„œ ë¶„í•  ì¤‘ (chunk_size={self.chunk_size}, overlap={self.chunk_overlap})")
        chunks = self.text_splitter.split_documents(documents)
        print(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")

        return chunks

    def _load_with_unstructured(self) -> List[Document]:
        """UnstructuredMarkdownLoaderë¥¼ ì‚¬ìš©í•œ ë¬¸ì„œ ë¡œë“œ"""
        documents = []
        for file_path in Path(self.docs_path).glob("**/*.md"):
            try:
                loader = UnstructuredMarkdownLoader(str(file_path))
                docs = loader.load()
                documents.extend(docs)
            except Exception as e:
                print(f"âš ï¸  {file_path} ë¡œë“œ ì‹¤íŒ¨: {e}")
        return documents

    def _enhance_metadata(self, doc: Document):
        """
        ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ê°•í™”

        Args:
            doc: ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€í•  ë¬¸ì„œ
        """
        source_path = doc.metadata.get("source", "")

        # íŒŒì¼ ì´ë¦„ ì¶”ê°€
        if source_path:
            doc.metadata["filename"] = Path(source_path).name
            doc.metadata["file_extension"] = Path(source_path).suffix

        # ë¬¸ì„œ ê¸¸ì´ ì¶”ê°€
        doc.metadata["length"] = len(doc.page_content)

        # ë¬¸ì„œ íƒ€ì… ì¶”ì¸¡
        doc.metadata["doc_type"] = self._guess_doc_type(doc.page_content)

    def _guess_doc_type(self, content: str) -> str:
        """
        ë¬¸ì„œ ë‚´ìš©ìœ¼ë¡œë¶€í„° íƒ€ì… ì¶”ì¸¡

        Args:
            content: ë¬¸ì„œ ë‚´ìš©

        Returns:
            str: ì¶”ì¸¡ëœ ë¬¸ì„œ íƒ€ì…
        """
        content_lower = content.lower()

        if "```python" in content_lower or "def " in content_lower:
            return "code_tutorial"
        elif "langchain" in content_lower:
            return "langchain_doc"
        elif "ìœ¤ë¦¬" in content_lower or "ethics" in content_lower:
            return "ethics"
        else:
            return "general"

    def load_single_file(self, file_path: str) -> List[Document]:
        """
        ë‹¨ì¼ íŒŒì¼ ë¡œë“œ

        Args:
            file_path: íŒŒì¼ ê²½ë¡œ

        Returns:
            List[Document]: ì²­í¬ë¡œ ë¶„í• ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

        print(f"ğŸ“„ ë‹¨ì¼ íŒŒì¼ ë¡œë“œ: {file_path}")

        try:
            loader = TextLoader(file_path, encoding="utf-8")
            documents = loader.load()
        except Exception as e:
            print(f"âš ï¸  TextLoader ì‹¤íŒ¨: {e}")
            loader = UnstructuredMarkdownLoader(file_path)
            documents = loader.load()

        # ë©”íƒ€ë°ì´í„° ê°•í™”
        for doc in documents:
            self._enhance_metadata(doc)

        # ë¬¸ì„œ ë¶„í• 
        chunks = self.text_splitter.split_documents(documents)
        print(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")

        return chunks

    def get_document_stats(self, documents: List[Document]) -> dict:
        """
        ë¬¸ì„œ í†µê³„ ì •ë³´ ë°˜í™˜

        Args:
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

        Returns:
            dict: í†µê³„ ì •ë³´
        """
        if not documents:
            return {"total_docs": 0}

        total_length = sum(len(doc.page_content) for doc in documents)
        avg_length = total_length / len(documents) if documents else 0

        # ë¬¸ì„œ íƒ€ì…ë³„ ê°œìˆ˜
        doc_types = {}
        for doc in documents:
            doc_type = doc.metadata.get("doc_type", "unknown")
            doc_types[doc_type] = doc_types.get(doc_type, 0) + 1

        return {
            "total_docs": len(documents),
            "total_length": total_length,
            "avg_length": round(avg_length, 2),
            "doc_types": doc_types,
        }


class CustomMarkdownLoader(DocumentLoader):
    """
    ì»¤ìŠ¤í…€ Markdown ë¡œë” (í—¤ë” ê¸°ë°˜ ë¶„í• )
    ë„ì „ ê³¼ì œìš©
    """

    def __init__(self, docs_path: str, **kwargs):
        super().__init__(docs_path, **kwargs)
        # ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°ë°˜ ë¶„í• ê¸°
        from langchain.text_splitter import MarkdownHeaderTextSplitter

        self.md_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=[
                ("#", "Header 1"),
                ("##", "Header 2"),
                ("###", "Header 3"),
            ]
        )

    def load_documents(self) -> List[Document]:
        """í—¤ë” ê¸°ë°˜ìœ¼ë¡œ Markdown ë¬¸ì„œ ë¡œë“œ"""
        documents = []

        for file_path in Path(self.docs_path).glob("**/*.md"):
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()

                # í—¤ë” ê¸°ë°˜ ë¶„í• 
                md_docs = self.md_splitter.split_text(content)

                # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                for doc in md_docs:
                    doc.metadata["source"] = str(file_path)
                    doc.metadata["filename"] = file_path.name
                    self._enhance_metadata(doc)

                documents.extend(md_docs)
                print(f"âœ… {file_path.name}: {len(md_docs)}ê°œ ì„¹ì…˜")

            except Exception as e:
                print(f"âš ï¸  {file_path} ë¡œë“œ ì‹¤íŒ¨: {e}")

        # ì¶”ê°€ ì²­í‚¹ (ì„¹ì…˜ì´ ë„ˆë¬´ í° ê²½ìš°)
        final_chunks = []
        for doc in documents:
            if len(doc.page_content) > self.chunk_size:
                chunks = self.text_splitter.split_documents([doc])
                final_chunks.extend(chunks)
            else:
                final_chunks.append(doc)

        print(f"âœ… ì´ {len(final_chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        return final_chunks
