# Chapter 16: ìŠ¤íŠ¸ë¦¬ë°

> ğŸ“Œ **í•™ìŠµ ëª©í‘œ**: ì´ ì¥ì„ ë§ˆì¹˜ë©´ ë‹¤ì–‘í•œ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¥¼ í™œìš©í•˜ì—¬ ì‹¤ì‹œê°„ ì¶œë ¥ê³¼ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ê°œìš”

**ìŠ¤íŠ¸ë¦¬ë°**ì€ ê·¸ë˜í”„ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°›ì•„ë³¼ ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ ì§„í–‰ ìƒí™©ì„ ë³´ì—¬ì£¼ê±°ë‚˜, LLM í† í°ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥í•  ë•Œ í•„ìˆ˜ì…ë‹ˆë‹¤.

```mermaid
graph LR
    GRAPH[Graph ì‹¤í–‰] --> |stream| EVENTS[ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼]
    EVENTS --> |values| V[ì „ì²´ ìƒíƒœ]
    EVENTS --> |updates| U[ì—…ë°ì´íŠ¸ë§Œ]
    EVENTS --> |messages| M[ë©”ì‹œì§€ë§Œ]
    EVENTS --> |events| E[ëª¨ë“  ì´ë²¤íŠ¸]
```

## í•µì‹¬ ê°œë…

### ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ

| ëª¨ë“œ | ì„¤ëª… | ì¶œë ¥ |
|-----|------|------|
| **values** | ê° ë‹¨ê³„ì˜ ì „ì²´ ìƒíƒœ | `{"messages": [...]}` |
| **updates** | ê° ë…¸ë“œì˜ ì—…ë°ì´íŠ¸ë§Œ | `{"node": {"key": "value"}}` |
| **messages** | ë©”ì‹œì§€ ê´€ë ¨ë§Œ | `(message, metadata)` |
| **events** | ëª¨ë“  ë‚´ë¶€ ì´ë²¤íŠ¸ | ìƒì„¸ ì´ë²¤íŠ¸ ì •ë³´ |

## ì‹¤ìŠµ 1: ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë°

```python
# ğŸ“ src/part4_production/15_streaming.py
from langgraph.graph import StateGraph, START, END, MessagesState
from langchain_core.messages import HumanMessage


def node_a(state: MessagesState) -> MessagesState:
    return {"messages": ["Node A ì™„ë£Œ"]}


def node_b(state: MessagesState) -> MessagesState:
    return {"messages": ["Node B ì™„ë£Œ"]}


graph = StateGraph(MessagesState)
graph.add_node("a", node_a)
graph.add_node("b", node_b)
graph.add_edge(START, "a")
graph.add_edge("a", "b")
graph.add_edge("b", END)

app = graph.compile()

# values ëª¨ë“œ (ê¸°ë³¸) - ê° ë‹¨ê³„ì˜ ì „ì²´ ìƒíƒœ
print("=== values ëª¨ë“œ ===")
for chunk in app.stream({"messages": [HumanMessage(content="ì‹œì‘")]}):
    print(chunk)

# updates ëª¨ë“œ - ë…¸ë“œë³„ ì—…ë°ì´íŠ¸ë§Œ
print("\n=== updates ëª¨ë“œ ===")
for chunk in app.stream(
    {"messages": [HumanMessage(content="ì‹œì‘")]},
    stream_mode="updates"
):
    for node, update in chunk.items():
        print(f"[{node}] {update}")
```

> ğŸ’¡ **ì „ì²´ ì½”ë“œ**: [src/part4_production/15_streaming.py](../../src/part4_production/15_streaming.py)

## ì‹¤ìŠµ 2: LLM í† í° ìŠ¤íŠ¸ë¦¬ë°

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import AIMessageChunk


llm = ChatAnthropic(model="claude-sonnet-4-5-20250929", streaming=True)


def llm_node(state: MessagesState) -> MessagesState:
    response = llm.invoke(state["messages"])
    return {"messages": [response]}


# events ëª¨ë“œë¡œ LLM í† í°ê¹Œì§€ ìŠ¤íŠ¸ë¦¬ë°
for event in app.stream(
    {"messages": [HumanMessage(content="ì§§ì€ ì´ì•¼ê¸°ë¥¼ í•´ì£¼ì„¸ìš”")]},
    stream_mode="events"
):
    # LLM í† í° ì´ë²¤íŠ¸ ì²˜ë¦¬
    if event["event"] == "on_chat_model_stream":
        chunk = event["data"]["chunk"]
        if isinstance(chunk, AIMessageChunk) and chunk.content:
            print(chunk.content, end="", flush=True)
```

## ì‹¤ìŠµ 3: messages ëª¨ë“œ

ë©”ì‹œì§€ ê´€ë ¨ ì´ë²¤íŠ¸ë§Œ ë°›ìŠµë‹ˆë‹¤.

```python
# messages ëª¨ë“œ - AI ë©”ì‹œì§€ì™€ ë„êµ¬ í˜¸ì¶œë§Œ
for message, metadata in app.stream(
    {"messages": [HumanMessage(content="ê²€ìƒ‰í•´ì¤˜")]},
    stream_mode="messages"
):
    print(f"[{metadata.get('langgraph_node')}] {message}")
```

### ë©”ì‹œì§€ í•„í„°ë§

```python
from langchain_core.messages import AIMessage, ToolMessage


for message, metadata in app.stream(..., stream_mode="messages"):
    if isinstance(message, AIMessage):
        if message.tool_calls:
            print(f"ë„êµ¬ í˜¸ì¶œ: {message.tool_calls}")
        else:
            print(f"ì‘ë‹µ: {message.content}")
    elif isinstance(message, ToolMessage):
        print(f"ë„êµ¬ ê²°ê³¼: {message.content}")
```

## ì‹¤ìŠµ 4: ì„œë¸Œê·¸ë˜í”„ ìŠ¤íŠ¸ë¦¬ë°

```python
# subgraphs=Trueë¡œ ì„œë¸Œê·¸ë˜í”„ ì´ë²¤íŠ¸ í¬í•¨
for namespace, chunk in app.stream(
    {"messages": [HumanMessage(content="ì‹œì‘")]},
    stream_mode="updates",
    subgraphs=True  # ì„œë¸Œê·¸ë˜í”„ ì´ë²¤íŠ¸ í¬í•¨
):
    # namespaceëŠ” ì„œë¸Œê·¸ë˜í”„ ê²½ë¡œë¥¼ ë‚˜íƒ€ëƒ„
    # () - ë£¨íŠ¸ ê·¸ë˜í”„
    # ("subgraph_name",) - ì²« ë²ˆì§¸ ë ˆë²¨ ì„œë¸Œê·¸ë˜í”„
    print(f"Namespace: {namespace}")
    print(f"Update: {chunk}")
```

## ì‹¤ìŠµ 5: ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°

```python
import asyncio


async def stream_async():
    """ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°"""
    async for chunk in app.astream(
        {"messages": [HumanMessage(content="ë¹„ë™ê¸° ì‹œì‘")]},
        stream_mode="updates"
    ):
        print(chunk)


# ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°
async def stream_events_async():
    """ë¹„ë™ê¸° ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°"""
    async for event in app.astream_events(
        {"messages": [HumanMessage(content="ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°")]},
        version="v2"
    ):
        kind = event["event"]
        if kind == "on_chat_model_stream":
            content = event["data"]["chunk"].content
            if content:
                print(content, end="", flush=True)


asyncio.run(stream_async())
```

## ì‹¤ìŠµ 6: ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì¡°í•©

ì—¬ëŸ¬ ëª¨ë“œë¥¼ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# ì—¬ëŸ¬ ëª¨ë“œ ë™ì‹œ ì‚¬ìš©
for chunk in app.stream(
    {"messages": [HumanMessage(content="ë³µí•© ìŠ¤íŠ¸ë¦¬ë°")]},
    stream_mode=["values", "updates", "messages"]
):
    mode, data = chunk
    print(f"[{mode}] {data}")
```

## ê³ ê¸‰ íŒ¨í„´: ì§„í–‰ë¥  í‘œì‹œ

```python
from typing import TypedDict


class ProgressState(TypedDict):
    messages: list
    current_step: int
    total_steps: int
    progress: float


def update_progress(state: ProgressState) -> ProgressState:
    """ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
    current = state["current_step"] + 1
    total = state["total_steps"]
    progress = current / total * 100

    return {
        "current_step": current,
        "progress": progress,
        "messages": [f"ì§„í–‰ ì¤‘: {progress:.1f}%"]
    }


# í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì§„í–‰ë¥  í‘œì‹œ
for chunk in app.stream(initial_state, stream_mode="updates"):
    if "progress" in chunk.get("update_progress", {}):
        progress = chunk["update_progress"]["progress"]
        print(f"\rì§„í–‰ë¥ : {progress:.1f}%", end="", flush=True)
```

## ê³ ê¸‰ íŒ¨í„´: ì‹¤ì‹œê°„ UI ì—…ë°ì´íŠ¸

```python
import json


async def stream_to_websocket(websocket, input_data):
    """WebSocketìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°"""
    async for event in app.astream_events(input_data, version="v2"):
        # ì´ë²¤íŠ¸ ìœ í˜•ì— ë”°ë¼ ë‹¤ë¥¸ ë©”ì‹œì§€ ì „ì†¡
        if event["event"] == "on_chain_start":
            await websocket.send(json.dumps({
                "type": "start",
                "node": event["name"]
            }))
        elif event["event"] == "on_chat_model_stream":
            content = event["data"]["chunk"].content
            if content:
                await websocket.send(json.dumps({
                    "type": "token",
                    "content": content
                }))
        elif event["event"] == "on_chain_end":
            await websocket.send(json.dumps({
                "type": "end",
                "node": event["name"]
            }))
```

## ìš”ì•½

- **values**: ê° ë‹¨ê³„ì˜ ì „ì²´ ìƒíƒœ
- **updates**: ë…¸ë“œë³„ ì—…ë°ì´íŠ¸ë§Œ (`{node: update}`)
- **messages**: AI ë©”ì‹œì§€ì™€ ë„êµ¬ ê´€ë ¨ë§Œ
- **events**: ëª¨ë“  ë‚´ë¶€ ì´ë²¤íŠ¸ (í† í° ìŠ¤íŠ¸ë¦¬ë° í¬í•¨)
- **subgraphs=True**: ì„œë¸Œê·¸ë˜í”„ ì´ë²¤íŠ¸ í¬í•¨

## ë‹¤ìŒ ë‹¨ê³„

ë‹¤ìŒ ì¥ì—ì„œëŠ” **Time Travel**ì„ í•™ìŠµí•©ë‹ˆë‹¤. ìƒíƒœ íˆìŠ¤í† ë¦¬ íƒìƒ‰ê³¼ ë³µì›ì„ ë‹¤ë£¹ë‹ˆë‹¤.

ğŸ‘‰ [Chapter 17: Time Travel](./17-time-travel.md)

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [Streaming (ê³µì‹ ì˜¨ë¼ì¸)](https://docs.langchain.com/oss/python/langgraph/streaming) - ìŠ¤íŠ¸ë¦¬ë° ê°€ì´ë“œ
- [astream_events (ê³µì‹ ì˜¨ë¼ì¸)](https://docs.langchain.com/oss/python/langgraph/streaming#events) - ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°

### ì‹¤ìŠµ ì½”ë“œ
- [ì „ì²´ ì†ŒìŠ¤](../../src/part4_production/15_streaming.py) - ì‹¤í–‰ ê°€ëŠ¥í•œ ì „ì²´ ì½”ë“œ

### ê´€ë ¨ ì±•í„°
- [ì´ì „: Chapter 15 - Human-in-the-Loop](./15-human-in-the-loop.md)
- [ë‹¤ìŒ: Chapter 17 - Time Travel](./17-time-travel.md)
